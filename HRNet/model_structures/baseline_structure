digraph {
	graph [size="904.8,904.8"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140571584970208 [label="
 (1, 9, 64, 64)" fillcolor=darkolivegreen1]
	140571601152096 [label=ConvolutionBackward0]
	140571585133104 -> 140571601152096
	140571585133104 [label=ReluBackward0]
	140571585130896 -> 140571585133104
	140571585130896 [label=NativeBatchNormBackward0]
	140571585132288 -> 140571585130896
	140571585132288 [label=ConvolutionBackward0]
	140571585133728 -> 140571585132288
	140571585133728 [label=CatBackward0]
	140571585130368 -> 140571585133728
	140571585130368 [label=ReluBackward0]
	140571585131760 -> 140571585130368
	140571585131760 [label=AddBackward0]
	140571585132096 -> 140571585131760
	140571585132096 [label=AddBackward0]
	140571585132432 -> 140571585132096
	140571585132432 [label=AddBackward0]
	140571585129744 -> 140571585132432
	140571585129744 [label=ReluBackward0]
	140571585131712 -> 140571585129744
	140571585131712 [label=AddBackward0]
	140571585133968 -> 140571585131712
	140571585133968 [label=NativeBatchNormBackward0]
	140571585130416 -> 140571585133968
	140571585130416 [label=ConvolutionBackward0]
	140571585132240 -> 140571585130416
	140571585132240 [label=ReluBackward0]
	140571585130752 -> 140571585132240
	140571585130752 [label=NativeBatchNormBackward0]
	140571585129072 -> 140571585130752
	140571585129072 [label=ConvolutionBackward0]
	140571585131952 -> 140571585129072
	140571585131952 [label=ReluBackward0]
	140571585128400 -> 140571585131952
	140571585128400 [label=AddBackward0]
	140571585128352 -> 140571585128400
	140571585128352 [label=NativeBatchNormBackward0]
	140571585127920 -> 140571585128352
	140571585127920 [label=ConvolutionBackward0]
	140571585127536 -> 140571585127920
	140571585127536 [label=ReluBackward0]
	140571585127056 -> 140571585127536
	140571585127056 [label=NativeBatchNormBackward0]
	140571585127008 -> 140571585127056
	140571585127008 [label=ConvolutionBackward0]
	140571585128256 -> 140571585127008
	140571585128256 [label=ReluBackward0]
	140571585126336 -> 140571585128256
	140571585126336 [label=AddBackward0]
	140571585126048 -> 140571585126336
	140571585126048 [label=NativeBatchNormBackward0]
	140571585125568 -> 140571585126048
	140571585125568 [label=ConvolutionBackward0]
	140571585125232 -> 140571585125568
	140571585125232 [label=ReluBackward0]
	140571585124992 -> 140571585125232
	140571585124992 [label=NativeBatchNormBackward0]
	140571585124704 -> 140571585124992
	140571585124704 [label=ConvolutionBackward0]
	140571585126192 -> 140571585124704
	140571585126192 [label=ReluBackward0]
	140571585124032 -> 140571585126192
	140571585124032 [label=AddBackward0]
	140571585123696 -> 140571585124032
	140571585123696 [label=NativeBatchNormBackward0]
	140571585123504 -> 140571585123696
	140571585123504 [label=ConvolutionBackward0]
	140571585122880 -> 140571585123504
	140571585122880 [label=ReluBackward0]
	140571585122688 -> 140571585122880
	140571585122688 [label=NativeBatchNormBackward0]
	140571585122400 -> 140571585122688
	140571585122400 [label=ConvolutionBackward0]
	140571585123888 -> 140571585122400
	140571585123888 [label=ReluBackward0]
	140571585121680 -> 140571585123888
	140571585121680 [label=AddBackward0]
	140571585121632 -> 140571585121680
	140571585121632 [label=AddBackward0]
	140571585121200 -> 140571585121632
	140571585121200 [label=AddBackward0]
	140571585120960 -> 140571585121200
	140571585120960 [label=ReluBackward0]
	140571585120528 -> 140571585120960
	140571585120528 [label=AddBackward0]
	140571585120192 -> 140571585120528
	140571585120192 [label=NativeBatchNormBackward0]
	140571585120000 -> 140571585120192
	140571585120000 [label=ConvolutionBackward0]
	140571585119616 -> 140571585120000
	140571585119616 [label=ReluBackward0]
	140571585119184 -> 140571585119616
	140571585119184 [label=NativeBatchNormBackward0]
	140571585118848 -> 140571585119184
	140571585118848 [label=ConvolutionBackward0]
	140571585120336 -> 140571585118848
	140571585120336 [label=ReluBackward0]
	140571585118320 -> 140571585120336
	140571585118320 [label=AddBackward0]
	140571585118272 -> 140571585118320
	140571585118272 [label=NativeBatchNormBackward0]
	140571584986512 -> 140571585118272
	140571584986512 [label=ConvolutionBackward0]
	140571584986176 -> 140571584986512
	140571584986176 [label=ReluBackward0]
	140571584985696 -> 140571584986176
	140571584985696 [label=NativeBatchNormBackward0]
	140571584985648 -> 140571584985696
	140571584985648 [label=ConvolutionBackward0]
	140571585118800 -> 140571584985648
	140571585118800 [label=ReluBackward0]
	140571584984976 -> 140571585118800
	140571584984976 [label=AddBackward0]
	140571584984688 -> 140571584984976
	140571584984688 [label=NativeBatchNormBackward0]
	140571584984448 -> 140571584984688
	140571584984448 [label=ConvolutionBackward0]
	140571584983824 -> 140571584984448
	140571584983824 [label=ReluBackward0]
	140571584983632 -> 140571584983824
	140571584983632 [label=NativeBatchNormBackward0]
	140571584983344 -> 140571584983632
	140571584983344 [label=ConvolutionBackward0]
	140571584984832 -> 140571584983344
	140571584984832 [label=ReluBackward0]
	140571584982672 -> 140571584984832
	140571584982672 [label=AddBackward0]
	140571584982336 -> 140571584982672
	140571584982336 [label=NativeBatchNormBackward0]
	140571584982144 -> 140571584982336
	140571584982144 [label=ConvolutionBackward0]
	140571584981760 -> 140571584982144
	140571584981760 [label=ReluBackward0]
	140571584981328 -> 140571584981760
	140571584981328 [label=NativeBatchNormBackward0]
	140571584980992 -> 140571584981328
	140571584980992 [label=ConvolutionBackward0]
	140571584982480 -> 140571584980992
	140571584982480 [label=ReluBackward0]
	140571584980320 -> 140571584982480
	140571584980320 [label=AddBackward0]
	140571584980272 -> 140571584980320
	140571584980272 [label=AddBackward0]
	140571584979792 -> 140571584980272
	140571584979792 [label=AddBackward0]
	140571584979600 -> 140571584979792
	140571584979600 [label=ReluBackward0]
	140571584979120 -> 140571584979600
	140571584979120 [label=AddBackward0]
	140571584979072 -> 140571584979120
	140571584979072 [label=NativeBatchNormBackward0]
	140571584978640 -> 140571584979072
	140571584978640 [label=ConvolutionBackward0]
	140571584978256 -> 140571584978640
	140571584978256 [label=ReluBackward0]
	140571584977776 -> 140571584978256
	140571584977776 [label=NativeBatchNormBackward0]
	140571584977728 -> 140571584977776
	140571584977728 [label=ConvolutionBackward0]
	140571584978976 -> 140571584977728
	140571584978976 [label=ReluBackward0]
	140571584977056 -> 140571584978976
	140571584977056 [label=AddBackward0]
	140571584976768 -> 140571584977056
	140571584976768 [label=NativeBatchNormBackward0]
	140571584976288 -> 140571584976768
	140571584976288 [label=ConvolutionBackward0]
	140571584975952 -> 140571584976288
	140571584975952 [label=ReluBackward0]
	140571584975712 -> 140571584975952
	140571584975712 [label=NativeBatchNormBackward0]
	140571584975424 -> 140571584975712
	140571584975424 [label=ConvolutionBackward0]
	140571584976912 -> 140571584975424
	140571584976912 [label=ReluBackward0]
	140571584974752 -> 140571584976912
	140571584974752 [label=AddBackward0]
	140571584974416 -> 140571584974752
	140571584974416 [label=NativeBatchNormBackward0]
	140571584974224 -> 140571584974416
	140571584974224 [label=ConvolutionBackward0]
	140571584973600 -> 140571584974224
	140571584973600 [label=ReluBackward0]
	140571584973408 -> 140571584973600
	140571584973408 [label=NativeBatchNormBackward0]
	140571584973072 -> 140571584973408
	140571584973072 [label=ConvolutionBackward0]
	140571584974608 -> 140571584973072
	140571584974608 [label=ReluBackward0]
	140571584972400 -> 140571584974608
	140571584972400 [label=AddBackward0]
	140571584972352 -> 140571584972400
	140571584972352 [label=NativeBatchNormBackward0]
	140571584971920 -> 140571584972352
	140571584971920 [label=ConvolutionBackward0]
	140571584971536 -> 140571584971920
	140571584971536 [label=ReluBackward0]
	140571584971056 -> 140571584971536
	140571584971056 [label=NativeBatchNormBackward0]
	140571584971008 -> 140571584971056
	140571584971008 [label=ConvolutionBackward0]
	140571584972256 -> 140571584971008
	140571584972256 [label=ReluBackward0]
	140571584822816 -> 140571584972256
	140571584822816 [label=AddBackward0]
	140571584822528 -> 140571584822816
	140571584822528 [label=AddBackward0]
	140571584822048 -> 140571584822528
	140571584822048 [label=ReluBackward0]
	140571584821856 -> 140571584822048
	140571584821856 [label=AddBackward0]
	140571584821520 -> 140571584821856
	140571584821520 [label=NativeBatchNormBackward0]
	140571584821328 -> 140571584821520
	140571584821328 [label=ConvolutionBackward0]
	140571584820704 -> 140571584821328
	140571584820704 [label=ReluBackward0]
	140571584820512 -> 140571584820704
	140571584820512 [label=NativeBatchNormBackward0]
	140571584820176 -> 140571584820512
	140571584820176 [label=ConvolutionBackward0]
	140571584821712 -> 140571584820176
	140571584821712 [label=ReluBackward0]
	140571584819504 -> 140571584821712
	140571584819504 [label=AddBackward0]
	140571584819456 -> 140571584819504
	140571584819456 [label=NativeBatchNormBackward0]
	140571584819024 -> 140571584819456
	140571584819024 [label=ConvolutionBackward0]
	140571584818640 -> 140571584819024
	140571584818640 [label=ReluBackward0]
	140571584818160 -> 140571584818640
	140571584818160 [label=NativeBatchNormBackward0]
	140571584818112 -> 140571584818160
	140571584818112 [label=ConvolutionBackward0]
	140571584819360 -> 140571584818112
	140571584819360 [label=ReluBackward0]
	140571584817440 -> 140571584819360
	140571584817440 [label=AddBackward0]
	140571584817152 -> 140571584817440
	140571584817152 [label=NativeBatchNormBackward0]
	140571584816672 -> 140571584817152
	140571584816672 [label=ConvolutionBackward0]
	140571584816336 -> 140571584816672
	140571584816336 [label=ReluBackward0]
	140571584816096 -> 140571584816336
	140571584816096 [label=NativeBatchNormBackward0]
	140571584815808 -> 140571584816096
	140571584815808 [label=ConvolutionBackward0]
	140571584817296 -> 140571584815808
	140571584817296 [label=ReluBackward0]
	140571584815136 -> 140571584817296
	140571584815136 [label=AddBackward0]
	140571584814800 -> 140571584815136
	140571584814800 [label=NativeBatchNormBackward0]
	140571584814608 -> 140571584814800
	140571584814608 [label=ConvolutionBackward0]
	140571584813984 -> 140571584814608
	140571584813984 [label=ReluBackward0]
	140571584813792 -> 140571584813984
	140571584813792 [label=NativeBatchNormBackward0]
	140571584813456 -> 140571584813792
	140571584813456 [label=ConvolutionBackward0]
	140571584814992 -> 140571584813456
	140571584814992 [label=ReluBackward0]
	140571584812784 -> 140571584814992
	140571584812784 [label=AddBackward0]
	140571584812736 -> 140571584812784
	140571584812736 [label=AddBackward0]
	140571584812304 -> 140571584812736
	140571584812304 [label=ReluBackward0]
	140571584812064 -> 140571584812304
	140571584812064 [label=AddBackward0]
	140571584811776 -> 140571584812064
	140571584811776 [label=NativeBatchNormBackward0]
	140571584811296 -> 140571584811776
	140571584811296 [label=ConvolutionBackward0]
	140571584810960 -> 140571584811296
	140571584810960 [label=ReluBackward0]
	140571584810720 -> 140571584810960
	140571584810720 [label=NativeBatchNormBackward0]
	140571584810432 -> 140571584810720
	140571584810432 [label=ConvolutionBackward0]
	140571584811920 -> 140571584810432
	140571584811920 [label=ReluBackward0]
	140571584809760 -> 140571584811920
	140571584809760 [label=AddBackward0]
	140571584809424 -> 140571584809760
	140571584809424 [label=NativeBatchNormBackward0]
	140571584809232 -> 140571584809424
	140571584809232 [label=ConvolutionBackward0]
	140571584808608 -> 140571584809232
	140571584808608 [label=ReluBackward0]
	140571584808416 -> 140571584808608
	140571584808416 [label=NativeBatchNormBackward0]
	140571584808080 -> 140571584808416
	140571584808080 [label=ConvolutionBackward0]
	140571584809616 -> 140571584808080
	140571584809616 [label=ReluBackward0]
	140571584807408 -> 140571584809616
	140571584807408 [label=AddBackward0]
	140571584807360 -> 140571584807408
	140571584807360 [label=NativeBatchNormBackward0]
	140571584807072 -> 140571584807360
	140571584807072 [label=ConvolutionBackward0]
	140571584691792 -> 140571584807072
	140571584691792 [label=ReluBackward0]
	140571584691312 -> 140571584691792
	140571584691312 [label=NativeBatchNormBackward0]
	140571584691264 -> 140571584691312
	140571584691264 [label=ConvolutionBackward0]
	140571584807264 -> 140571584691264
	140571584807264 [label=ReluBackward0]
	140571584690592 -> 140571584807264
	140571584690592 [label=AddBackward0]
	140571584690304 -> 140571584690592
	140571584690304 [label=NativeBatchNormBackward0]
	140571584689824 -> 140571584690304
	140571584689824 [label=ConvolutionBackward0]
	140571584689488 -> 140571584689824
	140571584689488 [label=ReluBackward0]
	140571584689248 -> 140571584689488
	140571584689248 [label=NativeBatchNormBackward0]
	140571584688960 -> 140571584689248
	140571584688960 [label=ConvolutionBackward0]
	140571584690448 -> 140571584688960
	140571584690448 [label=ReluBackward0]
	140571584688288 -> 140571584690448
	140571584688288 [label=AddBackward0]
	140571584687952 -> 140571584688288
	140571584687952 [label=AddBackward0]
	140571584687760 -> 140571584687952
	140571584687760 [label=ReluBackward0]
	140571584687280 -> 140571584687760
	140571584687280 [label=AddBackward0]
	140571584687232 -> 140571584687280
	140571584687232 [label=NativeBatchNormBackward0]
	140571584686800 -> 140571584687232
	140571584686800 [label=ConvolutionBackward0]
	140571584686416 -> 140571584686800
	140571584686416 [label=ReluBackward0]
	140571584685936 -> 140571584686416
	140571584685936 [label=NativeBatchNormBackward0]
	140571584685888 -> 140571584685936
	140571584685888 [label=ConvolutionBackward0]
	140571584687136 -> 140571584685888
	140571584687136 [label=ReluBackward0]
	140571584685216 -> 140571584687136
	140571584685216 [label=AddBackward0]
	140571584684928 -> 140571584685216
	140571584684928 [label=NativeBatchNormBackward0]
	140571584684448 -> 140571584684928
	140571584684448 [label=ConvolutionBackward0]
	140571584684112 -> 140571584684448
	140571584684112 [label=ReluBackward0]
	140571584683872 -> 140571584684112
	140571584683872 [label=NativeBatchNormBackward0]
	140571584683584 -> 140571584683872
	140571584683584 [label=ConvolutionBackward0]
	140571584685072 -> 140571584683584
	140571584685072 [label=ReluBackward0]
	140571584682912 -> 140571584685072
	140571584682912 [label=AddBackward0]
	140571584682576 -> 140571584682912
	140571584682576 [label=NativeBatchNormBackward0]
	140571584682384 -> 140571584682576
	140571584682384 [label=ConvolutionBackward0]
	140571584681760 -> 140571584682384
	140571584681760 [label=ReluBackward0]
	140571584681568 -> 140571584681760
	140571584681568 [label=NativeBatchNormBackward0]
	140571584681232 -> 140571584681568
	140571584681232 [label=ConvolutionBackward0]
	140571584682768 -> 140571584681232
	140571584682768 [label=ReluBackward0]
	140571584680560 -> 140571584682768
	140571584680560 [label=AddBackward0]
	140571584680512 -> 140571584680560
	140571584680512 [label=NativeBatchNormBackward0]
	140571584680080 -> 140571584680512
	140571584680080 [label=ConvolutionBackward0]
	140571584679696 -> 140571584680080
	140571584679696 [label=ReluBackward0]
	140571584679216 -> 140571584679696
	140571584679216 [label=NativeBatchNormBackward0]
	140571584679168 -> 140571584679216
	140571584679168 [label=ConvolutionBackward0]
	140571584680416 -> 140571584679168
	140571584680416 [label=ReluBackward0]
	140571584678496 -> 140571584680416
	140571584678496 [label=AddBackward0]
	140571584678208 -> 140571584678496
	140571584678208 [label=AddBackward0]
	140571584677728 -> 140571584678208
	140571584677728 [label=ReluBackward0]
	140571584677536 -> 140571584677728
	140571584677536 [label=AddBackward0]
	140571584677200 -> 140571584677536
	140571584677200 [label=NativeBatchNormBackward0]
	140571584677008 -> 140571584677200
	140571584677008 [label=ConvolutionBackward0]
	140571584676384 -> 140571584677008
	140571584676384 [label=ReluBackward0]
	140571584676192 -> 140571584676384
	140571584676192 [label=NativeBatchNormBackward0]
	140571584675904 -> 140571584676192
	140571584675904 [label=ConvolutionBackward0]
	140571584677392 -> 140571584675904
	140571584677392 [label=ReluBackward0]
	140571584544048 -> 140571584677392
	140571584544048 [label=AddBackward0]
	140571584544000 -> 140571584544048
	140571584544000 [label=NativeBatchNormBackward0]
	140571584543568 -> 140571584544000
	140571584543568 [label=ConvolutionBackward0]
	140571584543184 -> 140571584543568
	140571584543184 [label=ReluBackward0]
	140571584542704 -> 140571584543184
	140571584542704 [label=NativeBatchNormBackward0]
	140571584542656 -> 140571584542704
	140571584542656 [label=ConvolutionBackward0]
	140571584543904 -> 140571584542656
	140571584543904 [label=ReluBackward0]
	140571584541984 -> 140571584543904
	140571584541984 [label=AddBackward0]
	140571584541696 -> 140571584541984
	140571584541696 [label=NativeBatchNormBackward0]
	140571584541216 -> 140571584541696
	140571584541216 [label=ConvolutionBackward0]
	140571584540880 -> 140571584541216
	140571584540880 [label=ReluBackward0]
	140571584540640 -> 140571584540880
	140571584540640 [label=NativeBatchNormBackward0]
	140571584540352 -> 140571584540640
	140571584540352 [label=ConvolutionBackward0]
	140571584541840 -> 140571584540352
	140571584541840 [label=ReluBackward0]
	140571584539680 -> 140571584541840
	140571584539680 [label=AddBackward0]
	140571584539344 -> 140571584539680
	140571584539344 [label=NativeBatchNormBackward0]
	140571584539152 -> 140571584539344
	140571584539152 [label=ConvolutionBackward0]
	140571584538528 -> 140571584539152
	140571584538528 [label=ReluBackward0]
	140571584538336 -> 140571584538528
	140571584538336 [label=NativeBatchNormBackward0]
	140571584538000 -> 140571584538336
	140571584538000 [label=ConvolutionBackward0]
	140571584539536 -> 140571584538000
	140571584539536 [label=ReluBackward0]
	140571584537328 -> 140571584539536
	140571584537328 [label=AddBackward0]
	140571584537280 -> 140571584537328
	140571584537280 [label=ReluBackward0]
	140571584536848 -> 140571584537280
	140571584536848 [label=AddBackward0]
	140571584536512 -> 140571584536848
	140571584536512 [label=NativeBatchNormBackward0]
	140571584536320 -> 140571584536512
	140571584536320 [label=ConvolutionBackward0]
	140571584535936 -> 140571584536320
	140571584535936 [label=ReluBackward0]
	140571584535504 -> 140571584535936
	140571584535504 [label=NativeBatchNormBackward0]
	140571584535168 -> 140571584535504
	140571584535168 [label=ConvolutionBackward0]
	140571584536656 -> 140571584535168
	140571584536656 [label=ReluBackward0]
	140571584534496 -> 140571584536656
	140571584534496 [label=AddBackward0]
	140571584534448 -> 140571584534496
	140571584534448 [label=NativeBatchNormBackward0]
	140571584533968 -> 140571584534448
	140571584533968 [label=ConvolutionBackward0]
	140571584533632 -> 140571584533968
	140571584533632 [label=ReluBackward0]
	140571584533152 -> 140571584533632
	140571584533152 [label=NativeBatchNormBackward0]
	140571584533104 -> 140571584533152
	140571584533104 [label=ConvolutionBackward0]
	140571584534592 -> 140571584533104
	140571584534592 [label=ReluBackward0]
	140571584532432 -> 140571584534592
	140571584532432 [label=AddBackward0]
	140571584532144 -> 140571584532432
	140571584532144 [label=NativeBatchNormBackward0]
	140571584531904 -> 140571584532144
	140571584531904 [label=ConvolutionBackward0]
	140571584531280 -> 140571584531904
	140571584531280 [label=ReluBackward0]
	140571584531088 -> 140571584531280
	140571584531088 [label=NativeBatchNormBackward0]
	140571584530800 -> 140571584531088
	140571584530800 [label=ConvolutionBackward0]
	140571584532288 -> 140571584530800
	140571584532288 [label=ReluBackward0]
	140571584530128 -> 140571584532288
	140571584530128 [label=AddBackward0]
	140571584529792 -> 140571584530128
	140571584529792 [label=NativeBatchNormBackward0]
	140571584529600 -> 140571584529792
	140571584529600 [label=ConvolutionBackward0]
	140571584529216 -> 140571584529600
	140571584529216 [label=ReluBackward0]
	140571584528784 -> 140571584529216
	140571584528784 [label=NativeBatchNormBackward0]
	140571584528448 -> 140571584528784
	140571584528448 [label=ConvolutionBackward0]
	140571584529936 -> 140571584528448
	140571584529936 [label=ReluBackward0]
	140571584363872 -> 140571584529936
	140571584363872 [label=NativeBatchNormBackward0]
	140571584363824 -> 140571584363872
	140571584363824 [label=ConvolutionBackward0]
	140571584363200 -> 140571584363824
	140571584363200 [label=ReluBackward0]
	140571584363008 -> 140571584363200
	140571584363008 [label=AddBackward0]
	140571584362672 -> 140571584363008
	140571584362672 [label=NativeBatchNormBackward0]
	140571584362480 -> 140571584362672
	140571584362480 [label=ConvolutionBackward0]
	140571584361856 -> 140571584362480
	140571584361856 [label=ReluBackward0]
	140571584361664 -> 140571584361856
	140571584361664 [label=NativeBatchNormBackward0]
	140571584361328 -> 140571584361664
	140571584361328 [label=ConvolutionBackward0]
	140571584360992 -> 140571584361328
	140571584360992 [label=ReluBackward0]
	140571584360512 -> 140571584360992
	140571584360512 [label=NativeBatchNormBackward0]
	140571584360464 -> 140571584360512
	140571584360464 [label=ConvolutionBackward0]
	140571584362864 -> 140571584360464
	140571584362864 [label=ReluBackward0]
	140571584359792 -> 140571584362864
	140571584359792 [label=AddBackward0]
	140571584359504 -> 140571584359792
	140571584359504 [label=NativeBatchNormBackward0]
	140571584359264 -> 140571584359504
	140571584359264 [label=ConvolutionBackward0]
	140571584358640 -> 140571584359264
	140571584358640 [label=ReluBackward0]
	140571584358448 -> 140571584358640
	140571584358448 [label=NativeBatchNormBackward0]
	140571584358160 -> 140571584358448
	140571584358160 [label=ConvolutionBackward0]
	140571584357776 -> 140571584358160
	140571584357776 [label=ReluBackward0]
	140571584357296 -> 140571584357776
	140571584357296 [label=NativeBatchNormBackward0]
	140571584357248 -> 140571584357296
	140571584357248 [label=ConvolutionBackward0]
	140571584359648 -> 140571584357248
	140571584359648 [label=ReluBackward0]
	140571584356576 -> 140571584359648
	140571584356576 [label=AddBackward0]
	140571584356288 -> 140571584356576
	140571584356288 [label=NativeBatchNormBackward0]
	140571584355808 -> 140571584356288
	140571584355808 [label=ConvolutionBackward0]
	140571584355472 -> 140571584355808
	140571584355472 [label=ReluBackward0]
	140571584355232 -> 140571584355472
	140571584355232 [label=NativeBatchNormBackward0]
	140571584354944 -> 140571584355232
	140571584354944 [label=ConvolutionBackward0]
	140571584354560 -> 140571584354944
	140571584354560 [label=ReluBackward0]
	140571584354128 -> 140571584354560
	140571584354128 [label=NativeBatchNormBackward0]
	140571584353792 -> 140571584354128
	140571584353792 [label=ConvolutionBackward0]
	140571584356432 -> 140571584353792
	140571584356432 [label=ReluBackward0]
	140571584353120 -> 140571584356432
	140571584353120 [label=AddBackward0]
	140571584353072 -> 140571584353120
	140571584353072 [label=NativeBatchNormBackward0]
	140571584352592 -> 140571584353072
	140571584352592 [label=ConvolutionBackward0]
	140571584352256 -> 140571584352592
	140571584352256 [label=ReluBackward0]
	140571584351776 -> 140571584352256
	140571584351776 [label=NativeBatchNormBackward0]
	140571584351728 -> 140571584351776
	140571584351728 [label=ConvolutionBackward0]
	140571584351104 -> 140571584351728
	140571584351104 [label=ReluBackward0]
	140571584350912 -> 140571584351104
	140571584350912 [label=NativeBatchNormBackward0]
	140571584350576 -> 140571584350912
	140571584350576 [label=ConvolutionBackward0]
	140571584350240 -> 140571584350576
	140571584350240 [label=ReluBackward0]
	140571584349760 -> 140571584350240
	140571584349760 [label=NativeBatchNormBackward0]
	140571584349712 -> 140571584349760
	140571584349712 [label=ConvolutionBackward0]
	140571584349088 -> 140571584349712
	140571584349088 [label=ReluBackward0]
	140571584348896 -> 140571584349088
	140571584348896 [label=NativeBatchNormBackward0]
	140571584348560 -> 140571584348896
	140571584348560 [label=ConvolutionBackward0]
	140571584348224 -> 140571584348560
	140571942837600 [label="conv1.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	140571942837600 -> 140571584348224
	140571584348224 [label=AccumulateGrad]
	140571584348752 -> 140571584348896
	140571942837760 [label="bn1.weight
 (64)" fillcolor=lightblue]
	140571942837760 -> 140571584348752
	140571584348752 [label=AccumulateGrad]
	140571584349184 -> 140571584348896
	140571942837440 [label="bn1.bias
 (64)" fillcolor=lightblue]
	140571942837440 -> 140571584349184
	140571584349184 [label=AccumulateGrad]
	140571584349232 -> 140571584349712
	140571942830160 [label="conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140571942830160 -> 140571584349232
	140571584349232 [label=AccumulateGrad]
	140571584349856 -> 140571584349760
	140571942836880 [label="bn2.weight
 (64)" fillcolor=lightblue]
	140571942836880 -> 140571584349856
	140571584349856 [label=AccumulateGrad]
	140571584350096 -> 140571584349760
	140571942836800 [label="bn2.bias
 (64)" fillcolor=lightblue]
	140571942836800 -> 140571584350096
	140571584350096 [label=AccumulateGrad]
	140571584350384 -> 140571584350576
	140571942836000 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	140571942836000 -> 140571584350384
	140571584350384 [label=AccumulateGrad]
	140571584350768 -> 140571584350912
	140571942835920 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	140571942835920 -> 140571584350768
	140571584350768 [label=AccumulateGrad]
	140571584351200 -> 140571584350912
	140571942835840 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	140571942835840 -> 140571584351200
	140571584351200 [label=AccumulateGrad]
	140571584351248 -> 140571584351728
	140571942835280 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140571942835280 -> 140571584351248
	140571584351248 [label=AccumulateGrad]
	140571584351872 -> 140571584351776
	140571942835200 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	140571942835200 -> 140571584351872
	140571584351872 [label=AccumulateGrad]
	140571584352112 -> 140571584351776
	140571942835120 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	140571942835120 -> 140571584352112
	140571584352112 [label=AccumulateGrad]
	140571584352400 -> 140571584352592
	140571942834480 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	140571942834480 -> 140571584352400
	140571584352400 [label=AccumulateGrad]
	140571584352784 -> 140571584353072
	140571942834400 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	140571942834400 -> 140571584352784
	140571584352784 [label=AccumulateGrad]
	140571584352928 -> 140571584353072
	140571942834320 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	140571942834320 -> 140571584352928
	140571584352928 [label=AccumulateGrad]
	140571584353216 -> 140571584353120
	140571584353216 [label=NativeBatchNormBackward0]
	140571584351056 -> 140571584353216
	140571584351056 [label=ConvolutionBackward0]
	140571584350240 -> 140571584351056
	140571584351440 -> 140571584351056
	140571942836480 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	140571942836480 -> 140571584351440
	140571584351440 [label=AccumulateGrad]
	140571584351920 -> 140571584353216
	140571942836400 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	140571942836400 -> 140571584351920
	140571584351920 [label=AccumulateGrad]
	140571584352448 -> 140571584353216
	140571942836320 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	140571942836320 -> 140571584352448
	140571584352448 [label=AccumulateGrad]
	140571584353456 -> 140571584353792
	140571942832720 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	140571942832720 -> 140571584353456
	140571584353456 [label=AccumulateGrad]
	140571584353936 -> 140571584354128
	140571942832640 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	140571942832640 -> 140571584353936
	140571584353936 [label=AccumulateGrad]
	140571584354416 -> 140571584354128
	140571942839440 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	140571942839440 -> 140571584354416
	140571584354416 [label=AccumulateGrad]
	140571584354464 -> 140571584354944
	140571942817056 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140571942817056 -> 140571584354464
	140571584354464 [label=AccumulateGrad]
	140571584355088 -> 140571584355232
	140571942816976 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	140571942816976 -> 140571584355088
	140571584355088 [label=AccumulateGrad]
	140571584355280 -> 140571584355232
	140571942825856 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	140571942825856 -> 140571584355280
	140571584355280 [label=AccumulateGrad]
	140571584355616 -> 140571584355808
	140571942825616 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	140571942825616 -> 140571584355616
	140571584355616 [label=AccumulateGrad]
	140571584355952 -> 140571584356288
	140571942816416 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	140571942816416 -> 140571584355952
	140571584355952 [label=AccumulateGrad]
	140571584356144 -> 140571584356288
	140571942816576 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	140571942816576 -> 140571584356144
	140571584356144 [label=AccumulateGrad]
	140571584356432 -> 140571584356576
	140571584356624 -> 140571584357248
	140571942816256 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	140571942816256 -> 140571584356624
	140571584356624 [label=AccumulateGrad]
	140571584357152 -> 140571584357296
	140571942825376 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	140571942825376 -> 140571584357152
	140571584357152 [label=AccumulateGrad]
	140571584357632 -> 140571584357296
	140571942825296 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	140571942825296 -> 140571584357632
	140571584357632 [label=AccumulateGrad]
	140571584357920 -> 140571584358160
	140571942815696 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140571942815696 -> 140571584357920
	140571584357920 [label=AccumulateGrad]
	140571584358304 -> 140571584358448
	140571942815856 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	140571942815856 -> 140571584358304
	140571584358304 [label=AccumulateGrad]
	140571584358496 -> 140571584358448
	140571942815776 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	140571942815776 -> 140571584358496
	140571584358496 [label=AccumulateGrad]
	140571584358832 -> 140571584359264
	140571942824736 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	140571942824736 -> 140571584358832
	140571584358832 [label=AccumulateGrad]
	140571584359168 -> 140571584359504
	140571942824656 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	140571942824656 -> 140571584359168
	140571584359168 [label=AccumulateGrad]
	140571584359312 -> 140571584359504
	140571942815456 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	140571942815456 -> 140571584359312
	140571584359312 [label=AccumulateGrad]
	140571584359648 -> 140571584359792
	140571584359840 -> 140571584360464
	140571942814976 [label="layer1.3.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	140571942814976 -> 140571584359840
	140571584359840 [label=AccumulateGrad]
	140571584360608 -> 140571584360512
	140571942815136 [label="layer1.3.bn1.weight
 (64)" fillcolor=lightblue]
	140571942815136 -> 140571584360608
	140571584360608 [label=AccumulateGrad]
	140571584360848 -> 140571584360512
	140571942815056 [label="layer1.3.bn1.bias
 (64)" fillcolor=lightblue]
	140571942815056 -> 140571584360848
	140571584360848 [label=AccumulateGrad]
	140571584361136 -> 140571584361328
	140571942824416 [label="layer1.3.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140571942824416 -> 140571584361136
	140571584361136 [label=AccumulateGrad]
	140571584361520 -> 140571584361664
	140571942824336 [label="layer1.3.bn2.weight
 (64)" fillcolor=lightblue]
	140571942824336 -> 140571584361520
	140571584361520 [label=AccumulateGrad]
	140571584361952 -> 140571584361664
	140571942814496 [label="layer1.3.bn2.bias
 (64)" fillcolor=lightblue]
	140571942814496 -> 140571584361952
	140571584361952 [label=AccumulateGrad]
	140571584362000 -> 140571584362480
	140571942814416 [label="layer1.3.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	140571942814416 -> 140571584362000
	140571584362000 [label=AccumulateGrad]
	140571584362624 -> 140571584362672
	140571942814336 [label="layer1.3.bn3.weight
 (256)" fillcolor=lightblue]
	140571942814336 -> 140571584362624
	140571584362624 [label=AccumulateGrad]
	140571584362528 -> 140571584362672
	140571942824096 [label="layer1.3.bn3.bias
 (256)" fillcolor=lightblue]
	140571942824096 -> 140571584362528
	140571584362528 [label=AccumulateGrad]
	140571584362864 -> 140571584363008
	140571584363344 -> 140571584363824
	140571942823856 [label="transition1.0.0.weight
 (18, 256, 3, 3)" fillcolor=lightblue]
	140571942823856 -> 140571584363344
	140571584363344 [label=AccumulateGrad]
	140571584363968 -> 140571584363872
	140571942813776 [label="transition1.0.1.weight
 (18)" fillcolor=lightblue]
	140571942813776 -> 140571584363968
	140571584363968 [label=AccumulateGrad]
	140571584364496 -> 140571584363872
	140571942813936 [label="transition1.0.1.bias
 (18)" fillcolor=lightblue]
	140571942813936 -> 140571584364496
	140571584364496 [label=AccumulateGrad]
	140571584364208 -> 140571584528448
	140571942823296 [label="stage2.0.branches.0.0.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571942823296 -> 140571584364208
	140571584364208 [label=AccumulateGrad]
	140571584528592 -> 140571584528784
	140571942823216 [label="stage2.0.branches.0.0.bn1.weight
 (18)" fillcolor=lightblue]
	140571942823216 -> 140571584528592
	140571584528592 [label=AccumulateGrad]
	140571584529072 -> 140571584528784
	140571942829616 [label="stage2.0.branches.0.0.bn1.bias
 (18)" fillcolor=lightblue]
	140571942829616 -> 140571584529072
	140571584529072 [label=AccumulateGrad]
	140571584529120 -> 140571584529600
	140571942822976 [label="stage2.0.branches.0.0.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571942822976 -> 140571584529120
	140571584529120 [label=AccumulateGrad]
	140571584529744 -> 140571584529792
	140571942822896 [label="stage2.0.branches.0.0.bn2.weight
 (18)" fillcolor=lightblue]
	140571942822896 -> 140571584529744
	140571584529744 [label=AccumulateGrad]
	140571584529888 -> 140571584529792
	140571942829296 [label="stage2.0.branches.0.0.bn2.bias
 (18)" fillcolor=lightblue]
	140571942829296 -> 140571584529888
	140571584529888 [label=AccumulateGrad]
	140571584529936 -> 140571584530128
	140571584530416 -> 140571584530800
	140571942829056 [label="stage2.0.branches.0.1.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571942829056 -> 140571584530416
	140571584530416 [label=AccumulateGrad]
	140571584530944 -> 140571584531088
	140571942822656 [label="stage2.0.branches.0.1.bn1.weight
 (18)" fillcolor=lightblue]
	140571942822656 -> 140571584530944
	140571584530944 [label=AccumulateGrad]
	140571584531136 -> 140571584531088
	140571942822576 [label="stage2.0.branches.0.1.bn1.bias
 (18)" fillcolor=lightblue]
	140571942822576 -> 140571584531136
	140571584531136 [label=AccumulateGrad]
	140571584531472 -> 140571584531904
	140571942828896 [label="stage2.0.branches.0.1.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571942828896 -> 140571584531472
	140571584531472 [label=AccumulateGrad]
	140571584531808 -> 140571584532144
	140571942822496 [label="stage2.0.branches.0.1.bn2.weight
 (18)" fillcolor=lightblue]
	140571942822496 -> 140571584531808
	140571584531808 [label=AccumulateGrad]
	140571584531952 -> 140571584532144
	140571942822416 [label="stage2.0.branches.0.1.bn2.bias
 (18)" fillcolor=lightblue]
	140571942822416 -> 140571584531952
	140571584531952 [label=AccumulateGrad]
	140571584532288 -> 140571584532432
	140571584532480 -> 140571584533104
	140571942828496 [label="stage2.0.branches.0.2.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571942828496 -> 140571584532480
	140571584532480 [label=AccumulateGrad]
	140571584533248 -> 140571584533152
	140571942828416 [label="stage2.0.branches.0.2.bn1.weight
 (18)" fillcolor=lightblue]
	140571942828416 -> 140571584533248
	140571584533248 [label=AccumulateGrad]
	140571584533488 -> 140571584533152
	140571942822016 [label="stage2.0.branches.0.2.bn1.bias
 (18)" fillcolor=lightblue]
	140571942822016 -> 140571584533488
	140571584533488 [label=AccumulateGrad]
	140571584533776 -> 140571584533968
	140571942828016 [label="stage2.0.branches.0.2.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571942828016 -> 140571584533776
	140571584533776 [label=AccumulateGrad]
	140571584534160 -> 140571584534448
	140571942821776 [label="stage2.0.branches.0.2.bn2.weight
 (18)" fillcolor=lightblue]
	140571942821776 -> 140571584534160
	140571584534160 [label=AccumulateGrad]
	140571584534304 -> 140571584534448
	140571942827936 [label="stage2.0.branches.0.2.bn2.bias
 (18)" fillcolor=lightblue]
	140571942827936 -> 140571584534304
	140571584534304 [label=AccumulateGrad]
	140571584534592 -> 140571584534496
	140571584534832 -> 140571584535168
	140571942821296 [label="stage2.0.branches.0.3.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571942821296 -> 140571584534832
	140571584534832 [label=AccumulateGrad]
	140571584535312 -> 140571584535504
	140571942827696 [label="stage2.0.branches.0.3.bn1.weight
 (18)" fillcolor=lightblue]
	140571942827696 -> 140571584535312
	140571584535312 [label=AccumulateGrad]
	140571584535792 -> 140571584535504
	140571942827616 [label="stage2.0.branches.0.3.bn1.bias
 (18)" fillcolor=lightblue]
	140571942827616 -> 140571584535792
	140571584535792 [label=AccumulateGrad]
	140571584535840 -> 140571584536320
	140571942820976 [label="stage2.0.branches.0.3.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571942820976 -> 140571584535840
	140571584535840 [label=AccumulateGrad]
	140571584536464 -> 140571584536512
	140571942827376 [label="stage2.0.branches.0.3.bn2.weight
 (18)" fillcolor=lightblue]
	140571942827376 -> 140571584536464
	140571584536464 [label=AccumulateGrad]
	140571584536608 -> 140571584536512
	140571942827296 [label="stage2.0.branches.0.3.bn2.bias
 (18)" fillcolor=lightblue]
	140571942827296 -> 140571584536608
	140571584536608 [label=AccumulateGrad]
	140571584536656 -> 140571584536848
	140571584537184 -> 140571584537328
	140571584537184 [label=UpsampleBilinear2DBackward0]
	140571584536176 -> 140571584537184
	140571584536176 [label=NativeBatchNormBackward0]
	140571584534640 -> 140571584536176
	140571584534640 [label=ConvolutionBackward0]
	140571584534976 -> 140571584534640
	140571584534976 [label=ReluBackward0]
	140571584532576 -> 140571584534976
	140571584532576 [label=AddBackward0]
	140571584532960 -> 140571584532576
	140571584532960 [label=NativeBatchNormBackward0]
	140571584532816 -> 140571584532960
	140571584532816 [label=ConvolutionBackward0]
	140571584530608 -> 140571584532816
	140571584530608 [label=ReluBackward0]
	140571584530464 -> 140571584530608
	140571584530464 [label=NativeBatchNormBackward0]
	140571584528544 -> 140571584530464
	140571584528544 [label=ConvolutionBackward0]
	140571584533920 -> 140571584528544
	140571584533920 [label=ReluBackward0]
	140571584362336 -> 140571584533920
	140571584362336 [label=AddBackward0]
	140571584363296 -> 140571584362336
	140571584363296 [label=NativeBatchNormBackward0]
	140571584362192 -> 140571584363296
	140571584362192 [label=ConvolutionBackward0]
	140571584360320 -> 140571584362192
	140571584360320 [label=ReluBackward0]
	140571584360176 -> 140571584360320
	140571584360176 [label=NativeBatchNormBackward0]
	140571584357488 -> 140571584360176
	140571584357488 [label=ConvolutionBackward0]
	140571584363536 -> 140571584357488
	140571584363536 [label=ReluBackward0]
	140571584355904 -> 140571584363536
	140571584355904 [label=AddBackward0]
	140571584356960 -> 140571584355904
	140571584356960 [label=NativeBatchNormBackward0]
	140571584355760 -> 140571584356960
	140571584355760 [label=ConvolutionBackward0]
	140571584353888 -> 140571584355760
	140571584353888 [label=ReluBackward0]
	140571584353744 -> 140571584353888
	140571584353744 [label=NativeBatchNormBackward0]
	140571584349040 -> 140571584353744
	140571584349040 [label=ConvolutionBackward0]
	140571584356816 -> 140571584349040
	140571584356816 [label=ReluBackward0]
	140571584349424 -> 140571584356816
	140571584349424 [label=AddBackward0]
	140571584348512 -> 140571584349424
	140571584348512 [label=NativeBatchNormBackward0]
	140571584232992 -> 140571584348512
	140571584232992 [label=ConvolutionBackward0]
	140571584232656 -> 140571584232992
	140571584232656 [label=ReluBackward0]
	140571584232416 -> 140571584232656
	140571584232416 [label=NativeBatchNormBackward0]
	140571584232128 -> 140571584232416
	140571584232128 [label=ConvolutionBackward0]
	140571584348416 -> 140571584232128
	140571584348416 [label=ReluBackward0]
	140571584231456 -> 140571584348416
	140571584231456 [label=NativeBatchNormBackward0]
	140571584231120 -> 140571584231456
	140571584231120 [label=ConvolutionBackward0]
	140571584363200 -> 140571584231120
	140571584230784 -> 140571584231120
	140571942823536 [label="transition1.1.0.0.weight
 (36, 256, 3, 3)" fillcolor=lightblue]
	140571942823536 -> 140571584230784
	140571584230784 [label=AccumulateGrad]
	140571584231312 -> 140571584231456
	140571942829936 [label="transition1.1.0.1.weight
 (36)" fillcolor=lightblue]
	140571942829936 -> 140571584231312
	140571584231312 [label=AccumulateGrad]
	140571584231792 -> 140571584231456
	140571942829856 [label="transition1.1.0.1.bias
 (36)" fillcolor=lightblue]
	140571942829856 -> 140571584231792
	140571584231792 [label=AccumulateGrad]
	140571584231744 -> 140571584232128
	140571942820656 [label="stage2.0.branches.1.0.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571942820656 -> 140571584231744
	140571584231744 [label=AccumulateGrad]
	140571584232272 -> 140571584232416
	140571942828176 [label="stage2.0.branches.1.0.bn1.weight
 (36)" fillcolor=lightblue]
	140571942828176 -> 140571584232272
	140571584232272 [label=AccumulateGrad]
	140571584232464 -> 140571584232416
	140571942828096 [label="stage2.0.branches.1.0.bn1.bias
 (36)" fillcolor=lightblue]
	140571942828096 -> 140571584232464
	140571584232464 [label=AccumulateGrad]
	140571584232800 -> 140571584232992
	140571942826256 [label="stage2.0.branches.1.0.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571942826256 -> 140571584232800
	140571584232800 [label=AccumulateGrad]
	140571584233328 -> 140571584348512
	140571942826176 [label="stage2.0.branches.1.0.bn2.weight
 (36)" fillcolor=lightblue]
	140571942826176 -> 140571584233328
	140571584233328 [label=AccumulateGrad]
	140571584233136 -> 140571584348512
	140571869670736 [label="stage2.0.branches.1.0.bn2.bias
 (36)" fillcolor=lightblue]
	140571869670736 -> 140571584233136
	140571584233136 [label=AccumulateGrad]
	140571584348416 -> 140571584349424
	140571584349568 -> 140571584349040
	140571869670336 [label="stage2.0.branches.1.1.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571869670336 -> 140571584349568
	140571584349568 [label=AccumulateGrad]
	140571584351584 -> 140571584353744
	140571869675376 [label="stage2.0.branches.1.1.bn1.weight
 (36)" fillcolor=lightblue]
	140571869675376 -> 140571584351584
	140571584351584 [label=AccumulateGrad]
	140571584352544 -> 140571584353744
	140571869675296 [label="stage2.0.branches.1.1.bn1.bias
 (36)" fillcolor=lightblue]
	140571869675296 -> 140571584352544
	140571584352544 [label=AccumulateGrad]
	140571584354608 -> 140571584355760
	140571869667136 [label="stage2.0.branches.1.1.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571869667136 -> 140571584354608
	140571584354608 [label=AccumulateGrad]
	140571584354272 -> 140571584356960
	140571869667056 [label="stage2.0.branches.1.1.bn2.weight
 (36)" fillcolor=lightblue]
	140571869667056 -> 140571584354272
	140571584354272 [label=AccumulateGrad]
	140571584355136 -> 140571584356960
	140571869672816 [label="stage2.0.branches.1.1.bn2.bias
 (36)" fillcolor=lightblue]
	140571869672816 -> 140571584355136
	140571584355136 [label=AccumulateGrad]
	140571584356816 -> 140571584355904
	140571584357824 -> 140571584357488
	140571869674896 [label="stage2.0.branches.1.2.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571869674896 -> 140571584357824
	140571584357824 [label=AccumulateGrad]
	140571584358592 -> 140571584360176
	140571869674816 [label="stage2.0.branches.1.2.bn1.weight
 (36)" fillcolor=lightblue]
	140571869674816 -> 140571584358592
	140571584358592 [label=AccumulateGrad]
	140571584359120 -> 140571584360176
	140571869816512 [label="stage2.0.branches.1.2.bn1.bias
 (36)" fillcolor=lightblue]
	140571869816512 -> 140571584359120
	140571584359120 [label=AccumulateGrad]
	140571584361280 -> 140571584362192
	140571869810592 [label="stage2.0.branches.1.2.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571869810592 -> 140571584361280
	140571584361280 [label=AccumulateGrad]
	140571584360656 -> 140571584363296
	140571869810512 [label="stage2.0.branches.1.2.bn2.weight
 (36)" fillcolor=lightblue]
	140571869810512 -> 140571584360656
	140571584360656 [label=AccumulateGrad]
	140571584361808 -> 140571584363296
	140571869816192 [label="stage2.0.branches.1.2.bn2.bias
 (36)" fillcolor=lightblue]
	140571869816192 -> 140571584361808
	140571584361808 [label=AccumulateGrad]
	140571584363536 -> 140571584362336
	140571584364352 -> 140571584528544
	140571869810272 [label="stage2.0.branches.1.3.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571869810272 -> 140571584364352
	140571584364352 [label=AccumulateGrad]
	140571584528928 -> 140571584530464
	140571869810192 [label="stage2.0.branches.1.3.bn1.weight
 (36)" fillcolor=lightblue]
	140571869810192 -> 140571584528928
	140571584528928 [label=AccumulateGrad]
	140571584529456 -> 140571584530464
	140571869815872 [label="stage2.0.branches.1.3.bn1.bias
 (36)" fillcolor=lightblue]
	140571869815872 -> 140571584529456
	140571584529456 [label=AccumulateGrad]
	140571584531616 -> 140571584532816
	140571869809952 [label="stage2.0.branches.1.3.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571869809952 -> 140571584531616
	140571584531616 [label=AccumulateGrad]
	140571584532624 -> 140571584532960
	140571869809872 [label="stage2.0.branches.1.3.bn2.weight
 (36)" fillcolor=lightblue]
	140571869809872 -> 140571584532624
	140571584532624 [label=AccumulateGrad]
	140571584531760 -> 140571584532960
	140571869815472 [label="stage2.0.branches.1.3.bn2.bias
 (36)" fillcolor=lightblue]
	140571869815472 -> 140571584531760
	140571584531760 [label=AccumulateGrad]
	140571584533920 -> 140571584532576
	140571584533824 -> 140571584534640
	140571869809632 [label="stage2.0.fuse_layers.0.1.0.weight
 (18, 36, 1, 1)" fillcolor=lightblue]
	140571869809632 -> 140571584533824
	140571584533824 [label=AccumulateGrad]
	140571584535648 -> 140571584536176
	140571869815232 [label="stage2.0.fuse_layers.0.1.1.weight
 (18)" fillcolor=lightblue]
	140571869815232 -> 140571584535648
	140571584535648 [label=AccumulateGrad]
	140571584536992 -> 140571584536176
	140571869815152 [label="stage2.0.fuse_layers.0.1.1.bias
 (18)" fillcolor=lightblue]
	140571869815152 -> 140571584536992
	140571584536992 [label=AccumulateGrad]
	140571584537664 -> 140571584538000
	140571869808512 [label="stage3.0.branches.0.0.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571869808512 -> 140571584537664
	140571584537664 [label=AccumulateGrad]
	140571584538192 -> 140571584538336
	140571869814112 [label="stage3.0.branches.0.0.bn1.weight
 (18)" fillcolor=lightblue]
	140571869814112 -> 140571584538192
	140571584538192 [label=AccumulateGrad]
	140571584538624 -> 140571584538336
	140571869814032 [label="stage3.0.branches.0.0.bn1.bias
 (18)" fillcolor=lightblue]
	140571869814032 -> 140571584538624
	140571584538624 [label=AccumulateGrad]
	140571584538672 -> 140571584539152
	140571869808272 [label="stage3.0.branches.0.0.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571869808272 -> 140571584538672
	140571584538672 [label=AccumulateGrad]
	140571584539296 -> 140571584539344
	140571869808192 [label="stage3.0.branches.0.0.bn2.weight
 (18)" fillcolor=lightblue]
	140571869808192 -> 140571584539296
	140571584539296 [label=AccumulateGrad]
	140571584539200 -> 140571584539344
	140571869813792 [label="stage3.0.branches.0.0.bn2.bias
 (18)" fillcolor=lightblue]
	140571869813792 -> 140571584539200
	140571584539200 [label=AccumulateGrad]
	140571584539536 -> 140571584539680
	140571584539968 -> 140571584540352
	140571869813552 [label="stage3.0.branches.0.1.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571869813552 -> 140571584539968
	140571584539968 [label=AccumulateGrad]
	140571584540496 -> 140571584540640
	140571869807952 [label="stage3.0.branches.0.1.bn1.weight
 (18)" fillcolor=lightblue]
	140571869807952 -> 140571584540496
	140571584540496 [label=AccumulateGrad]
	140571584540688 -> 140571584540640
	140571869807872 [label="stage3.0.branches.0.1.bn1.bias
 (18)" fillcolor=lightblue]
	140571869807872 -> 140571584540688
	140571584540688 [label=AccumulateGrad]
	140571584541024 -> 140571584541216
	140571869813232 [label="stage3.0.branches.0.1.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571869813232 -> 140571584541024
	140571584541024 [label=AccumulateGrad]
	140571584541360 -> 140571584541696
	140571869807632 [label="stage3.0.branches.0.1.bn2.weight
 (18)" fillcolor=lightblue]
	140571869807632 -> 140571584541360
	140571584541360 [label=AccumulateGrad]
	140571584541552 -> 140571584541696
	140571869807552 [label="stage3.0.branches.0.1.bn2.bias
 (18)" fillcolor=lightblue]
	140571869807552 -> 140571584541552
	140571584541552 [label=AccumulateGrad]
	140571584541840 -> 140571584541984
	140571584542032 -> 140571584542656
	140571869813072 [label="stage3.0.branches.0.2.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571869813072 -> 140571584542032
	140571584542032 [label=AccumulateGrad]
	140571584542560 -> 140571584542704
	140571869807472 [label="stage3.0.branches.0.2.bn1.weight
 (18)" fillcolor=lightblue]
	140571869807472 -> 140571584542560
	140571584542560 [label=AccumulateGrad]
	140571584543040 -> 140571584542704
	140571869807392 [label="stage3.0.branches.0.2.bn1.bias
 (18)" fillcolor=lightblue]
	140571869807392 -> 140571584543040
	140571584543040 [label=AccumulateGrad]
	140571584543328 -> 140571584543568
	140571869812592 [label="stage3.0.branches.0.2.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571869812592 -> 140571584543328
	140571584543328 [label=AccumulateGrad]
	140571584543712 -> 140571584544000
	140571869806992 [label="stage3.0.branches.0.2.bn2.weight
 (18)" fillcolor=lightblue]
	140571869806992 -> 140571584543712
	140571584543712 [label=AccumulateGrad]
	140571584543856 -> 140571584544000
	140571869806912 [label="stage3.0.branches.0.2.bn2.bias
 (18)" fillcolor=lightblue]
	140571869806912 -> 140571584543856
	140571584543856 [label=AccumulateGrad]
	140571584543904 -> 140571584544048
	140571584544384 -> 140571584675904
	140571869812272 [label="stage3.0.branches.0.3.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571869812272 -> 140571584544384
	140571584544384 [label=AccumulateGrad]
	140571584676048 -> 140571584676192
	140571869806672 [label="stage3.0.branches.0.3.bn1.weight
 (18)" fillcolor=lightblue]
	140571869806672 -> 140571584676048
	140571584676048 [label=AccumulateGrad]
	140571584676480 -> 140571584676192
	140571869812032 [label="stage3.0.branches.0.3.bn1.bias
 (18)" fillcolor=lightblue]
	140571869812032 -> 140571584676480
	140571584676480 [label=AccumulateGrad]
	140571584676528 -> 140571584677008
	140571869814592 [label="stage3.0.branches.0.3.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571869814592 -> 140571584676528
	140571584676528 [label=AccumulateGrad]
	140571584677152 -> 140571584677200
	140571869814512 [label="stage3.0.branches.0.3.bn2.weight
 (18)" fillcolor=lightblue]
	140571869814512 -> 140571584677152
	140571584677152 [label=AccumulateGrad]
	140571584677056 -> 140571584677200
	140571869808912 [label="stage3.0.branches.0.3.bn2.bias
 (18)" fillcolor=lightblue]
	140571869808912 -> 140571584677056
	140571584677056 [label=AccumulateGrad]
	140571584677392 -> 140571584677536
	140571584677872 -> 140571584678208
	140571584677872 [label=UpsampleBilinear2DBackward0]
	140571584676864 -> 140571584677872
	140571584676864 [label=NativeBatchNormBackward0]
	140571584676336 -> 140571584676864
	140571584676336 [label=ConvolutionBackward0]
	140571584544528 -> 140571584676336
	140571584544528 [label=ReluBackward0]
	140571584541888 -> 140571584544528
	140571584541888 [label=AddBackward0]
	140571584542512 -> 140571584541888
	140571584542512 [label=NativeBatchNormBackward0]
	140571584542368 -> 140571584542512
	140571584542368 [label=ConvolutionBackward0]
	140571584540208 -> 140571584542368
	140571584540208 [label=ReluBackward0]
	140571584540016 -> 140571584540208
	140571584540016 [label=NativeBatchNormBackward0]
	140571584537520 -> 140571584540016
	140571584537520 [label=ConvolutionBackward0]
	140571584543232 -> 140571584537520
	140571584543232 [label=ReluBackward0]
	140571584535984 -> 140571584543232
	140571584535984 [label=AddBackward0]
	140571584531232 -> 140571584535984
	140571584531232 [label=NativeBatchNormBackward0]
	140571584530560 -> 140571584531232
	140571584530560 [label=ConvolutionBackward0]
	140571584530272 -> 140571584530560
	140571584530272 [label=ReluBackward0]
	140571584359984 -> 140571584530272
	140571584359984 [label=NativeBatchNormBackward0]
	140571584359936 -> 140571584359984
	140571584359936 [label=ConvolutionBackward0]
	140571584533296 -> 140571584359936
	140571584533296 [label=ReluBackward0]
	140571584348368 -> 140571584533296
	140571584348368 [label=AddBackward0]
	140571584349904 -> 140571584348368
	140571584349904 [label=NativeBatchNormBackward0]
	140571584350528 -> 140571584349904
	140571584350528 [label=ConvolutionBackward0]
	140571584231984 -> 140571584350528
	140571584231984 [label=ReluBackward0]
	140571584230976 -> 140571584231984
	140571584230976 [label=NativeBatchNormBackward0]
	140571584230928 -> 140571584230976
	140571584230928 [label=ConvolutionBackward0]
	140571584353264 -> 140571584230928
	140571584353264 [label=ReluBackward0]
	140571584229968 -> 140571584353264
	140571584229968 [label=AddBackward0]
	140571584229632 -> 140571584229968
	140571584229632 [label=NativeBatchNormBackward0]
	140571584229440 -> 140571584229632
	140571584229440 [label=ConvolutionBackward0]
	140571584229056 -> 140571584229440
	140571584229056 [label=ReluBackward0]
	140571584228624 -> 140571584229056
	140571584228624 [label=NativeBatchNormBackward0]
	140571584228288 -> 140571584228624
	140571584228288 [label=ConvolutionBackward0]
	140571584229776 -> 140571584228288
	140571584229776 [label=ReluBackward0]
	140571584227616 -> 140571584229776
	140571584227616 [label=AddBackward0]
	140571584227568 -> 140571584227616
	140571584227568 [label=NativeBatchNormBackward0]
	140571584227280 -> 140571584227568
	140571584227280 [label=ConvolutionBackward0]
	140571584537280 -> 140571584227280
	140571584226896 -> 140571584227280
	140571869809312 [label="stage2.0.fuse_layers.1.0.0.0.weight
 (36, 18, 3, 3)" fillcolor=lightblue]
	140571869809312 -> 140571584226896
	140571584226896 [label=AccumulateGrad]
	140571584227424 -> 140571584227568
	140571869814912 [label="stage2.0.fuse_layers.1.0.0.1.weight
 (36)" fillcolor=lightblue]
	140571869814912 -> 140571584227424
	140571584227424 [label=AccumulateGrad]
	140571584228240 -> 140571584227568
	140571869814832 [label="stage2.0.fuse_layers.1.0.0.1.bias
 (36)" fillcolor=lightblue]
	140571869814832 -> 140571584228240
	140571584228240 [label=AccumulateGrad]
	140571584534976 -> 140571584227616
	140571584227952 -> 140571584228288
	140571869810832 [label="stage3.0.branches.1.0.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571869810832 -> 140571584227952
	140571584227952 [label=AccumulateGrad]
	140571584228432 -> 140571584228624
	140571588960704 [label="stage3.0.branches.1.0.bn1.weight
 (36)" fillcolor=lightblue]
	140571588960704 -> 140571584228432
	140571584228432 [label=AccumulateGrad]
	140571584228912 -> 140571584228624
	140571588960544 [label="stage3.0.branches.1.0.bn1.bias
 (36)" fillcolor=lightblue]
	140571588960544 -> 140571584228912
	140571584228912 [label=AccumulateGrad]
	140571584228960 -> 140571584229440
	140571588961104 [label="stage3.0.branches.1.0.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571588961104 -> 140571584228960
	140571584228960 [label=AccumulateGrad]
	140571584229584 -> 140571584229632
	140571588961184 [label="stage3.0.branches.1.0.bn2.weight
 (36)" fillcolor=lightblue]
	140571588961184 -> 140571584229584
	140571584229584 [label=AccumulateGrad]
	140571584229728 -> 140571584229632
	140571588961264 [label="stage3.0.branches.1.0.bn2.bias
 (36)" fillcolor=lightblue]
	140571588961264 -> 140571584229728
	140571584229728 [label=AccumulateGrad]
	140571584229776 -> 140571584229968
	140571584230256 -> 140571584230928
	140571588961664 [label="stage3.0.branches.1.1.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571588961664 -> 140571584230256
	140571584230256 [label=AccumulateGrad]
	140571584230448 -> 140571584230976
	140571588961744 [label="stage3.0.branches.1.1.bn1.weight
 (36)" fillcolor=lightblue]
	140571588961744 -> 140571584230448
	140571584230448 [label=AccumulateGrad]
	140571584231072 -> 140571584230976
	140571588961904 [label="stage3.0.branches.1.1.bn1.bias
 (36)" fillcolor=lightblue]
	140571588961904 -> 140571584231072
	140571584231072 [label=AccumulateGrad]
	140571584232944 -> 140571584350528
	140571588962304 [label="stage3.0.branches.1.1.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571588962304 -> 140571584232944
	140571584232944 [label=AccumulateGrad]
	140571584350432 -> 140571584349904
	140571588962384 [label="stage3.0.branches.1.1.bn2.weight
 (36)" fillcolor=lightblue]
	140571588962384 -> 140571584350432
	140571584350432 [label=AccumulateGrad]
	140571584233088 -> 140571584349904
	140571588962544 [label="stage3.0.branches.1.1.bn2.bias
 (36)" fillcolor=lightblue]
	140571588962544 -> 140571584233088
	140571584233088 [label=AccumulateGrad]
	140571584353264 -> 140571584348368
	140571584357968 -> 140571584359936
	140571588963024 [label="stage3.0.branches.1.2.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571588963024 -> 140571584357968
	140571584357968 [label=AccumulateGrad]
	140571584357104 -> 140571584359984
	140571588963264 [label="stage3.0.branches.1.2.bn1.weight
 (36)" fillcolor=lightblue]
	140571588963264 -> 140571584357104
	140571584357104 [label=AccumulateGrad]
	140571584363152 -> 140571584359984
	140571588963184 [label="stage3.0.branches.1.2.bn1.bias
 (36)" fillcolor=lightblue]
	140571588963184 -> 140571584363152
	140571584363152 [label=AccumulateGrad]
	140571584361184 -> 140571584530560
	140571588963744 [label="stage3.0.branches.1.2.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571588963744 -> 140571584361184
	140571584361184 [label=AccumulateGrad]
	140571584535120 -> 140571584531232
	140571588963904 [label="stage3.0.branches.1.2.bn2.weight
 (36)" fillcolor=lightblue]
	140571588963904 -> 140571584535120
	140571584535120 [label=AccumulateGrad]
	140571584535264 -> 140571584531232
	140571588962864 [label="stage3.0.branches.1.2.bn2.bias
 (36)" fillcolor=lightblue]
	140571588962864 -> 140571584535264
	140571584535264 [label=AccumulateGrad]
	140571584533296 -> 140571584535984
	140571584537808 -> 140571584537520
	140571588964384 [label="stage3.0.branches.1.3.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571588964384 -> 140571584537808
	140571584537808 [label=AccumulateGrad]
	140571584538480 -> 140571584540016
	140571588964464 [label="stage3.0.branches.1.3.bn1.weight
 (36)" fillcolor=lightblue]
	140571588964464 -> 140571584538480
	140571584538480 [label=AccumulateGrad]
	140571584539008 -> 140571584540016
	140571588967184 [label="stage3.0.branches.1.3.bn1.bias
 (36)" fillcolor=lightblue]
	140571588967184 -> 140571584539008
	140571584539008 [label=AccumulateGrad]
	140571584541168 -> 140571584542368
	140571588963504 [label="stage3.0.branches.1.3.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571588963504 -> 140571584541168
	140571584541168 [label=AccumulateGrad]
	140571584542224 -> 140571584542512
	140571588964944 [label="stage3.0.branches.1.3.bn2.weight
 (36)" fillcolor=lightblue]
	140571588964944 -> 140571584542224
	140571584542224 [label=AccumulateGrad]
	140571584541312 -> 140571584542512
	140571588964624 [label="stage3.0.branches.1.3.bn2.bias
 (36)" fillcolor=lightblue]
	140571588964624 -> 140571584541312
	140571584541312 [label=AccumulateGrad]
	140571584543232 -> 140571584541888
	140571584543376 -> 140571584676336
	140571589051488 [label="stage3.0.fuse_layers.0.1.0.weight
 (18, 36, 1, 1)" fillcolor=lightblue]
	140571589051488 -> 140571584543376
	140571584543376 [label=AccumulateGrad]
	140571584676720 -> 140571584676864
	140571589052528 [label="stage3.0.fuse_layers.0.1.1.weight
 (18)" fillcolor=lightblue]
	140571589052528 -> 140571584676720
	140571584676720 [label=AccumulateGrad]
	140571584677680 -> 140571584676864
	140571589052448 [label="stage3.0.fuse_layers.0.1.1.bias
 (18)" fillcolor=lightblue]
	140571589052448 -> 140571584677680
	140571584677680 [label=AccumulateGrad]
	140571584678352 -> 140571584678496
	140571584678352 [label=UpsampleBilinear2DBackward0]
	140571584677824 -> 140571584678352
	140571584677824 [label=NativeBatchNormBackward0]
	140571584540544 -> 140571584677824
	140571584540544 [label=ConvolutionBackward0]
	140571584537952 -> 140571584540544
	140571584537952 [label=ReluBackward0]
	140571584529264 -> 140571584537952
	140571584529264 [label=AddBackward0]
	140571584537856 -> 140571584529264
	140571584537856 [label=NativeBatchNormBackward0]
	140571584363680 -> 140571584537856
	140571584363680 [label=ConvolutionBackward0]
	140571584356480 -> 140571584363680
	140571584356480 [label=ReluBackward0]
	140571584231600 -> 140571584356480
	140571584231600 [label=NativeBatchNormBackward0]
	140571584229296 -> 140571584231600
	140571584229296 [label=ConvolutionBackward0]
	140571584537136 -> 140571584229296
	140571584537136 [label=ReluBackward0]
	140571584228384 -> 140571584537136
	140571584228384 [label=AddBackward0]
	140571584228096 -> 140571584228384
	140571584228096 [label=NativeBatchNormBackward0]
	140571584226608 -> 140571584228096
	140571584226608 [label=ConvolutionBackward0]
	140571584226272 -> 140571584226608
	140571584226272 [label=ReluBackward0]
	140571584226080 -> 140571584226272
	140571584226080 [label=NativeBatchNormBackward0]
	140571584225744 -> 140571584226080
	140571584225744 [label=ConvolutionBackward0]
	140571584227088 -> 140571584225744
	140571584227088 [label=ReluBackward0]
	140571584225072 -> 140571584227088
	140571584225072 [label=AddBackward0]
	140571584225024 -> 140571584225072
	140571584225024 [label=NativeBatchNormBackward0]
	140571584224592 -> 140571584225024
	140571584224592 [label=ConvolutionBackward0]
	140571584224208 -> 140571584224592
	140571584224208 [label=ReluBackward0]
	140571584223728 -> 140571584224208
	140571584223728 [label=NativeBatchNormBackward0]
	140571584223680 -> 140571584223728
	140571584223680 [label=ConvolutionBackward0]
	140571584224928 -> 140571584223680
	140571584224928 [label=ReluBackward0]
	140571584223008 -> 140571584224928
	140571584223008 [label=AddBackward0]
	140571584222720 -> 140571584223008
	140571584222720 [label=NativeBatchNormBackward0]
	140571584222240 -> 140571584222720
	140571584222240 [label=ConvolutionBackward0]
	140571584221904 -> 140571584222240
	140571584221904 [label=ReluBackward0]
	140571584221664 -> 140571584221904
	140571584221664 [label=NativeBatchNormBackward0]
	140571584221376 -> 140571584221664
	140571584221376 [label=ConvolutionBackward0]
	140571584222864 -> 140571584221376
	140571584222864 [label=ReluBackward0]
	140571584220704 -> 140571584222864
	140571584220704 [label=NativeBatchNormBackward0]
	140571584220368 -> 140571584220704
	140571584220368 [label=ConvolutionBackward0]
	140571584229776 -> 140571584220368
	140571584220032 -> 140571584220368
	140571869808992 [label="transition2.2.0.0.weight
 (72, 36, 3, 3)" fillcolor=lightblue]
	140571869808992 -> 140571584220032
	140571584220032 [label=AccumulateGrad]
	140571584220560 -> 140571584220704
	140571869814432 [label="transition2.2.0.1.weight
 (72)" fillcolor=lightblue]
	140571869814432 -> 140571584220560
	140571584220560 [label=AccumulateGrad]
	140571584221040 -> 140571584220704
	140571869814352 [label="transition2.2.0.1.bias
 (72)" fillcolor=lightblue]
	140571869814352 -> 140571584221040
	140571584221040 [label=AccumulateGrad]
	140571584220992 -> 140571584221376
	140571588965664 [label="stage3.0.branches.2.0.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571588965664 -> 140571584220992
	140571584220992 [label=AccumulateGrad]
	140571584221520 -> 140571584221664
	140571588965424 [label="stage3.0.branches.2.0.bn1.weight
 (72)" fillcolor=lightblue]
	140571588965424 -> 140571584221520
	140571584221520 [label=AccumulateGrad]
	140571584221712 -> 140571584221664
	140571588965824 [label="stage3.0.branches.2.0.bn1.bias
 (72)" fillcolor=lightblue]
	140571588965824 -> 140571584221712
	140571584221712 [label=AccumulateGrad]
	140571584222048 -> 140571584222240
	140571588966304 [label="stage3.0.branches.2.0.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571588966304 -> 140571584222048
	140571584222048 [label=AccumulateGrad]
	140571584222384 -> 140571584222720
	140571588965584 [label="stage3.0.branches.2.0.bn2.weight
 (72)" fillcolor=lightblue]
	140571588965584 -> 140571584222384
	140571584222384 [label=AccumulateGrad]
	140571584222576 -> 140571584222720
	140571588966464 [label="stage3.0.branches.2.0.bn2.bias
 (72)" fillcolor=lightblue]
	140571588966464 -> 140571584222576
	140571584222576 [label=AccumulateGrad]
	140571584222864 -> 140571584223008
	140571584223056 -> 140571584223680
	140571588966944 [label="stage3.0.branches.2.1.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571588966944 -> 140571584223056
	140571584223056 [label=AccumulateGrad]
	140571584223584 -> 140571584223728
	140571588966784 [label="stage3.0.branches.2.1.bn1.weight
 (72)" fillcolor=lightblue]
	140571588966784 -> 140571584223584
	140571584223584 [label=AccumulateGrad]
	140571584224064 -> 140571584223728
	140571588965904 [label="stage3.0.branches.2.1.bn1.bias
 (72)" fillcolor=lightblue]
	140571588965904 -> 140571584224064
	140571584224064 [label=AccumulateGrad]
	140571584224352 -> 140571584224592
	140571588967664 [label="stage3.0.branches.2.1.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571588967664 -> 140571584224352
	140571584224352 [label=AccumulateGrad]
	140571584224736 -> 140571584225024
	140571588967584 [label="stage3.0.branches.2.1.bn2.weight
 (72)" fillcolor=lightblue]
	140571588967584 -> 140571584224736
	140571584224736 [label=AccumulateGrad]
	140571584224880 -> 140571584225024
	140571588967424 [label="stage3.0.branches.2.1.bn2.bias
 (72)" fillcolor=lightblue]
	140571588967424 -> 140571584224880
	140571584224880 [label=AccumulateGrad]
	140571584224928 -> 140571584225072
	140571584225408 -> 140571584225744
	140571588968224 [label="stage3.0.branches.2.2.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571588968224 -> 140571584225408
	140571584225408 [label=AccumulateGrad]
	140571584225936 -> 140571584226080
	140571588967984 [label="stage3.0.branches.2.2.bn1.weight
 (72)" fillcolor=lightblue]
	140571588967984 -> 140571584225936
	140571584225936 [label=AccumulateGrad]
	140571584226368 -> 140571584226080
	140571588968144 [label="stage3.0.branches.2.2.bn1.bias
 (72)" fillcolor=lightblue]
	140571588968144 -> 140571584226368
	140571584226368 [label=AccumulateGrad]
	140571584226416 -> 140571584226608
	140571589055808 [label="stage3.0.branches.2.2.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571589055808 -> 140571584226416
	140571584226416 [label=AccumulateGrad]
	140571584226944 -> 140571584228096
	140571589052368 [label="stage3.0.branches.2.2.bn2.weight
 (72)" fillcolor=lightblue]
	140571589052368 -> 140571584226944
	140571584226944 [label=AccumulateGrad]
	140571584227712 -> 140571584228096
	140571589050528 [label="stage3.0.branches.2.2.bn2.bias
 (72)" fillcolor=lightblue]
	140571589050528 -> 140571584227712
	140571584227712 [label=AccumulateGrad]
	140571584227088 -> 140571584228384
	140571584227760 -> 140571584229296
	140571589051008 [label="stage3.0.branches.2.3.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571589051008 -> 140571584227760
	140571584227760 [label=AccumulateGrad]
	140571584230640 -> 140571584231600
	140571589051248 [label="stage3.0.branches.2.3.bn1.weight
 (72)" fillcolor=lightblue]
	140571589051248 -> 140571584230640
	140571584230640 [label=AccumulateGrad]
	140571584231648 -> 140571584231600
	140571589051168 [label="stage3.0.branches.2.3.bn1.bias
 (72)" fillcolor=lightblue]
	140571589051168 -> 140571584231648
	140571584231648 [label=AccumulateGrad]
	140571584354800 -> 140571584363680
	140571589051648 [label="stage3.0.branches.2.3.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571589051648 -> 140571584354800
	140571584354800 [label=AccumulateGrad]
	140571584353600 -> 140571584537856
	140571589051888 [label="stage3.0.branches.2.3.bn2.weight
 (72)" fillcolor=lightblue]
	140571589051888 -> 140571584353600
	140571584353600 [label=AccumulateGrad]
	140571584364016 -> 140571584537856
	140571589051808 [label="stage3.0.branches.2.3.bn2.bias
 (72)" fillcolor=lightblue]
	140571589051808 -> 140571584364016
	140571584364016 [label=AccumulateGrad]
	140571584537136 -> 140571584529264
	140571584539872 -> 140571584540544
	140571589052608 [label="stage3.0.fuse_layers.0.2.0.weight
 (18, 72, 1, 1)" fillcolor=lightblue]
	140571589052608 -> 140571584539872
	140571584539872 [label=AccumulateGrad]
	140571584542896 -> 140571584677824
	140571589052688 [label="stage3.0.fuse_layers.0.2.1.weight
 (18)" fillcolor=lightblue]
	140571589052688 -> 140571584542896
	140571584542896 [label=AccumulateGrad]
	140571584544240 -> 140571584677824
	140571589053088 [label="stage3.0.fuse_layers.0.2.1.bias
 (18)" fillcolor=lightblue]
	140571589053088 -> 140571584544240
	140571584544240 [label=AccumulateGrad]
	140571584678544 -> 140571584679168
	140571589056368 [label="stage3.1.branches.0.0.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571589056368 -> 140571584678544
	140571584678544 [label=AccumulateGrad]
	140571584679072 -> 140571584679216
	140571589056768 [label="stage3.1.branches.0.0.bn1.weight
 (18)" fillcolor=lightblue]
	140571589056768 -> 140571584679072
	140571584679072 [label=AccumulateGrad]
	140571584679552 -> 140571584679216
	140571589056608 [label="stage3.1.branches.0.0.bn1.bias
 (18)" fillcolor=lightblue]
	140571589056608 -> 140571584679552
	140571584679552 [label=AccumulateGrad]
	140571584679840 -> 140571584680080
	140571589057008 [label="stage3.1.branches.0.0.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571589057008 -> 140571584679840
	140571584679840 [label=AccumulateGrad]
	140571584680224 -> 140571584680512
	140571589056928 [label="stage3.1.branches.0.0.bn2.weight
 (18)" fillcolor=lightblue]
	140571589056928 -> 140571584680224
	140571584680224 [label=AccumulateGrad]
	140571584680368 -> 140571584680512
	140571589057408 [label="stage3.1.branches.0.0.bn2.bias
 (18)" fillcolor=lightblue]
	140571589057408 -> 140571584680368
	140571584680368 [label=AccumulateGrad]
	140571584680416 -> 140571584680560
	140571584680896 -> 140571584681232
	140571589057648 [label="stage3.1.branches.0.1.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571589057648 -> 140571584680896
	140571584680896 [label=AccumulateGrad]
	140571584681424 -> 140571584681568
	140571589057568 [label="stage3.1.branches.0.1.bn1.weight
 (18)" fillcolor=lightblue]
	140571589057568 -> 140571584681424
	140571584681424 [label=AccumulateGrad]
	140571584681856 -> 140571584681568
	140571589058048 [label="stage3.1.branches.0.1.bn1.bias
 (18)" fillcolor=lightblue]
	140571589058048 -> 140571584681856
	140571584681856 [label=AccumulateGrad]
	140571584681904 -> 140571584682384
	140571589058288 [label="stage3.1.branches.0.1.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571589058288 -> 140571584681904
	140571584681904 [label=AccumulateGrad]
	140571584682528 -> 140571584682576
	140571589058208 [label="stage3.1.branches.0.1.bn2.weight
 (18)" fillcolor=lightblue]
	140571589058208 -> 140571584682528
	140571584682528 [label=AccumulateGrad]
	140571584682432 -> 140571584682576
	140571589058688 [label="stage3.1.branches.0.1.bn2.bias
 (18)" fillcolor=lightblue]
	140571589058688 -> 140571584682432
	140571584682432 [label=AccumulateGrad]
	140571584682768 -> 140571584682912
	140571584683200 -> 140571584683584
	140571589059168 [label="stage3.1.branches.0.2.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571589059168 -> 140571584683200
	140571584683200 [label=AccumulateGrad]
	140571584683728 -> 140571584683872
	140571589058848 [label="stage3.1.branches.0.2.bn1.weight
 (18)" fillcolor=lightblue]
	140571589058848 -> 140571584683728
	140571584683728 [label=AccumulateGrad]
	140571584683920 -> 140571584683872
	140571589059328 [label="stage3.1.branches.0.2.bn1.bias
 (18)" fillcolor=lightblue]
	140571589059328 -> 140571584683920
	140571584683920 [label=AccumulateGrad]
	140571584684256 -> 140571584684448
	140571589059568 [label="stage3.1.branches.0.2.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571589059568 -> 140571584684256
	140571584684256 [label=AccumulateGrad]
	140571584684592 -> 140571584684928
	140571589058928 [label="stage3.1.branches.0.2.bn2.weight
 (18)" fillcolor=lightblue]
	140571589058928 -> 140571584684592
	140571584684592 [label=AccumulateGrad]
	140571584684784 -> 140571584684928
	140571589059088 [label="stage3.1.branches.0.2.bn2.bias
 (18)" fillcolor=lightblue]
	140571589059088 -> 140571584684784
	140571584684784 [label=AccumulateGrad]
	140571584685072 -> 140571584685216
	140571584685264 -> 140571584685888
	140571589060288 [label="stage3.1.branches.0.3.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571589060288 -> 140571584685264
	140571584685264 [label=AccumulateGrad]
	140571584685792 -> 140571584685936
	140571589060128 [label="stage3.1.branches.0.3.bn1.weight
 (18)" fillcolor=lightblue]
	140571589060128 -> 140571584685792
	140571584685792 [label=AccumulateGrad]
	140571584686272 -> 140571584685936
	140571589060048 [label="stage3.1.branches.0.3.bn1.bias
 (18)" fillcolor=lightblue]
	140571589060048 -> 140571584686272
	140571584686272 [label=AccumulateGrad]
	140571584686560 -> 140571584686800
	140571589061168 [label="stage3.1.branches.0.3.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571589061168 -> 140571584686560
	140571584686560 [label=AccumulateGrad]
	140571584686944 -> 140571584687232
	140571589061248 [label="stage3.1.branches.0.3.bn2.weight
 (18)" fillcolor=lightblue]
	140571589061248 -> 140571584686944
	140571584686944 [label=AccumulateGrad]
	140571584687088 -> 140571584687232
	140571589061328 [label="stage3.1.branches.0.3.bn2.bias
 (18)" fillcolor=lightblue]
	140571589061328 -> 140571584687088
	140571584687088 [label=AccumulateGrad]
	140571584687136 -> 140571584687280
	140571584687904 -> 140571584687952
	140571584687904 [label=UpsampleBilinear2DBackward0]
	140571584686608 -> 140571584687904
	140571584686608 [label=NativeBatchNormBackward0]
	140571584685120 -> 140571584686608
	140571584685120 [label=ConvolutionBackward0]
	140571584685456 -> 140571584685120
	140571584685456 [label=ReluBackward0]
	140571584683056 -> 140571584685456
	140571584683056 [label=AddBackward0]
	140571584683440 -> 140571584683056
	140571584683440 [label=NativeBatchNormBackward0]
	140571584683248 -> 140571584683440
	140571584683248 [label=ConvolutionBackward0]
	140571584681088 -> 140571584683248
	140571584681088 [label=ReluBackward0]
	140571584681184 -> 140571584681088
	140571584681184 [label=NativeBatchNormBackward0]
	140571584678400 -> 140571584681184
	140571584678400 [label=ConvolutionBackward0]
	140571584684400 -> 140571584678400
	140571584684400 [label=ReluBackward0]
	140571584678880 -> 140571584684400
	140571584678880 [label=AddBackward0]
	140571584539824 -> 140571584678880
	140571584539824 [label=NativeBatchNormBackward0]
	140571584358976 -> 140571584539824
	140571584358976 [label=ConvolutionBackward0]
	140571584227040 -> 140571584358976
	140571584227040 [label=ReluBackward0]
	140571584226224 -> 140571584227040
	140571584226224 [label=NativeBatchNormBackward0]
	140571584226752 -> 140571584226224
	140571584226752 [label=ConvolutionBackward0]
	140571584538864 -> 140571584226752
	140571584538864 [label=ReluBackward0]
	140571584222912 -> 140571584538864
	140571584222912 [label=AddBackward0]
	140571584223536 -> 140571584222912
	140571584223536 [label=NativeBatchNormBackward0]
	140571584223392 -> 140571584223536
	140571584223392 [label=ConvolutionBackward0]
	140571584221232 -> 140571584223392
	140571584221232 [label=ReluBackward0]
	140571584220224 -> 140571584221232
	140571584220224 [label=NativeBatchNormBackward0]
	140571584220176 -> 140571584220224
	140571584220176 [label=ConvolutionBackward0]
	140571584224256 -> 140571584220176
	140571584224256 [label=ReluBackward0]
	140571584219216 -> 140571584224256
	140571584219216 [label=AddBackward0]
	140571584218880 -> 140571584219216
	140571584218880 [label=NativeBatchNormBackward0]
	140571584218688 -> 140571584218880
	140571584218688 [label=ConvolutionBackward0]
	140571584218304 -> 140571584218688
	140571584218304 [label=ReluBackward0]
	140571584217872 -> 140571584218304
	140571584217872 [label=NativeBatchNormBackward0]
	140571584217536 -> 140571584217872
	140571584217536 [label=ConvolutionBackward0]
	140571584219024 -> 140571584217536
	140571584219024 [label=ReluBackward0]
	140571584217488 -> 140571584219024
	140571584217488 [label=AddBackward0]
	140571586150064 -> 140571584217488
	140571586150064 [label=AddBackward0]
	140571586149584 -> 140571586150064
	140571586149584 [label=NativeBatchNormBackward0]
	140571586149536 -> 140571586149584
	140571586149536 [label=ConvolutionBackward0]
	140571584677728 -> 140571586149536
	140571586148912 -> 140571586149536
	140571589053168 [label="stage3.0.fuse_layers.1.0.0.0.weight
 (36, 18, 3, 3)" fillcolor=lightblue]
	140571589053168 -> 140571586148912
	140571586148912 [label=AccumulateGrad]
	140571586149440 -> 140571586149584
	140571589053328 [label="stage3.0.fuse_layers.1.0.0.1.weight
 (36)" fillcolor=lightblue]
	140571589053328 -> 140571586149440
	140571586149440 [label=AccumulateGrad]
	140571586149920 -> 140571586149584
	140571589053248 [label="stage3.0.fuse_layers.1.0.0.1.bias
 (36)" fillcolor=lightblue]
	140571589053248 -> 140571586149920
	140571586149920 [label=AccumulateGrad]
	140571584544528 -> 140571586150064
	140571586150208 -> 140571584217488
	140571586150208 [label=UpsampleBilinear2DBackward0]
	140571586149248 -> 140571586150208
	140571586149248 [label=NativeBatchNormBackward0]
	140571586149104 -> 140571586149248
	140571586149104 [label=ConvolutionBackward0]
	140571584537952 -> 140571586149104
	140571586148432 -> 140571586149104
	140571589054128 [label="stage3.0.fuse_layers.1.2.0.weight
 (36, 72, 1, 1)" fillcolor=lightblue]
	140571589054128 -> 140571586148432
	140571586148432 [label=AccumulateGrad]
	140571586148864 -> 140571586149248
	140571589054048 [label="stage3.0.fuse_layers.1.2.1.weight
 (36)" fillcolor=lightblue]
	140571589054048 -> 140571586148864
	140571586148864 [label=AccumulateGrad]
	140571586149776 -> 140571586149248
	140571589054288 [label="stage3.0.fuse_layers.1.2.1.bias
 (36)" fillcolor=lightblue]
	140571589054288 -> 140571586149776
	140571586149776 [label=AccumulateGrad]
	140571584217200 -> 140571584217536
	140571589061728 [label="stage3.1.branches.1.0.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571589061728 -> 140571584217200
	140571584217200 [label=AccumulateGrad]
	140571584217680 -> 140571584217872
	140571589061648 [label="stage3.1.branches.1.0.bn1.weight
 (36)" fillcolor=lightblue]
	140571589061648 -> 140571584217680
	140571584217680 [label=AccumulateGrad]
	140571584218160 -> 140571584217872
	140571589061808 [label="stage3.1.branches.1.0.bn1.bias
 (36)" fillcolor=lightblue]
	140571589061808 -> 140571584218160
	140571584218160 [label=AccumulateGrad]
	140571584218208 -> 140571584218688
	140571589062288 [label="stage3.1.branches.1.0.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571589062288 -> 140571584218208
	140571584218208 [label=AccumulateGrad]
	140571584218832 -> 140571584218880
	140571589062368 [label="stage3.1.branches.1.0.bn2.weight
 (36)" fillcolor=lightblue]
	140571589062368 -> 140571584218832
	140571584218832 [label=AccumulateGrad]
	140571584218976 -> 140571584218880
	140571589062448 [label="stage3.1.branches.1.0.bn2.bias
 (36)" fillcolor=lightblue]
	140571589062448 -> 140571584218976
	140571584218976 [label=AccumulateGrad]
	140571584219024 -> 140571584219216
	140571584219504 -> 140571584220176
	140571589062848 [label="stage3.1.branches.1.1.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571589062848 -> 140571584219504
	140571584219504 [label=AccumulateGrad]
	140571584219696 -> 140571584220224
	140571589062928 [label="stage3.1.branches.1.1.bn1.weight
 (36)" fillcolor=lightblue]
	140571589062928 -> 140571584219696
	140571584219696 [label=AccumulateGrad]
	140571584220320 -> 140571584220224
	140571589063008 [label="stage3.1.branches.1.1.bn1.bias
 (36)" fillcolor=lightblue]
	140571589063008 -> 140571584220320
	140571584220320 [label=AccumulateGrad]
	140571584222192 -> 140571584223392
	140571589063408 [label="stage3.1.branches.1.1.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571589063408 -> 140571584222192
	140571584222192 [label=AccumulateGrad]
	140571584223248 -> 140571584223536
	140571589063488 [label="stage3.1.branches.1.1.bn2.weight
 (36)" fillcolor=lightblue]
	140571589063488 -> 140571584223248
	140571584223248 [label=AccumulateGrad]
	140571584222336 -> 140571584223536
	140571589063568 [label="stage3.1.branches.1.1.bn2.bias
 (36)" fillcolor=lightblue]
	140571589063568 -> 140571584222336
	140571584222336 [label=AccumulateGrad]
	140571584224256 -> 140571584222912
	140571584225696 -> 140571584226752
	140571589064048 [label="stage3.1.branches.1.2.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571589064048 -> 140571584225696
	140571584225696 [label=AccumulateGrad]
	140571584225264 -> 140571584226224
	140571589064128 [label="stage3.1.branches.1.2.bn1.weight
 (36)" fillcolor=lightblue]
	140571589064128 -> 140571584225264
	140571584225264 [label=AccumulateGrad]
	140571584228768 -> 140571584226224
	140571589064208 [label="stage3.1.branches.1.2.bn1.bias
 (36)" fillcolor=lightblue]
	140571589064208 -> 140571584228768
	140571584228768 [label=AccumulateGrad]
	140571584230400 -> 140571584358976
	140571589064608 [label="stage3.1.branches.1.2.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571589064608 -> 140571584230400
	140571584230400 [label=AccumulateGrad]
	140571584544672 -> 140571584539824
	140571589064688 [label="stage3.1.branches.1.2.bn2.weight
 (36)" fillcolor=lightblue]
	140571589064688 -> 140571584544672
	140571584544672 [label=AccumulateGrad]
	140571584230112 -> 140571584539824
	140571589064768 [label="stage3.1.branches.1.2.bn2.bias
 (36)" fillcolor=lightblue]
	140571589064768 -> 140571584230112
	140571584230112 [label=AccumulateGrad]
	140571584538864 -> 140571584678880
	140571584678736 -> 140571584678400
	140571589065248 [label="stage3.1.branches.1.3.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571589065248 -> 140571584678736
	140571584678736 [label=AccumulateGrad]
	140571584679408 -> 140571584681184
	140571589065168 [label="stage3.1.branches.1.3.bn1.weight
 (36)" fillcolor=lightblue]
	140571589065168 -> 140571584679408
	140571584679408 [label=AccumulateGrad]
	140571584679888 -> 140571584681184
	140571589065328 [label="stage3.1.branches.1.3.bn1.bias
 (36)" fillcolor=lightblue]
	140571589065328 -> 140571584679888
	140571584679888 [label=AccumulateGrad]
	140571584682096 -> 140571584683248
	140571589065808 [label="stage3.1.branches.1.3.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571589065808 -> 140571584682096
	140571584682096 [label=AccumulateGrad]
	140571584683104 -> 140571584683440
	140571589065888 [label="stage3.1.branches.1.3.bn2.weight
 (36)" fillcolor=lightblue]
	140571589065888 -> 140571584683104
	140571584683104 [label=AccumulateGrad]
	140571584682240 -> 140571584683440
	140571589065968 [label="stage3.1.branches.1.3.bn2.bias
 (36)" fillcolor=lightblue]
	140571589065968 -> 140571584682240
	140571584682240 [label=AccumulateGrad]
	140571584684400 -> 140571584683056
	140571584684544 -> 140571584685120
	140571587695216 [label="stage3.1.fuse_layers.0.1.0.weight
 (18, 36, 1, 1)" fillcolor=lightblue]
	140571587695216 -> 140571584684544
	140571584684544 [label=AccumulateGrad]
	140571584686128 -> 140571584686608
	140571587695296 [label="stage3.1.fuse_layers.0.1.1.weight
 (18)" fillcolor=lightblue]
	140571587695296 -> 140571584686128
	140571584686128 [label=AccumulateGrad]
	140571584687472 -> 140571584686608
	140571587695376 [label="stage3.1.fuse_layers.0.1.1.bias
 (18)" fillcolor=lightblue]
	140571587695376 -> 140571584687472
	140571584687472 [label=AccumulateGrad]
	140571584688144 -> 140571584688288
	140571584688144 [label=UpsampleBilinear2DBackward0]
	140571584686464 -> 140571584688144
	140571584686464 [label=NativeBatchNormBackward0]
	140571584681712 -> 140571584686464
	140571584681712 [label=ConvolutionBackward0]
	140571584679024 -> 140571584681712
	140571584679024 [label=ReluBackward0]
	140571584678064 -> 140571584679024
	140571584678064 [label=AddBackward0]
	140571584544576 -> 140571584678064
	140571584544576 [label=NativeBatchNormBackward0]
	140571584232320 -> 140571584544576
	140571584232320 [label=ConvolutionBackward0]
	140571584224400 -> 140571584232320
	140571584224400 [label=ReluBackward0]
	140571584220848 -> 140571584224400
	140571584220848 [label=NativeBatchNormBackward0]
	140571584218544 -> 140571584220848
	140571584218544 [label=ConvolutionBackward0]
	140571584680752 -> 140571584218544
	140571584680752 [label=ReluBackward0]
	140571584217632 -> 140571584680752
	140571584217632 [label=AddBackward0]
	140571584219552 -> 140571584217632
	140571584219552 [label=NativeBatchNormBackward0]
	140571586148720 -> 140571584219552
	140571586148720 [label=ConvolutionBackward0]
	140571586148192 -> 140571586148720
	140571586148192 [label=ReluBackward0]
	140571586147760 -> 140571586148192
	140571586147760 [label=NativeBatchNormBackward0]
	140571586147424 -> 140571586147760
	140571586147424 [label=ConvolutionBackward0]
	140571586149392 -> 140571586147424
	140571586149392 [label=ReluBackward0]
	140571586146752 -> 140571586149392
	140571586146752 [label=AddBackward0]
	140571586146704 -> 140571586146752
	140571586146704 [label=NativeBatchNormBackward0]
	140571586146224 -> 140571586146704
	140571586146224 [label=ConvolutionBackward0]
	140571586145888 -> 140571586146224
	140571586145888 [label=ReluBackward0]
	140571586145408 -> 140571586145888
	140571586145408 [label=NativeBatchNormBackward0]
	140571586145360 -> 140571586145408
	140571586145360 [label=ConvolutionBackward0]
	140571586146848 -> 140571586145360
	140571586146848 [label=ReluBackward0]
	140571586144688 -> 140571586146848
	140571586144688 [label=AddBackward0]
	140571586144400 -> 140571586144688
	140571586144400 [label=NativeBatchNormBackward0]
	140571586144160 -> 140571586144400
	140571586144160 [label=ConvolutionBackward0]
	140571586143536 -> 140571586144160
	140571586143536 [label=ReluBackward0]
	140571586143344 -> 140571586143536
	140571586143344 [label=NativeBatchNormBackward0]
	140571586143056 -> 140571586143344
	140571586143056 [label=ConvolutionBackward0]
	140571586144544 -> 140571586143056
	140571586144544 [label=ReluBackward0]
	140571586142384 -> 140571586144544
	140571586142384 [label=AddBackward0]
	140571586142048 -> 140571586142384
	140571586142048 [label=AddBackward0]
	140571586142000 -> 140571586142048
	140571586142000 [label=NativeBatchNormBackward0]
	140571586141520 -> 140571586142000
	140571586141520 [label=ConvolutionBackward0]
	140571586141184 -> 140571586141520
	140571586141184 [label=ReluBackward0]
	140571586140704 -> 140571586141184
	140571586140704 [label=NativeBatchNormBackward0]
	140571586140656 -> 140571586140704
	140571586140656 [label=ConvolutionBackward0]
	140571584677728 -> 140571586140656
	140571586140032 -> 140571586140656
	140571589054688 [label="stage3.0.fuse_layers.2.0.0.0.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571589054688 -> 140571586140032
	140571586140032 [label=AccumulateGrad]
	140571586140800 -> 140571586140704
	140571589053888 [label="stage3.0.fuse_layers.2.0.0.1.weight
 (18)" fillcolor=lightblue]
	140571589053888 -> 140571586140800
	140571586140800 [label=AccumulateGrad]
	140571586141040 -> 140571586140704
	140571589055488 [label="stage3.0.fuse_layers.2.0.0.1.bias
 (18)" fillcolor=lightblue]
	140571589055488 -> 140571586141040
	140571586141040 [label=AccumulateGrad]
	140571586141328 -> 140571586141520
	140571589055408 [label="stage3.0.fuse_layers.2.0.1.0.weight
 (72, 18, 3, 3)" fillcolor=lightblue]
	140571589055408 -> 140571586141328
	140571586141328 [label=AccumulateGrad]
	140571586141712 -> 140571586142000
	140571589055328 [label="stage3.0.fuse_layers.2.0.1.1.weight
 (72)" fillcolor=lightblue]
	140571589055328 -> 140571586141712
	140571586141712 [label=AccumulateGrad]
	140571586141856 -> 140571586142000
	140571589055648 [label="stage3.0.fuse_layers.2.0.1.1.bias
 (72)" fillcolor=lightblue]
	140571589055648 -> 140571586141856
	140571586141856 [label=AccumulateGrad]
	140571586142144 -> 140571586142048
	140571586142144 [label=NativeBatchNormBackward0]
	140571586140368 -> 140571586142144
	140571586140368 [label=ConvolutionBackward0]
	140571584544528 -> 140571586140368
	140571586140176 -> 140571586140368
	140571589056208 [label="stage3.0.fuse_layers.2.1.0.0.weight
 (72, 36, 3, 3)" fillcolor=lightblue]
	140571589056208 -> 140571586140176
	140571586140176 [label=AccumulateGrad]
	140571586140848 -> 140571586142144
	140571589056128 [label="stage3.0.fuse_layers.2.1.0.1.weight
 (72)" fillcolor=lightblue]
	140571589056128 -> 140571586140848
	140571586140848 [label=AccumulateGrad]
	140571586141376 -> 140571586142144
	140571589060208 [label="stage3.0.fuse_layers.2.1.0.1.bias
 (72)" fillcolor=lightblue]
	140571589060208 -> 140571586141376
	140571586141376 [label=AccumulateGrad]
	140571584537952 -> 140571586142384
	140571586142672 -> 140571586143056
	140571589066368 [label="stage3.1.branches.2.0.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571589066368 -> 140571586142672
	140571586142672 [label=AccumulateGrad]
	140571586143200 -> 140571586143344
	140571589066448 [label="stage3.1.branches.2.0.bn1.weight
 (72)" fillcolor=lightblue]
	140571589066448 -> 140571586143200
	140571586143200 [label=AccumulateGrad]
	140571586143392 -> 140571586143344
	140571589066528 [label="stage3.1.branches.2.0.bn1.bias
 (72)" fillcolor=lightblue]
	140571589066528 -> 140571586143392
	140571586143392 [label=AccumulateGrad]
	140571586143728 -> 140571586144160
	140571587690736 [label="stage3.1.branches.2.0.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587690736 -> 140571586143728
	140571586143728 [label=AccumulateGrad]
	140571586144208 -> 140571586144400
	140571587690816 [label="stage3.1.branches.2.0.bn2.weight
 (72)" fillcolor=lightblue]
	140571587690816 -> 140571586144208
	140571586144208 [label=AccumulateGrad]
	140571586144064 -> 140571586144400
	140571587690896 [label="stage3.1.branches.2.0.bn2.bias
 (72)" fillcolor=lightblue]
	140571587690896 -> 140571586144064
	140571586144064 [label=AccumulateGrad]
	140571586144544 -> 140571586144688
	140571586144736 -> 140571586145360
	140571587691376 [label="stage3.1.branches.2.1.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587691376 -> 140571586144736
	140571586144736 [label=AccumulateGrad]
	140571586145504 -> 140571586145408
	140571587691456 [label="stage3.1.branches.2.1.bn1.weight
 (72)" fillcolor=lightblue]
	140571587691456 -> 140571586145504
	140571586145504 [label=AccumulateGrad]
	140571586145744 -> 140571586145408
	140571587691536 [label="stage3.1.branches.2.1.bn1.bias
 (72)" fillcolor=lightblue]
	140571587691536 -> 140571586145744
	140571586145744 [label=AccumulateGrad]
	140571586146032 -> 140571586146224
	140571587692016 [label="stage3.1.branches.2.1.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587692016 -> 140571586146032
	140571586146032 [label=AccumulateGrad]
	140571586146416 -> 140571586146704
	140571587692096 [label="stage3.1.branches.2.1.bn2.weight
 (72)" fillcolor=lightblue]
	140571587692096 -> 140571586146416
	140571586146416 [label=AccumulateGrad]
	140571586146560 -> 140571586146704
	140571587692176 [label="stage3.1.branches.2.1.bn2.bias
 (72)" fillcolor=lightblue]
	140571587692176 -> 140571586146560
	140571586146560 [label=AccumulateGrad]
	140571586146848 -> 140571586146752
	140571586147088 -> 140571586147424
	140571587692656 [label="stage3.1.branches.2.2.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587692656 -> 140571586147088
	140571586147088 [label=AccumulateGrad]
	140571586147568 -> 140571586147760
	140571587692736 [label="stage3.1.branches.2.2.bn1.weight
 (72)" fillcolor=lightblue]
	140571587692736 -> 140571586147568
	140571586147568 [label=AccumulateGrad]
	140571586148048 -> 140571586147760
	140571587692816 [label="stage3.1.branches.2.2.bn1.bias
 (72)" fillcolor=lightblue]
	140571587692816 -> 140571586148048
	140571586148048 [label=AccumulateGrad]
	140571586148240 -> 140571586148720
	140571587693296 [label="stage3.1.branches.2.2.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587693296 -> 140571586148240
	140571586148240 [label=AccumulateGrad]
	140571586148768 -> 140571584219552
	140571587693376 [label="stage3.1.branches.2.2.bn2.weight
 (72)" fillcolor=lightblue]
	140571587693376 -> 140571586148768
	140571586148768 [label=AccumulateGrad]
	140571586150256 -> 140571584219552
	140571587693456 [label="stage3.1.branches.2.2.bn2.bias
 (72)" fillcolor=lightblue]
	140571587693456 -> 140571586150256
	140571586150256 [label=AccumulateGrad]
	140571586149392 -> 140571584217632
	140571584217344 -> 140571584218544
	140571587693936 [label="stage3.1.branches.2.3.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587693936 -> 140571584217344
	140571584217344 [label=AccumulateGrad]
	140571584219888 -> 140571584220848
	140571587694016 [label="stage3.1.branches.2.3.bn1.weight
 (72)" fillcolor=lightblue]
	140571587694016 -> 140571584219888
	140571584219888 [label=AccumulateGrad]
	140571584220896 -> 140571584220848
	140571587694096 [label="stage3.1.branches.2.3.bn1.bias
 (72)" fillcolor=lightblue]
	140571587694096 -> 140571584220896
	140571584220896 [label=AccumulateGrad]
	140571584225552 -> 140571584232320
	140571587694576 [label="stage3.1.branches.2.3.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587694576 -> 140571584225552
	140571584225552 [label=AccumulateGrad]
	140571584223920 -> 140571584544576
	140571587694656 [label="stage3.1.branches.2.3.bn2.weight
 (72)" fillcolor=lightblue]
	140571587694656 -> 140571584223920
	140571584223920 [label=AccumulateGrad]
	140571584230304 -> 140571584544576
	140571587694736 [label="stage3.1.branches.2.3.bn2.bias
 (72)" fillcolor=lightblue]
	140571587694736 -> 140571584230304
	140571584230304 [label=AccumulateGrad]
	140571584680752 -> 140571584678064
	140571584681040 -> 140571584681712
	140571587695856 [label="stage3.1.fuse_layers.0.2.0.weight
 (18, 72, 1, 1)" fillcolor=lightblue]
	140571587695856 -> 140571584681040
	140571584681040 [label=AccumulateGrad]
	140571584683776 -> 140571584686464
	140571587695936 [label="stage3.1.fuse_layers.0.2.1.weight
 (18)" fillcolor=lightblue]
	140571587695936 -> 140571584683776
	140571584683776 [label=AccumulateGrad]
	140571584687808 -> 140571584686464
	140571587696016 [label="stage3.1.fuse_layers.0.2.1.bias
 (18)" fillcolor=lightblue]
	140571587696016 -> 140571584687808
	140571584687808 [label=AccumulateGrad]
	140571584688576 -> 140571584688960
	140571587699536 [label="stage3.2.branches.0.0.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587699536 -> 140571584688576
	140571584688576 [label=AccumulateGrad]
	140571584689104 -> 140571584689248
	140571587699616 [label="stage3.2.branches.0.0.bn1.weight
 (18)" fillcolor=lightblue]
	140571587699616 -> 140571584689104
	140571584689104 [label=AccumulateGrad]
	140571584689296 -> 140571584689248
	140571587699696 [label="stage3.2.branches.0.0.bn1.bias
 (18)" fillcolor=lightblue]
	140571587699696 -> 140571584689296
	140571584689296 [label=AccumulateGrad]
	140571584689632 -> 140571584689824
	140571587700176 [label="stage3.2.branches.0.0.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587700176 -> 140571584689632
	140571584689632 [label=AccumulateGrad]
	140571584689968 -> 140571584690304
	140571587700256 [label="stage3.2.branches.0.0.bn2.weight
 (18)" fillcolor=lightblue]
	140571587700256 -> 140571584689968
	140571584689968 [label=AccumulateGrad]
	140571584690160 -> 140571584690304
	140571587700336 [label="stage3.2.branches.0.0.bn2.bias
 (18)" fillcolor=lightblue]
	140571587700336 -> 140571584690160
	140571584690160 [label=AccumulateGrad]
	140571584690448 -> 140571584690592
	140571584690640 -> 140571584691264
	140571587700816 [label="stage3.2.branches.0.1.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587700816 -> 140571584690640
	140571584690640 [label=AccumulateGrad]
	140571584691168 -> 140571584691312
	140571587700896 [label="stage3.2.branches.0.1.bn1.weight
 (18)" fillcolor=lightblue]
	140571587700896 -> 140571584691168
	140571584691168 [label=AccumulateGrad]
	140571584691648 -> 140571584691312
	140571587700976 [label="stage3.2.branches.0.1.bn1.bias
 (18)" fillcolor=lightblue]
	140571587700976 -> 140571584691648
	140571584691648 [label=AccumulateGrad]
	140571584691936 -> 140571584807072
	140571587701376 [label="stage3.2.branches.0.1.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587701376 -> 140571584691936
	140571584691936 [label=AccumulateGrad]
	140571584807216 -> 140571584807360
	140571587701456 [label="stage3.2.branches.0.1.bn2.weight
 (18)" fillcolor=lightblue]
	140571587701456 -> 140571584807216
	140571584807216 [label=AccumulateGrad]
	140571584692176 -> 140571584807360
	140571587701536 [label="stage3.2.branches.0.1.bn2.bias
 (18)" fillcolor=lightblue]
	140571587701536 -> 140571584692176
	140571584692176 [label=AccumulateGrad]
	140571584807264 -> 140571584807408
	140571584807744 -> 140571584808080
	140571587702016 [label="stage3.2.branches.0.2.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587702016 -> 140571584807744
	140571584807744 [label=AccumulateGrad]
	140571584808272 -> 140571584808416
	140571587702096 [label="stage3.2.branches.0.2.bn1.weight
 (18)" fillcolor=lightblue]
	140571587702096 -> 140571584808272
	140571584808272 [label=AccumulateGrad]
	140571584808704 -> 140571584808416
	140571587702176 [label="stage3.2.branches.0.2.bn1.bias
 (18)" fillcolor=lightblue]
	140571587702176 -> 140571584808704
	140571584808704 [label=AccumulateGrad]
	140571584808752 -> 140571584809232
	140571587702656 [label="stage3.2.branches.0.2.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587702656 -> 140571584808752
	140571584808752 [label=AccumulateGrad]
	140571584809376 -> 140571584809424
	140571587702736 [label="stage3.2.branches.0.2.bn2.weight
 (18)" fillcolor=lightblue]
	140571587702736 -> 140571584809376
	140571584809376 [label=AccumulateGrad]
	140571584809280 -> 140571584809424
	140571587702816 [label="stage3.2.branches.0.2.bn2.bias
 (18)" fillcolor=lightblue]
	140571587702816 -> 140571584809280
	140571584809280 [label=AccumulateGrad]
	140571584809616 -> 140571584809760
	140571584810048 -> 140571584810432
	140571587703296 [label="stage3.2.branches.0.3.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587703296 -> 140571584810048
	140571584810048 [label=AccumulateGrad]
	140571584810576 -> 140571584810720
	140571587703376 [label="stage3.2.branches.0.3.bn1.weight
 (18)" fillcolor=lightblue]
	140571587703376 -> 140571584810576
	140571584810576 [label=AccumulateGrad]
	140571584810768 -> 140571584810720
	140571587703456 [label="stage3.2.branches.0.3.bn1.bias
 (18)" fillcolor=lightblue]
	140571587703456 -> 140571584810768
	140571584810768 [label=AccumulateGrad]
	140571584811104 -> 140571584811296
	140571587703936 [label="stage3.2.branches.0.3.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587703936 -> 140571584811104
	140571584811104 [label=AccumulateGrad]
	140571584811440 -> 140571584811776
	140571587704016 [label="stage3.2.branches.0.3.bn2.weight
 (18)" fillcolor=lightblue]
	140571587704016 -> 140571584811440
	140571584811440 [label=AccumulateGrad]
	140571584811632 -> 140571584811776
	140571587704096 [label="stage3.2.branches.0.3.bn2.bias
 (18)" fillcolor=lightblue]
	140571587704096 -> 140571584811632
	140571584811632 [label=AccumulateGrad]
	140571584811920 -> 140571584812064
	140571584812448 -> 140571584812736
	140571584812448 [label=UpsampleBilinear2DBackward0]
	140571584811392 -> 140571584812448
	140571584811392 [label=NativeBatchNormBackward0]
	140571584809904 -> 140571584811392
	140571584809904 [label=ConvolutionBackward0]
	140571584809952 -> 140571584809904
	140571584809952 [label=ReluBackward0]
	140571584807600 -> 140571584809952
	140571584807600 [label=AddBackward0]
	140571584807936 -> 140571584807600
	140571584807936 [label=NativeBatchNormBackward0]
	140571584807888 -> 140571584807936
	140571584807888 [label=ConvolutionBackward0]
	140571584691120 -> 140571584807888
	140571584691120 [label=ReluBackward0]
	140571584690976 -> 140571584691120
	140571584690976 [label=NativeBatchNormBackward0]
	140571584688432 -> 140571584690976
	140571584688432 [label=ConvolutionBackward0]
	140571584808944 -> 140571584688432
	140571584808944 [label=ReluBackward0]
	140571584685744 -> 140571584808944
	140571584685744 [label=AddBackward0]
	140571584679744 -> 140571584685744
	140571584679744 [label=NativeBatchNormBackward0]
	140571584219360 -> 140571584679744
	140571584219360 [label=ConvolutionBackward0]
	140571584218352 -> 140571584219360
	140571584218352 [label=ReluBackward0]
	140571586147904 -> 140571584218352
	140571586147904 [label=NativeBatchNormBackward0]
	140571586148576 -> 140571586147904
	140571586148576 [label=ConvolutionBackward0]
	140571584685600 -> 140571586148576
	140571584685600 [label=ReluBackward0]
	140571586144832 -> 140571584685600
	140571586144832 [label=AddBackward0]
	140571586145216 -> 140571586144832
	140571586145216 [label=NativeBatchNormBackward0]
	140571586145072 -> 140571586145216
	140571586145072 [label=ConvolutionBackward0]
	140571586142864 -> 140571586145072
	140571586142864 [label=ReluBackward0]
	140571586142192 -> 140571586142864
	140571586142192 [label=NativeBatchNormBackward0]
	140571586140512 -> 140571586142192
	140571586140512 [label=ConvolutionBackward0]
	140571586146176 -> 140571586140512
	140571586146176 [label=ReluBackward0]
	140571586139360 -> 140571586146176
	140571586139360 [label=AddBackward0]
	140571586139312 -> 140571586139360
	140571586139312 [label=NativeBatchNormBackward0]
	140571586138832 -> 140571586139312
	140571586138832 [label=ConvolutionBackward0]
	140571586138496 -> 140571586138832
	140571586138496 [label=ReluBackward0]
	140571586138016 -> 140571586138496
	140571586138016 [label=NativeBatchNormBackward0]
	140571586137968 -> 140571586138016
	140571586137968 [label=ConvolutionBackward0]
	140571586139456 -> 140571586137968
	140571586139456 [label=ReluBackward0]
	140571586137296 -> 140571586139456
	140571586137296 [label=AddBackward0]
	140571586137008 -> 140571586137296
	140571586137008 [label=AddBackward0]
	140571586136768 -> 140571586137008
	140571586136768 [label=NativeBatchNormBackward0]
	140571586136480 -> 140571586136768
	140571586136480 [label=ConvolutionBackward0]
	140571584687760 -> 140571586136480
	140571586136096 -> 140571586136480
	140571587696416 [label="stage3.1.fuse_layers.1.0.0.0.weight
 (36, 18, 3, 3)" fillcolor=lightblue]
	140571587696416 -> 140571586136096
	140571586136096 [label=AccumulateGrad]
	140571586136624 -> 140571586136768
	140571587696496 [label="stage3.1.fuse_layers.1.0.0.1.weight
 (36)" fillcolor=lightblue]
	140571587696496 -> 140571586136624
	140571586136624 [label=AccumulateGrad]
	140571586136816 -> 140571586136768
	140571587696576 [label="stage3.1.fuse_layers.1.0.0.1.bias
 (36)" fillcolor=lightblue]
	140571587696576 -> 140571586136816
	140571586136816 [label=AccumulateGrad]
	140571584685456 -> 140571586137008
	140571586137152 -> 140571586137296
	140571586137152 [label=UpsampleBilinear2DBackward0]
	140571586136144 -> 140571586137152
	140571586136144 [label=NativeBatchNormBackward0]
	140571586136000 -> 140571586136144
	140571586136000 [label=ConvolutionBackward0]
	140571584679024 -> 140571586136000
	140571586135328 -> 140571586136000
	140571587697056 [label="stage3.1.fuse_layers.1.2.0.weight
 (36, 72, 1, 1)" fillcolor=lightblue]
	140571587697056 -> 140571586135328
	140571586135328 [label=AccumulateGrad]
	140571586135808 -> 140571586136144
	140571587696976 [label="stage3.1.fuse_layers.1.2.1.weight
 (36)" fillcolor=lightblue]
	140571587696976 -> 140571586135808
	140571586135808 [label=AccumulateGrad]
	140571586136672 -> 140571586136144
	140571587697136 [label="stage3.1.fuse_layers.1.2.1.bias
 (36)" fillcolor=lightblue]
	140571587697136 -> 140571586136672
	140571586136672 [label=AccumulateGrad]
	140571586137344 -> 140571586137968
	140571587704496 [label="stage3.2.branches.1.0.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571587704496 -> 140571586137344
	140571586137344 [label=AccumulateGrad]
	140571586138112 -> 140571586138016
	140571587704576 [label="stage3.2.branches.1.0.bn1.weight
 (36)" fillcolor=lightblue]
	140571587704576 -> 140571586138112
	140571586138112 [label=AccumulateGrad]
	140571586138352 -> 140571586138016
	140571587704656 [label="stage3.2.branches.1.0.bn1.bias
 (36)" fillcolor=lightblue]
	140571587704656 -> 140571586138352
	140571586138352 [label=AccumulateGrad]
	140571586138640 -> 140571586138832
	140571587705136 [label="stage3.2.branches.1.0.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571587705136 -> 140571586138640
	140571586138640 [label=AccumulateGrad]
	140571586139024 -> 140571586139312
	140571587705216 [label="stage3.2.branches.1.0.bn2.weight
 (36)" fillcolor=lightblue]
	140571587705216 -> 140571586139024
	140571586139024 [label=AccumulateGrad]
	140571586139168 -> 140571586139312
	140571587705296 [label="stage3.2.branches.1.0.bn2.bias
 (36)" fillcolor=lightblue]
	140571587705296 -> 140571586139168
	140571586139168 [label=AccumulateGrad]
	140571586139456 -> 140571586139360
	140571586139696 -> 140571586140512
	140571587705776 [label="stage3.2.branches.1.1.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571587705776 -> 140571586139696
	140571586139696 [label=AccumulateGrad]
	140571586141472 -> 140571586142192
	140571587705856 [label="stage3.2.branches.1.1.bn1.weight
 (36)" fillcolor=lightblue]
	140571587705856 -> 140571586141472
	140571586141472 [label=AccumulateGrad]
	140571586142720 -> 140571586142192
	140571587705936 [label="stage3.2.branches.1.1.bn1.bias
 (36)" fillcolor=lightblue]
	140571587705936 -> 140571586142720
	140571586142720 [label=AccumulateGrad]
	140571586143872 -> 140571586145072
	140571587706416 [label="stage3.2.branches.1.1.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571587706416 -> 140571586143872
	140571586143872 [label=AccumulateGrad]
	140571586144880 -> 140571586145216
	140571587706496 [label="stage3.2.branches.1.1.bn2.weight
 (36)" fillcolor=lightblue]
	140571587706496 -> 140571586144880
	140571586144880 [label=AccumulateGrad]
	140571586144016 -> 140571586145216
	140571587706576 [label="stage3.2.branches.1.1.bn2.bias
 (36)" fillcolor=lightblue]
	140571587706576 -> 140571586144016
	140571586144016 [label=AccumulateGrad]
	140571586146176 -> 140571586144832
	140571586147376 -> 140571586148576
	140571587903728 [label="stage3.2.branches.1.2.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571587903728 -> 140571586147376
	140571586147376 [label=AccumulateGrad]
	140571586146896 -> 140571586147904
	140571587903808 [label="stage3.2.branches.1.2.bn1.weight
 (36)" fillcolor=lightblue]
	140571587903808 -> 140571586146896
	140571586146896 [label=AccumulateGrad]
	140571586148096 -> 140571586147904
	140571587903888 [label="stage3.2.branches.1.2.bn1.bias
 (36)" fillcolor=lightblue]
	140571587903888 -> 140571586148096
	140571586148096 [label=AccumulateGrad]
	140571584219648 -> 140571584219360
	140571587904368 [label="stage3.2.branches.1.2.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571587904368 -> 140571584219648
	140571584219648 [label=AccumulateGrad]
	140571584229104 -> 140571584679744
	140571587904448 [label="stage3.2.branches.1.2.bn2.weight
 (36)" fillcolor=lightblue]
	140571587904448 -> 140571584229104
	140571584229104 [label=AccumulateGrad]
	140571584225600 -> 140571584679744
	140571587904528 [label="stage3.2.branches.1.2.bn2.bias
 (36)" fillcolor=lightblue]
	140571587904528 -> 140571584225600
	140571584225600 [label=AccumulateGrad]
	140571584685600 -> 140571584685744
	140571584688480 -> 140571584688432
	140571587904928 [label="stage3.2.branches.1.3.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571587904928 -> 140571584688480
	140571584688480 [label=AccumulateGrad]
	140571584689152 -> 140571584690976
	140571587905008 [label="stage3.2.branches.1.3.bn1.weight
 (36)" fillcolor=lightblue]
	140571587905008 -> 140571584689152
	140571584689152 [label=AccumulateGrad]
	140571584689920 -> 140571584690976
	140571587905088 [label="stage3.2.branches.1.3.bn1.bias
 (36)" fillcolor=lightblue]
	140571587905088 -> 140571584689920
	140571584689920 [label=AccumulateGrad]
	140571584691840 -> 140571584807888
	140571587905488 [label="stage3.2.branches.1.3.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571587905488 -> 140571584691840
	140571584691840 [label=AccumulateGrad]
	140571584808032 -> 140571584807936
	140571587905568 [label="stage3.2.branches.1.3.bn2.weight
 (36)" fillcolor=lightblue]
	140571587905568 -> 140571584808032
	140571584808032 [label=AccumulateGrad]
	140571584691984 -> 140571584807936
	140571587905648 [label="stage3.2.branches.1.3.bn2.bias
 (36)" fillcolor=lightblue]
	140571587905648 -> 140571584691984
	140571584691984 [label=AccumulateGrad]
	140571584808944 -> 140571584807600
	140571584809088 -> 140571584809904
	140571587910768 [label="stage3.2.fuse_layers.0.1.0.weight
 (18, 36, 1, 1)" fillcolor=lightblue]
	140571587910768 -> 140571584809088
	140571584809088 [label=AccumulateGrad]
	140571584810624 -> 140571584811392
	140571587910848 [label="stage3.2.fuse_layers.0.1.1.weight
 (18)" fillcolor=lightblue]
	140571587910848 -> 140571584810624
	140571584810624 [label=AccumulateGrad]
	140571584811968 -> 140571584811392
	140571587910928 [label="stage3.2.fuse_layers.0.1.1.bias
 (18)" fillcolor=lightblue]
	140571587910928 -> 140571584811968
	140571584811968 [label=AccumulateGrad]
	140571584812640 -> 140571584812784
	140571584812640 [label=UpsampleBilinear2DBackward0]
	140571584811248 -> 140571584812640
	140571584811248 [label=NativeBatchNormBackward0]
	140571584810096 -> 140571584811248
	140571584810096 [label=ConvolutionBackward0]
	140571584688624 -> 140571584810096
	140571584688624 [label=ReluBackward0]
	140571584688816 -> 140571584688624
	140571584688816 [label=AddBackward0]
	140571584687616 -> 140571584688816
	140571584687616 [label=NativeBatchNormBackward0]
	140571584221568 -> 140571584687616
	140571584221568 [label=ConvolutionBackward0]
	140571586146080 -> 140571584221568
	140571586146080 [label=ReluBackward0]
	140571586142528 -> 140571586146080
	140571586142528 [label=NativeBatchNormBackward0]
	140571586138688 -> 140571586142528
	140571586138688 [label=ConvolutionBackward0]
	140571584690496 -> 140571586138688
	140571584690496 [label=ReluBackward0]
	140571586137824 -> 140571584690496
	140571586137824 [label=AddBackward0]
	140571586137488 -> 140571586137824
	140571586137488 [label=NativeBatchNormBackward0]
	140571586135664 -> 140571586137488
	140571586135664 [label=ConvolutionBackward0]
	140571586135136 -> 140571586135664
	140571586135136 [label=ReluBackward0]
	140571586134656 -> 140571586135136
	140571586134656 [label=NativeBatchNormBackward0]
	140571586134608 -> 140571586134656
	140571586134608 [label=ConvolutionBackward0]
	140571586136336 -> 140571586134608
	140571586136336 [label=ReluBackward0]
	140571586134080 -> 140571586136336
	140571586134080 [label=AddBackward0]
	140571586018896 -> 140571586134080
	140571586018896 [label=NativeBatchNormBackward0]
	140571586018656 -> 140571586018896
	140571586018656 [label=ConvolutionBackward0]
	140571586018032 -> 140571586018656
	140571586018032 [label=ReluBackward0]
	140571586017840 -> 140571586018032
	140571586017840 [label=NativeBatchNormBackward0]
	140571586017552 -> 140571586017840
	140571586017552 [label=ConvolutionBackward0]
	140571586019040 -> 140571586017552
	140571586019040 [label=ReluBackward0]
	140571586016880 -> 140571586019040
	140571586016880 [label=AddBackward0]
	140571586016544 -> 140571586016880
	140571586016544 [label=NativeBatchNormBackward0]
	140571586016352 -> 140571586016544
	140571586016352 [label=ConvolutionBackward0]
	140571586015968 -> 140571586016352
	140571586015968 [label=ReluBackward0]
	140571586015536 -> 140571586015968
	140571586015536 [label=NativeBatchNormBackward0]
	140571586015200 -> 140571586015536
	140571586015200 [label=ConvolutionBackward0]
	140571586016688 -> 140571586015200
	140571586016688 [label=ReluBackward0]
	140571586014528 -> 140571586016688
	140571586014528 [label=AddBackward0]
	140571586014480 -> 140571586014528
	140571586014480 [label=AddBackward0]
	140571586014192 -> 140571586014480
	140571586014192 [label=NativeBatchNormBackward0]
	140571586013952 -> 140571586014192
	140571586013952 [label=ConvolutionBackward0]
	140571586013328 -> 140571586013952
	140571586013328 [label=ReluBackward0]
	140571586013136 -> 140571586013328
	140571586013136 [label=NativeBatchNormBackward0]
	140571586012848 -> 140571586013136
	140571586012848 [label=ConvolutionBackward0]
	140571584687760 -> 140571586012848
	140571586012464 -> 140571586012848
	140571587697616 [label="stage3.1.fuse_layers.2.0.0.0.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587697616 -> 140571586012464
	140571586012464 [label=AccumulateGrad]
	140571586012992 -> 140571586013136
	140571587697696 [label="stage3.1.fuse_layers.2.0.0.1.weight
 (18)" fillcolor=lightblue]
	140571587697696 -> 140571586012992
	140571586012992 [label=AccumulateGrad]
	140571586013184 -> 140571586013136
	140571587697776 [label="stage3.1.fuse_layers.2.0.0.1.bias
 (18)" fillcolor=lightblue]
	140571587697776 -> 140571586013184
	140571586013184 [label=AccumulateGrad]
	140571586013520 -> 140571586013952
	140571587698256 [label="stage3.1.fuse_layers.2.0.1.0.weight
 (72, 18, 3, 3)" fillcolor=lightblue]
	140571587698256 -> 140571586013520
	140571586013520 [label=AccumulateGrad]
	140571586013856 -> 140571586014192
	140571587698336 [label="stage3.1.fuse_layers.2.0.1.1.weight
 (72)" fillcolor=lightblue]
	140571587698336 -> 140571586013856
	140571586013856 [label=AccumulateGrad]
	140571586014000 -> 140571586014192
	140571587698416 [label="stage3.1.fuse_layers.2.0.1.1.bias
 (72)" fillcolor=lightblue]
	140571587698416 -> 140571586014000
	140571586014000 [label=AccumulateGrad]
	140571586014336 -> 140571586014480
	140571586014336 [label=NativeBatchNormBackward0]
	140571586012512 -> 140571586014336
	140571586012512 [label=ConvolutionBackward0]
	140571584685456 -> 140571586012512
	140571586012608 -> 140571586012512
	140571587698896 [label="stage3.1.fuse_layers.2.1.0.0.weight
 (72, 36, 3, 3)" fillcolor=lightblue]
	140571587698896 -> 140571586012608
	140571586012608 [label=AccumulateGrad]
	140571586013280 -> 140571586014336
	140571587698976 [label="stage3.1.fuse_layers.2.1.0.1.weight
 (72)" fillcolor=lightblue]
	140571587698976 -> 140571586013280
	140571586013280 [label=AccumulateGrad]
	140571586013808 -> 140571586014336
	140571587699056 [label="stage3.1.fuse_layers.2.1.0.1.bias
 (72)" fillcolor=lightblue]
	140571587699056 -> 140571586013808
	140571586013808 [label=AccumulateGrad]
	140571584679024 -> 140571586014528
	140571586014864 -> 140571586015200
	140571587906128 [label="stage3.2.branches.2.0.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587906128 -> 140571586014864
	140571586014864 [label=AccumulateGrad]
	140571586015344 -> 140571586015536
	140571587906208 [label="stage3.2.branches.2.0.bn1.weight
 (72)" fillcolor=lightblue]
	140571587906208 -> 140571586015344
	140571586015344 [label=AccumulateGrad]
	140571586015824 -> 140571586015536
	140571587906288 [label="stage3.2.branches.2.0.bn1.bias
 (72)" fillcolor=lightblue]
	140571587906288 -> 140571586015824
	140571586015824 [label=AccumulateGrad]
	140571586015872 -> 140571586016352
	140571587906688 [label="stage3.2.branches.2.0.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587906688 -> 140571586015872
	140571586015872 [label=AccumulateGrad]
	140571586016496 -> 140571586016544
	140571587906768 [label="stage3.2.branches.2.0.bn2.weight
 (72)" fillcolor=lightblue]
	140571587906768 -> 140571586016496
	140571586016496 [label=AccumulateGrad]
	140571586016640 -> 140571586016544
	140571587906848 [label="stage3.2.branches.2.0.bn2.bias
 (72)" fillcolor=lightblue]
	140571587906848 -> 140571586016640
	140571586016640 [label=AccumulateGrad]
	140571586016688 -> 140571586016880
	140571586017168 -> 140571586017552
	140571587907248 [label="stage3.2.branches.2.1.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587907248 -> 140571586017168
	140571586017168 [label=AccumulateGrad]
	140571586017696 -> 140571586017840
	140571587907328 [label="stage3.2.branches.2.1.bn1.weight
 (72)" fillcolor=lightblue]
	140571587907328 -> 140571586017696
	140571586017696 [label=AccumulateGrad]
	140571586017888 -> 140571586017840
	140571587907408 [label="stage3.2.branches.2.1.bn1.bias
 (72)" fillcolor=lightblue]
	140571587907408 -> 140571586017888
	140571586017888 [label=AccumulateGrad]
	140571586018224 -> 140571586018656
	140571587907808 [label="stage3.2.branches.2.1.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587907808 -> 140571586018224
	140571586018224 [label=AccumulateGrad]
	140571586018560 -> 140571586018896
	140571587907888 [label="stage3.2.branches.2.1.bn2.weight
 (72)" fillcolor=lightblue]
	140571587907888 -> 140571586018560
	140571586018560 [label=AccumulateGrad]
	140571586018704 -> 140571586018896
	140571587907968 [label="stage3.2.branches.2.1.bn2.bias
 (72)" fillcolor=lightblue]
	140571587907968 -> 140571586018704
	140571586018704 [label=AccumulateGrad]
	140571586019040 -> 140571586134080
	140571586134128 -> 140571586134608
	140571587908448 [label="stage3.2.branches.2.2.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587908448 -> 140571586134128
	140571586134128 [label=AccumulateGrad]
	140571586134752 -> 140571586134656
	140571587908528 [label="stage3.2.branches.2.2.bn1.weight
 (72)" fillcolor=lightblue]
	140571587908528 -> 140571586134752
	140571586134752 [label=AccumulateGrad]
	140571586134992 -> 140571586134656
	140571587908608 [label="stage3.2.branches.2.2.bn1.bias
 (72)" fillcolor=lightblue]
	140571587908608 -> 140571586134992
	140571586134992 [label=AccumulateGrad]
	140571586135424 -> 140571586135664
	140571587909088 [label="stage3.2.branches.2.2.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587909088 -> 140571586135424
	140571586135424 [label=AccumulateGrad]
	140571586135952 -> 140571586137488
	140571587909168 [label="stage3.2.branches.2.2.bn2.weight
 (72)" fillcolor=lightblue]
	140571587909168 -> 140571586135952
	140571586135952 [label=AccumulateGrad]
	140571586137680 -> 140571586137488
	140571587909008 [label="stage3.2.branches.2.2.bn2.bias
 (72)" fillcolor=lightblue]
	140571587909008 -> 140571586137680
	140571586137680 [label=AccumulateGrad]
	140571586136336 -> 140571586137824
	140571586137440 -> 140571586138688
	140571587909568 [label="stage3.2.branches.2.3.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587909568 -> 140571586137440
	140571586137440 [label=AccumulateGrad]
	140571586139840 -> 140571586142528
	140571587909648 [label="stage3.2.branches.2.3.bn1.weight
 (72)" fillcolor=lightblue]
	140571587909648 -> 140571586139840
	140571586139840 [label=AccumulateGrad]
	140571586142816 -> 140571586142528
	140571587909728 [label="stage3.2.branches.2.3.bn1.bias
 (72)" fillcolor=lightblue]
	140571587909728 -> 140571586142816
	140571586142816 [label=AccumulateGrad]
	140571586147232 -> 140571584221568
	140571587910208 [label="stage3.2.branches.2.3.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587910208 -> 140571586147232
	140571586147232 [label=AccumulateGrad]
	140571586145552 -> 140571584687616
	140571587910288 [label="stage3.2.branches.2.3.bn2.weight
 (72)" fillcolor=lightblue]
	140571587910288 -> 140571586145552
	140571586145552 [label=AccumulateGrad]
	140571586150112 -> 140571584687616
	140571587910368 [label="stage3.2.branches.2.3.bn2.bias
 (72)" fillcolor=lightblue]
	140571587910368 -> 140571586150112
	140571586150112 [label=AccumulateGrad]
	140571584690496 -> 140571584688816
	140571584690832 -> 140571584810096
	140571587911328 [label="stage3.2.fuse_layers.0.2.0.weight
 (18, 72, 1, 1)" fillcolor=lightblue]
	140571587911328 -> 140571584690832
	140571584690832 [label=AccumulateGrad]
	140571584808560 -> 140571584811248
	140571587911408 [label="stage3.2.fuse_layers.0.2.1.weight
 (18)" fillcolor=lightblue]
	140571587911408 -> 140571584808560
	140571584808560 [label=AccumulateGrad]
	140571584812592 -> 140571584811248
	140571587911488 [label="stage3.2.fuse_layers.0.2.1.bias
 (18)" fillcolor=lightblue]
	140571587911488 -> 140571584812592
	140571584812592 [label=AccumulateGrad]
	140571584813120 -> 140571584813456
	140571587915008 [label="stage3.3.branches.0.0.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587915008 -> 140571584813120
	140571584813120 [label=AccumulateGrad]
	140571584813648 -> 140571584813792
	140571587915088 [label="stage3.3.branches.0.0.bn1.weight
 (18)" fillcolor=lightblue]
	140571587915088 -> 140571584813648
	140571584813648 [label=AccumulateGrad]
	140571584814080 -> 140571584813792
	140571587915168 [label="stage3.3.branches.0.0.bn1.bias
 (18)" fillcolor=lightblue]
	140571587915168 -> 140571584814080
	140571584814080 [label=AccumulateGrad]
	140571584814128 -> 140571584814608
	140571587915568 [label="stage3.3.branches.0.0.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587915568 -> 140571584814128
	140571584814128 [label=AccumulateGrad]
	140571584814752 -> 140571584814800
	140571587915648 [label="stage3.3.branches.0.0.bn2.weight
 (18)" fillcolor=lightblue]
	140571587915648 -> 140571584814752
	140571584814752 [label=AccumulateGrad]
	140571584814656 -> 140571584814800
	140571587915728 [label="stage3.3.branches.0.0.bn2.bias
 (18)" fillcolor=lightblue]
	140571587915728 -> 140571584814656
	140571584814656 [label=AccumulateGrad]
	140571584814992 -> 140571584815136
	140571584815424 -> 140571584815808
	140571587916208 [label="stage3.3.branches.0.1.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587916208 -> 140571584815424
	140571584815424 [label=AccumulateGrad]
	140571584815952 -> 140571584816096
	140571587916288 [label="stage3.3.branches.0.1.bn1.weight
 (18)" fillcolor=lightblue]
	140571587916288 -> 140571584815952
	140571584815952 [label=AccumulateGrad]
	140571584816144 -> 140571584816096
	140571587916368 [label="stage3.3.branches.0.1.bn1.bias
 (18)" fillcolor=lightblue]
	140571587916368 -> 140571584816144
	140571584816144 [label=AccumulateGrad]
	140571584816480 -> 140571584816672
	140571587916848 [label="stage3.3.branches.0.1.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587916848 -> 140571584816480
	140571584816480 [label=AccumulateGrad]
	140571584816816 -> 140571584817152
	140571587916928 [label="stage3.3.branches.0.1.bn2.weight
 (18)" fillcolor=lightblue]
	140571587916928 -> 140571584816816
	140571584816816 [label=AccumulateGrad]
	140571584817008 -> 140571584817152
	140571587917008 [label="stage3.3.branches.0.1.bn2.bias
 (18)" fillcolor=lightblue]
	140571587917008 -> 140571584817008
	140571584817008 [label=AccumulateGrad]
	140571584817296 -> 140571584817440
	140571584817488 -> 140571584818112
	140571587917488 [label="stage3.3.branches.0.2.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587917488 -> 140571584817488
	140571584817488 [label=AccumulateGrad]
	140571584818016 -> 140571584818160
	140571587917568 [label="stage3.3.branches.0.2.bn1.weight
 (18)" fillcolor=lightblue]
	140571587917568 -> 140571584818016
	140571584818016 [label=AccumulateGrad]
	140571584818496 -> 140571584818160
	140571587917648 [label="stage3.3.branches.0.2.bn1.bias
 (18)" fillcolor=lightblue]
	140571587917648 -> 140571584818496
	140571584818496 [label=AccumulateGrad]
	140571584818784 -> 140571584819024
	140571587918048 [label="stage3.3.branches.0.2.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587918048 -> 140571584818784
	140571584818784 [label=AccumulateGrad]
	140571584819168 -> 140571584819456
	140571587918128 [label="stage3.3.branches.0.2.bn2.weight
 (18)" fillcolor=lightblue]
	140571587918128 -> 140571584819168
	140571584819168 [label=AccumulateGrad]
	140571584819312 -> 140571584819456
	140571587918208 [label="stage3.3.branches.0.2.bn2.bias
 (18)" fillcolor=lightblue]
	140571587918208 -> 140571584819312
	140571584819312 [label=AccumulateGrad]
	140571584819360 -> 140571584819504
	140571584819840 -> 140571584820176
	140571587918608 [label="stage3.3.branches.0.3.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587918608 -> 140571584819840
	140571584819840 [label=AccumulateGrad]
	140571584820368 -> 140571584820512
	140571587918688 [label="stage3.3.branches.0.3.bn1.weight
 (18)" fillcolor=lightblue]
	140571587918688 -> 140571584820368
	140571584820368 [label=AccumulateGrad]
	140571584820800 -> 140571584820512
	140571587918768 [label="stage3.3.branches.0.3.bn1.bias
 (18)" fillcolor=lightblue]
	140571587918768 -> 140571584820800
	140571584820800 [label=AccumulateGrad]
	140571584820848 -> 140571584821328
	140571587919248 [label="stage3.3.branches.0.3.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587919248 -> 140571584820848
	140571584820848 [label=AccumulateGrad]
	140571584821472 -> 140571584821520
	140571587919328 [label="stage3.3.branches.0.3.bn2.weight
 (18)" fillcolor=lightblue]
	140571587919328 -> 140571584821472
	140571584821472 [label=AccumulateGrad]
	140571584821376 -> 140571584821520
	140571587919408 [label="stage3.3.branches.0.3.bn2.bias
 (18)" fillcolor=lightblue]
	140571587919408 -> 140571584821376
	140571584821376 [label=AccumulateGrad]
	140571584821712 -> 140571584821856
	140571584822192 -> 140571584822528
	140571584822192 [label=UpsampleBilinear2DBackward0]
	140571584821184 -> 140571584822192
	140571584821184 [label=NativeBatchNormBackward0]
	140571584819696 -> 140571584821184
	140571584819696 [label=ConvolutionBackward0]
	140571584819984 -> 140571584819696
	140571584819984 [label=ReluBackward0]
	140571584817344 -> 140571584819984
	140571584817344 [label=AddBackward0]
	140571584817968 -> 140571584817344
	140571584817968 [label=NativeBatchNormBackward0]
	140571584817824 -> 140571584817968
	140571584817824 [label=ConvolutionBackward0]
	140571584815664 -> 140571584817824
	140571584815664 [label=ReluBackward0]
	140571584815472 -> 140571584815664
	140571584815472 [label=NativeBatchNormBackward0]
	140571584812976 -> 140571584815472
	140571584812976 [label=ConvolutionBackward0]
	140571584818688 -> 140571584812976
	140571584818688 [label=ReluBackward0]
	140571584810288 -> 140571584818688
	140571584810288 [label=AddBackward0]
	140571584691504 -> 140571584810288
	140571584691504 [label=NativeBatchNormBackward0]
	140571584218016 -> 140571584691504
	140571584218016 [label=ConvolutionBackward0]
	140571586135280 -> 140571584218016
	140571586135280 [label=ReluBackward0]
	140571586134800 -> 140571586135280
	140571586134800 [label=NativeBatchNormBackward0]
	140571586135472 -> 140571586134800
	140571586135472 [label=ConvolutionBackward0]
	140571584689776 -> 140571586135472
	140571584689776 [label=ReluBackward0]
	140571586017024 -> 140571584689776
	140571586017024 [label=AddBackward0]
	140571586017360 -> 140571586017024
	140571586017360 [label=NativeBatchNormBackward0]
	140571586017216 -> 140571586017360
	140571586017216 [label=ConvolutionBackward0]
	140571586015296 -> 140571586017216
	140571586015296 [label=ReluBackward0]
	140571586014624 -> 140571586015296
	140571586014624 [label=NativeBatchNormBackward0]
	140571586012656 -> 140571586014624
	140571586012656 [label=ConvolutionBackward0]
	140571586018368 -> 140571586012656
	140571586018368 [label=ReluBackward0]
	140571586011792 -> 140571586018368
	140571586011792 [label=AddBackward0]
	140571586011504 -> 140571586011792
	140571586011504 [label=NativeBatchNormBackward0]
	140571586011264 -> 140571586011504
	140571586011264 [label=ConvolutionBackward0]
	140571586010640 -> 140571586011264
	140571586010640 [label=ReluBackward0]
	140571586010448 -> 140571586010640
	140571586010448 [label=NativeBatchNormBackward0]
	140571586010160 -> 140571586010448
	140571586010160 [label=ConvolutionBackward0]
	140571586011648 -> 140571586010160
	140571586011648 [label=ReluBackward0]
	140571586009488 -> 140571586011648
	140571586009488 [label=AddBackward0]
	140571586009152 -> 140571586009488
	140571586009152 [label=AddBackward0]
	140571586008960 -> 140571586009152
	140571586008960 [label=NativeBatchNormBackward0]
	140571586008624 -> 140571586008960
	140571586008624 [label=ConvolutionBackward0]
	140571584812304 -> 140571586008624
	140571586008288 -> 140571586008624
	140571587911968 [label="stage3.2.fuse_layers.1.0.0.0.weight
 (36, 18, 3, 3)" fillcolor=lightblue]
	140571587911968 -> 140571586008288
	140571586008288 [label=AccumulateGrad]
	140571586008816 -> 140571586008960
	140571587912048 [label="stage3.2.fuse_layers.1.0.0.1.weight
 (36)" fillcolor=lightblue]
	140571587912048 -> 140571586008816
	140571586008816 [label=AccumulateGrad]
	140571586009248 -> 140571586008960
	140571587912128 [label="stage3.2.fuse_layers.1.0.0.1.bias
 (36)" fillcolor=lightblue]
	140571587912128 -> 140571586009248
	140571586009248 [label=AccumulateGrad]
	140571584809952 -> 140571586009152
	140571586009296 -> 140571586009488
	140571586009296 [label=UpsampleBilinear2DBackward0]
	140571586008576 -> 140571586009296
	140571586008576 [label=NativeBatchNormBackward0]
	140571586008432 -> 140571586008576
	140571586008432 [label=ConvolutionBackward0]
	140571584688624 -> 140571586008432
	140571586007760 -> 140571586008432
	140571587912608 [label="stage3.2.fuse_layers.1.2.0.weight
 (36, 72, 1, 1)" fillcolor=lightblue]
	140571587912608 -> 140571586007760
	140571586007760 [label=AccumulateGrad]
	140571586007952 -> 140571586008576
	140571587912688 [label="stage3.2.fuse_layers.1.2.1.weight
 (36)" fillcolor=lightblue]
	140571587912688 -> 140571586007952
	140571586007952 [label=AccumulateGrad]
	140571586009104 -> 140571586008576
	140571587912768 [label="stage3.2.fuse_layers.1.2.1.bias
 (36)" fillcolor=lightblue]
	140571587912768 -> 140571586009104
	140571586009104 [label=AccumulateGrad]
	140571586009776 -> 140571586010160
	140571588149328 [label="stage3.3.branches.1.0.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571588149328 -> 140571586009776
	140571586009776 [label=AccumulateGrad]
	140571586010304 -> 140571586010448
	140571588149408 [label="stage3.3.branches.1.0.bn1.weight
 (36)" fillcolor=lightblue]
	140571588149408 -> 140571586010304
	140571586010304 [label=AccumulateGrad]
	140571586010496 -> 140571586010448
	140571588149488 [label="stage3.3.branches.1.0.bn1.bias
 (36)" fillcolor=lightblue]
	140571588149488 -> 140571586010496
	140571586010496 [label=AccumulateGrad]
	140571586010832 -> 140571586011264
	140571588149968 [label="stage3.3.branches.1.0.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571588149968 -> 140571586010832
	140571586010832 [label=AccumulateGrad]
	140571586011168 -> 140571586011504
	140571588150048 [label="stage3.3.branches.1.0.bn2.weight
 (36)" fillcolor=lightblue]
	140571588150048 -> 140571586011168
	140571586011168 [label=AccumulateGrad]
	140571586011312 -> 140571586011504
	140571588150128 [label="stage3.3.branches.1.0.bn2.bias
 (36)" fillcolor=lightblue]
	140571588150128 -> 140571586011312
	140571586011312 [label=AccumulateGrad]
	140571586011648 -> 140571586011792
	140571586011840 -> 140571586012656
	140571588150608 [label="stage3.3.branches.1.1.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571588150608 -> 140571586011840
	140571586011840 [label=AccumulateGrad]
	140571586013664 -> 140571586014624
	140571588150688 [label="stage3.3.branches.1.1.bn1.weight
 (36)" fillcolor=lightblue]
	140571588150688 -> 140571586013664
	140571586013664 [label=AccumulateGrad]
	140571586015152 -> 140571586014624
	140571588150768 [label="stage3.3.branches.1.1.bn1.bias
 (36)" fillcolor=lightblue]
	140571588150768 -> 140571586015152
	140571586015152 [label=AccumulateGrad]
	140571586016016 -> 140571586017216
	140571588151248 [label="stage3.3.branches.1.1.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571588151248 -> 140571586016016
	140571586016016 [label=AccumulateGrad]
	140571586017312 -> 140571586017360
	140571588151328 [label="stage3.3.branches.1.1.bn2.weight
 (36)" fillcolor=lightblue]
	140571588151328 -> 140571586017312
	140571586017312 [label=AccumulateGrad]
	140571586016208 -> 140571586017360
	140571588151408 [label="stage3.3.branches.1.1.bn2.bias
 (36)" fillcolor=lightblue]
	140571588151408 -> 140571586016208
	140571586016208 [label=AccumulateGrad]
	140571586018368 -> 140571586017024
	140571586134320 -> 140571586135472
	140571588151808 [label="stage3.3.branches.1.2.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571588151808 -> 140571586134320
	140571586134320 [label=AccumulateGrad]
	140571586134176 -> 140571586134800
	140571588151888 [label="stage3.3.branches.1.2.bn1.weight
 (36)" fillcolor=lightblue]
	140571588151888 -> 140571586134176
	140571586134176 [label=AccumulateGrad]
	140571586138160 -> 140571586134800
	140571588151968 [label="stage3.3.branches.1.2.bn1.bias
 (36)" fillcolor=lightblue]
	140571588151968 -> 140571586138160
	140571586138160 [label=AccumulateGrad]
	140571586140128 -> 140571584218016
	140571588152448 [label="stage3.3.branches.1.2.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571588152448 -> 140571586140128
	140571586140128 [label=AccumulateGrad]
	140571586139504 -> 140571584691504
	140571588152528 [label="stage3.3.branches.1.2.bn2.weight
 (36)" fillcolor=lightblue]
	140571588152528 -> 140571586139504
	140571586139504 [label=AccumulateGrad]
	140571586147520 -> 140571584691504
	140571588152608 [label="stage3.3.branches.1.2.bn2.bias
 (36)" fillcolor=lightblue]
	140571588152608 -> 140571586147520
	140571586147520 [label=AccumulateGrad]
	140571584689776 -> 140571584810288
	140571584813264 -> 140571584812976
	140571588153088 [label="stage3.3.branches.1.3.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571588153088 -> 140571584813264
	140571584813264 [label=AccumulateGrad]
	140571584813936 -> 140571584815472
	140571588153168 [label="stage3.3.branches.1.3.bn1.weight
 (36)" fillcolor=lightblue]
	140571588153168 -> 140571584813936
	140571584813936 [label=AccumulateGrad]
	140571584814464 -> 140571584815472
	140571588153248 [label="stage3.3.branches.1.3.bn1.bias
 (36)" fillcolor=lightblue]
	140571588153248 -> 140571584814464
	140571584814464 [label=AccumulateGrad]
	140571584816624 -> 140571584817824
	140571588153728 [label="stage3.3.branches.1.3.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571588153728 -> 140571584816624
	140571584816624 [label=AccumulateGrad]
	140571584817680 -> 140571584817968
	140571588153808 [label="stage3.3.branches.1.3.bn2.weight
 (36)" fillcolor=lightblue]
	140571588153808 -> 140571584817680
	140571584817680 [label=AccumulateGrad]
	140571584816768 -> 140571584817968
	140571588153888 [label="stage3.3.branches.1.3.bn2.bias
 (36)" fillcolor=lightblue]
	140571588153888 -> 140571584816768
	140571584816768 [label=AccumulateGrad]
	140571584818688 -> 140571584817344
	140571584818832 -> 140571584819696
	140571588159248 [label="stage3.3.fuse_layers.0.1.0.weight
 (18, 36, 1, 1)" fillcolor=lightblue]
	140571588159248 -> 140571584818832
	140571584818832 [label=AccumulateGrad]
	140571584820656 -> 140571584821184
	140571588159328 [label="stage3.3.fuse_layers.0.1.1.weight
 (18)" fillcolor=lightblue]
	140571588159328 -> 140571584820656
	140571584820656 [label=AccumulateGrad]
	140571584822000 -> 140571584821184
	140571588159408 [label="stage3.3.fuse_layers.0.1.1.bias
 (18)" fillcolor=lightblue]
	140571588159408 -> 140571584822000
	140571584822000 [label=AccumulateGrad]
	140571584822672 -> 140571584822816
	140571584822672 [label=UpsampleBilinear2DBackward0]
	140571584821040 -> 140571584822672
	140571584821040 [label=NativeBatchNormBackward0]
	140571584816000 -> 140571584821040
	140571584816000 [label=ConvolutionBackward0]
	140571584813408 -> 140571584816000
	140571584813408 [label=ReluBackward0]
	140571584813312 -> 140571584813408
	140571584813312 [label=AddBackward0]
	140571584812112 -> 140571584813312
	140571584812112 [label=NativeBatchNormBackward0]
	140571586143488 -> 140571584812112
	140571586143488 [label=ConvolutionBackward0]
	140571586018512 -> 140571586143488
	140571586018512 [label=ReluBackward0]
	140571586014672 -> 140571586018512
	140571586014672 [label=NativeBatchNormBackward0]
	140571586011120 -> 140571586014672
	140571586011120 [label=ConvolutionBackward0]
	140571584815280 -> 140571586011120
	140571584815280 [label=ReluBackward0]
	140571586009968 -> 140571584815280
	140571586009968 [label=AddBackward0]
	140571586009920 -> 140571586009968
	140571586009920 [label=NativeBatchNormBackward0]
	140571586007808 -> 140571586009920
	140571586007808 [label=ConvolutionBackward0]
	140571586007280 -> 140571586007808
	140571586007280 [label=ReluBackward0]
	140571586007088 -> 140571586007280
	140571586007088 [label=NativeBatchNormBackward0]
	140571586006800 -> 140571586007088
	140571586006800 [label=ConvolutionBackward0]
	140571586008480 -> 140571586006800
	140571586008480 [label=ReluBackward0]
	140571586006128 -> 140571586008480
	140571586006128 [label=AddBackward0]
	140571586005792 -> 140571586006128
	140571586005792 [label=NativeBatchNormBackward0]
	140571586005600 -> 140571586005792
	140571586005600 [label=ConvolutionBackward0]
	140571586005216 -> 140571586005600
	140571586005216 [label=ReluBackward0]
	140571586004784 -> 140571586005216
	140571586004784 [label=NativeBatchNormBackward0]
	140571586004448 -> 140571586004784
	140571586004448 [label=ConvolutionBackward0]
	140571586005936 -> 140571586004448
	140571586005936 [label=ReluBackward0]
	140571586003776 -> 140571586005936
	140571586003776 [label=AddBackward0]
	140571586003728 -> 140571586003776
	140571586003728 [label=NativeBatchNormBackward0]
	140571586003248 -> 140571586003728
	140571586003248 [label=ConvolutionBackward0]
	140571862034864 -> 140571586003248
	140571862034864 [label=ReluBackward0]
	140574983992224 -> 140571862034864
	140574983992224 [label=NativeBatchNormBackward0]
	140571608269248 -> 140574983992224
	140571608269248 [label=ConvolutionBackward0]
	140571586003872 -> 140571608269248
	140571586003872 [label=ReluBackward0]
	140571592564720 -> 140571586003872
	140571592564720 [label=AddBackward0]
	140571600879376 -> 140571592564720
	140571600879376 [label=AddBackward0]
	140571588973552 -> 140571600879376
	140571588973552 [label=NativeBatchNormBackward0]
	140571588979120 -> 140571588973552
	140571588979120 [label=ConvolutionBackward0]
	140571588975760 -> 140571588979120
	140571588975760 [label=ReluBackward0]
	140571588976288 -> 140571588975760
	140571588976288 [label=NativeBatchNormBackward0]
	140571588975472 -> 140571588976288
	140571588975472 [label=ConvolutionBackward0]
	140571584812304 -> 140571588975472
	140571588974944 -> 140571588975472
	140571587913168 [label="stage3.2.fuse_layers.2.0.0.0.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587913168 -> 140571588974944
	140571588974944 [label=AccumulateGrad]
	140571588976240 -> 140571588976288
	140571587913248 [label="stage3.2.fuse_layers.2.0.0.1.weight
 (18)" fillcolor=lightblue]
	140571587913248 -> 140571588976240
	140571588976240 [label=AccumulateGrad]
	140571588976576 -> 140571588976288
	140571587913328 [label="stage3.2.fuse_layers.2.0.0.1.bias
 (18)" fillcolor=lightblue]
	140571587913328 -> 140571588976576
	140571588976576 [label=AccumulateGrad]
	140571588975328 -> 140571588979120
	140571587913808 [label="stage3.2.fuse_layers.2.0.1.0.weight
 (72, 18, 3, 3)" fillcolor=lightblue]
	140571587913808 -> 140571588975328
	140571588975328 [label=AccumulateGrad]
	140571588976720 -> 140571588973552
	140571587913888 [label="stage3.2.fuse_layers.2.0.1.1.weight
 (72)" fillcolor=lightblue]
	140571587913888 -> 140571588976720
	140571588976720 [label=AccumulateGrad]
	140571588976816 -> 140571588973552
	140571587913968 [label="stage3.2.fuse_layers.2.0.1.1.bias
 (72)" fillcolor=lightblue]
	140571587913968 -> 140571588976816
	140571588976816 [label=AccumulateGrad]
	140571588977392 -> 140571600879376
	140571588977392 [label=NativeBatchNormBackward0]
	140571588975568 -> 140571588977392
	140571588975568 [label=ConvolutionBackward0]
	140571584809952 -> 140571588975568
	140571588975376 -> 140571588975568
	140571587914368 [label="stage3.2.fuse_layers.2.1.0.0.weight
 (72, 36, 3, 3)" fillcolor=lightblue]
	140571587914368 -> 140571588975376
	140571588975376 [label=AccumulateGrad]
	140571588976048 -> 140571588977392
	140571587914448 [label="stage3.2.fuse_layers.2.1.0.1.weight
 (72)" fillcolor=lightblue]
	140571587914448 -> 140571588976048
	140571588976048 [label=AccumulateGrad]
	140571588978592 -> 140571588977392
	140571587914528 [label="stage3.2.fuse_layers.2.1.0.1.bias
 (72)" fillcolor=lightblue]
	140571587914528 -> 140571588978592
	140571588978592 [label=AccumulateGrad]
	140571584688624 -> 140571592564720
	140571895461200 -> 140571608269248
	140571588154368 [label="stage3.3.branches.2.0.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571588154368 -> 140571895461200
	140571895461200 [label=AccumulateGrad]
	140571608270880 -> 140574983992224
	140571588154448 [label="stage3.3.branches.2.0.bn1.weight
 (72)" fillcolor=lightblue]
	140571588154448 -> 140571608270880
	140571608270880 [label=AccumulateGrad]
	140571600998784 -> 140574983992224
	140571588154528 [label="stage3.3.branches.2.0.bn1.bias
 (72)" fillcolor=lightblue]
	140571588154528 -> 140571600998784
	140571600998784 [label=AccumulateGrad]
	140571586003056 -> 140571586003248
	140571588154928 [label="stage3.3.branches.2.0.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571588154928 -> 140571586003056
	140571586003056 [label=AccumulateGrad]
	140571586003440 -> 140571586003728
	140571588155008 [label="stage3.3.branches.2.0.bn2.weight
 (72)" fillcolor=lightblue]
	140571588155008 -> 140571586003440
	140571586003440 [label=AccumulateGrad]
	140571586003584 -> 140571586003728
	140571588155088 [label="stage3.3.branches.2.0.bn2.bias
 (72)" fillcolor=lightblue]
	140571588155088 -> 140571586003584
	140571586003584 [label=AccumulateGrad]
	140571586003872 -> 140571586003776
	140571586004112 -> 140571586004448
	140571588155568 [label="stage3.3.branches.2.1.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571588155568 -> 140571586004112
	140571586004112 [label=AccumulateGrad]
	140571586004592 -> 140571586004784
	140571588155648 [label="stage3.3.branches.2.1.bn1.weight
 (72)" fillcolor=lightblue]
	140571588155648 -> 140571586004592
	140571586004592 [label=AccumulateGrad]
	140571586005072 -> 140571586004784
	140571588155728 [label="stage3.3.branches.2.1.bn1.bias
 (72)" fillcolor=lightblue]
	140571588155728 -> 140571586005072
	140571586005072 [label=AccumulateGrad]
	140571586005120 -> 140571586005600
	140571588156208 [label="stage3.3.branches.2.1.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571588156208 -> 140571586005120
	140571586005120 [label=AccumulateGrad]
	140571586005744 -> 140571586005792
	140571588156288 [label="stage3.3.branches.2.1.bn2.weight
 (72)" fillcolor=lightblue]
	140571588156288 -> 140571586005744
	140571586005744 [label=AccumulateGrad]
	140571586005888 -> 140571586005792
	140571588156368 [label="stage3.3.branches.2.1.bn2.bias
 (72)" fillcolor=lightblue]
	140571588156368 -> 140571586005888
	140571586005888 [label=AccumulateGrad]
	140571586005936 -> 140571586006128
	140571586006416 -> 140571586006800
	140571588156848 [label="stage3.3.branches.2.2.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571588156848 -> 140571586006416
	140571586006416 [label=AccumulateGrad]
	140571586006944 -> 140571586007088
	140571588156928 [label="stage3.3.branches.2.2.bn1.weight
 (72)" fillcolor=lightblue]
	140571588156928 -> 140571586006944
	140571586006944 [label=AccumulateGrad]
	140571586007136 -> 140571586007088
	140571588157008 [label="stage3.3.branches.2.2.bn1.bias
 (72)" fillcolor=lightblue]
	140571588157008 -> 140571586007136
	140571586007136 [label=AccumulateGrad]
	140571586007616 -> 140571586007808
	140571588157488 [label="stage3.3.branches.2.2.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571588157488 -> 140571586007616
	140571586007616 [label=AccumulateGrad]
	140571586008144 -> 140571586009920
	140571588157568 [label="stage3.3.branches.2.2.bn2.weight
 (72)" fillcolor=lightblue]
	140571588157568 -> 140571586008144
	140571586008144 [label=AccumulateGrad]
	140571586009824 -> 140571586009920
	140571588157648 [label="stage3.3.branches.2.2.bn2.bias
 (72)" fillcolor=lightblue]
	140571588157648 -> 140571586009824
	140571586009824 [label=AccumulateGrad]
	140571586008480 -> 140571586009968
	140571586009632 -> 140571586011120
	140571588157968 [label="stage3.3.branches.2.3.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571588157968 -> 140571586009632
	140571586009632 [label=AccumulateGrad]
	140571586011984 -> 140571586014672
	140571588158048 [label="stage3.3.branches.2.3.bn1.weight
 (72)" fillcolor=lightblue]
	140571588158048 -> 140571586011984
	140571586011984 [label=AccumulateGrad]
	140571586015008 -> 140571586014672
	140571588158128 [label="stage3.3.branches.2.3.bn1.bias
 (72)" fillcolor=lightblue]
	140571588158128 -> 140571586015008
	140571586015008 [label=AccumulateGrad]
	140571586019184 -> 140571586143488
	140571588158608 [label="stage3.3.branches.2.3.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571588158608 -> 140571586019184
	140571586019184 [label=AccumulateGrad]
	140571586134464 -> 140571584812112
	140571588158688 [label="stage3.3.branches.2.3.bn2.weight
 (72)" fillcolor=lightblue]
	140571588158688 -> 140571586134464
	140571586134464 [label=AccumulateGrad]
	140571586139984 -> 140571584812112
	140571588158768 [label="stage3.3.branches.2.3.bn2.bias
 (72)" fillcolor=lightblue]
	140571588158768 -> 140571586139984
	140571586139984 [label=AccumulateGrad]
	140571584815280 -> 140571584813312
	140571584815328 -> 140571584816000
	140571588159888 [label="stage3.3.fuse_layers.0.2.0.weight
 (18, 72, 1, 1)" fillcolor=lightblue]
	140571588159888 -> 140571584815328
	140571584815328 [label=AccumulateGrad]
	140571584818352 -> 140571584821040
	140571588159968 [label="stage3.3.fuse_layers.0.2.1.weight
 (18)" fillcolor=lightblue]
	140571588159968 -> 140571584818352
	140571584818352 [label=AccumulateGrad]
	140571584822384 -> 140571584821040
	140571588160048 [label="stage3.3.fuse_layers.0.2.1.bias
 (18)" fillcolor=lightblue]
	140571588160048 -> 140571584822384
	140571584822384 [label=AccumulateGrad]
	140571584822864 -> 140571584971008
	140571588164128 [label="stage4.0.branches.0.0.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571588164128 -> 140571584822864
	140571584822864 [label=AccumulateGrad]
	140571584970912 -> 140571584971056
	140571588164208 [label="stage4.0.branches.0.0.bn1.weight
 (18)" fillcolor=lightblue]
	140571588164208 -> 140571584970912
	140571584970912 [label=AccumulateGrad]
	140571584971392 -> 140571584971056
	140571588164288 [label="stage4.0.branches.0.0.bn1.bias
 (18)" fillcolor=lightblue]
	140571588164288 -> 140571584971392
	140571584971392 [label=AccumulateGrad]
	140571584971680 -> 140571584971920
	140571588164768 [label="stage4.0.branches.0.0.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571588164768 -> 140571584971680
	140571584971680 [label=AccumulateGrad]
	140571584972064 -> 140571584972352
	140571588164848 [label="stage4.0.branches.0.0.bn2.weight
 (18)" fillcolor=lightblue]
	140571588164848 -> 140571584972064
	140571584972064 [label=AccumulateGrad]
	140571584972208 -> 140571584972352
	140571588164928 [label="stage4.0.branches.0.0.bn2.bias
 (18)" fillcolor=lightblue]
	140571588164928 -> 140571584972208
	140571584972208 [label=AccumulateGrad]
	140571584972256 -> 140571584972400
	140571584972736 -> 140571584973072
	140571588165408 [label="stage4.0.branches.0.1.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571588165408 -> 140571584972736
	140571584972736 [label=AccumulateGrad]
	140571584973264 -> 140571584973408
	140571588165488 [label="stage4.0.branches.0.1.bn1.weight
 (18)" fillcolor=lightblue]
	140571588165488 -> 140571584973264
	140571584973264 [label=AccumulateGrad]
	140571584973696 -> 140571584973408
	140571588165568 [label="stage4.0.branches.0.1.bn1.bias
 (18)" fillcolor=lightblue]
	140571588165568 -> 140571584973696
	140571584973696 [label=AccumulateGrad]
	140571584973744 -> 140571584974224
	140571586314720 [label="stage4.0.branches.0.1.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586314720 -> 140571584973744
	140571584973744 [label=AccumulateGrad]
	140571584974368 -> 140571584974416
	140571586314800 [label="stage4.0.branches.0.1.bn2.weight
 (18)" fillcolor=lightblue]
	140571586314800 -> 140571584974368
	140571584974368 [label=AccumulateGrad]
	140571584974272 -> 140571584974416
	140571586314880 [label="stage4.0.branches.0.1.bn2.bias
 (18)" fillcolor=lightblue]
	140571586314880 -> 140571584974272
	140571584974272 [label=AccumulateGrad]
	140571584974608 -> 140571584974752
	140571584975040 -> 140571584975424
	140571586315360 [label="stage4.0.branches.0.2.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586315360 -> 140571584975040
	140571584975040 [label=AccumulateGrad]
	140571584975568 -> 140571584975712
	140571586315440 [label="stage4.0.branches.0.2.bn1.weight
 (18)" fillcolor=lightblue]
	140571586315440 -> 140571584975568
	140571584975568 [label=AccumulateGrad]
	140571584975760 -> 140571584975712
	140571586315520 [label="stage4.0.branches.0.2.bn1.bias
 (18)" fillcolor=lightblue]
	140571586315520 -> 140571584975760
	140571584975760 [label=AccumulateGrad]
	140571584976096 -> 140571584976288
	140571586316000 [label="stage4.0.branches.0.2.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586316000 -> 140571584976096
	140571584976096 [label=AccumulateGrad]
	140571584976432 -> 140571584976768
	140571586316080 [label="stage4.0.branches.0.2.bn2.weight
 (18)" fillcolor=lightblue]
	140571586316080 -> 140571584976432
	140571584976432 [label=AccumulateGrad]
	140571584976624 -> 140571584976768
	140571586316160 [label="stage4.0.branches.0.2.bn2.bias
 (18)" fillcolor=lightblue]
	140571586316160 -> 140571584976624
	140571584976624 [label=AccumulateGrad]
	140571584976912 -> 140571584977056
	140571584977104 -> 140571584977728
	140571586316640 [label="stage4.0.branches.0.3.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586316640 -> 140571584977104
	140571584977104 [label=AccumulateGrad]
	140571584977632 -> 140571584977776
	140571586316720 [label="stage4.0.branches.0.3.bn1.weight
 (18)" fillcolor=lightblue]
	140571586316720 -> 140571584977632
	140571584977632 [label=AccumulateGrad]
	140571584978112 -> 140571584977776
	140571586316800 [label="stage4.0.branches.0.3.bn1.bias
 (18)" fillcolor=lightblue]
	140571586316800 -> 140571584978112
	140571584978112 [label=AccumulateGrad]
	140571584978400 -> 140571584978640
	140571586317280 [label="stage4.0.branches.0.3.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586317280 -> 140571584978400
	140571584978400 [label=AccumulateGrad]
	140571584978784 -> 140571584979072
	140571586317360 [label="stage4.0.branches.0.3.bn2.weight
 (18)" fillcolor=lightblue]
	140571586317360 -> 140571584978784
	140571584978784 [label=AccumulateGrad]
	140571584978928 -> 140571584979072
	140571586317440 [label="stage4.0.branches.0.3.bn2.bias
 (18)" fillcolor=lightblue]
	140571586317440 -> 140571584978928
	140571584978928 [label=AccumulateGrad]
	140571584978976 -> 140571584979120
	140571584979744 -> 140571584979792
	140571584979744 [label=UpsampleBilinear2DBackward0]
	140571584978448 -> 140571584979744
	140571584978448 [label=NativeBatchNormBackward0]
	140571584976960 -> 140571584978448
	140571584976960 [label=ConvolutionBackward0]
	140571584977296 -> 140571584976960
	140571584977296 [label=ReluBackward0]
	140571584974896 -> 140571584977296
	140571584974896 [label=AddBackward0]
	140571584975280 -> 140571584974896
	140571584975280 [label=NativeBatchNormBackward0]
	140571584975088 -> 140571584975280
	140571584975088 [label=ConvolutionBackward0]
	140571584972928 -> 140571584975088
	140571584972928 [label=ReluBackward0]
	140571584973024 -> 140571584972928
	140571584973024 [label=NativeBatchNormBackward0]
	140571584970864 -> 140571584973024
	140571584970864 [label=ConvolutionBackward0]
	140571584976240 -> 140571584970864
	140571584976240 [label=ReluBackward0]
	140571584820032 -> 140571584976240
	140571584820032 [label=AddBackward0]
	140571584820128 -> 140571584820032
	140571584820128 [label=NativeBatchNormBackward0]
	140571600878848 -> 140571584820128
	140571600878848 [label=ConvolutionBackward0]
	140571586012320 -> 140571600878848
	140571586012320 [label=ReluBackward0]
	140571586012176 -> 140571586012320
	140571586012176 [label=NativeBatchNormBackward0]
	140571586007904 -> 140571586012176
	140571586007904 [label=ConvolutionBackward0]
	140571584814320 -> 140571586007904
	140571584814320 [label=ReluBackward0]
	140571586005456 -> 140571584814320
	140571586005456 [label=AddBackward0]
	140571586004928 -> 140571586005456
	140571586004928 [label=NativeBatchNormBackward0]
	140571586003920 -> 140571586004928
	140571586003920 [label=ConvolutionBackward0]
	140574983471264 -> 140571586003920
	140574983471264 [label=ReluBackward0]
	140571592572784 -> 140574983471264
	140571592572784 [label=NativeBatchNormBackward0]
	140571588976096 -> 140571592572784
	140571588976096 [label=ConvolutionBackward0]
	140571586005264 -> 140571588976096
	140571586005264 [label=ReluBackward0]
	140571588974800 -> 140571586005264
	140571588974800 [label=AddBackward0]
	140571588973408 -> 140571588974800
	140571588973408 [label=NativeBatchNormBackward0]
	140571588973360 -> 140571588973408
	140571588973360 [label=ConvolutionBackward0]
	140571588977296 -> 140571588973360
	140571588977296 [label=ReluBackward0]
	140571588974080 -> 140571588977296
	140571588974080 [label=NativeBatchNormBackward0]
	140571588973792 -> 140571588974080
	140571588973792 [label=ConvolutionBackward0]
	140571588974320 -> 140571588973792
	140571588974320 [label=ReluBackward0]
	140571588974560 -> 140571588974320
	140571588974560 [label=AddBackward0]
	140571588973840 -> 140571588974560
	140571588973840 [label=AddBackward0]
	140571588974368 -> 140571588973840
	140571588974368 [label=NativeBatchNormBackward0]
	140571588973072 -> 140571588974368
	140571588973072 [label=ConvolutionBackward0]
	140571584822048 -> 140571588973072
	140571588975952 -> 140571588973072
	140571588160528 [label="stage3.3.fuse_layers.1.0.0.0.weight
 (36, 18, 3, 3)" fillcolor=lightblue]
	140571588160528 -> 140571588975952
	140571588975952 [label=AccumulateGrad]
	140571588974752 -> 140571588974368
	140571588160608 [label="stage3.3.fuse_layers.1.0.0.1.weight
 (36)" fillcolor=lightblue]
	140571588160608 -> 140571588974752
	140571588974752 [label=AccumulateGrad]
	140571588974608 -> 140571588974368
	140571588160688 [label="stage3.3.fuse_layers.1.0.0.1.bias
 (36)" fillcolor=lightblue]
	140571588160688 -> 140571588974608
	140571588974608 [label=AccumulateGrad]
	140571584819984 -> 140571588973840
	140571588974272 -> 140571588974560
	140571588974272 [label=UpsampleBilinear2DBackward0]
	140571588975184 -> 140571588974272
	140571588975184 [label=NativeBatchNormBackward0]
	140571588975808 -> 140571588975184
	140571588975808 [label=ConvolutionBackward0]
	140571584813408 -> 140571588975808
	140571588976192 -> 140571588975808
	140571588161168 [label="stage3.3.fuse_layers.1.2.0.weight
 (36, 72, 1, 1)" fillcolor=lightblue]
	140571588161168 -> 140571588976192
	140571588976192 [label=AccumulateGrad]
	140571588975712 -> 140571588975184
	140571588161248 [label="stage3.3.fuse_layers.1.2.1.weight
 (36)" fillcolor=lightblue]
	140571588161248 -> 140571588975712
	140571588975712 [label=AccumulateGrad]
	140571588975088 -> 140571588975184
	140571588161328 [label="stage3.3.fuse_layers.1.2.1.bias
 (36)" fillcolor=lightblue]
	140571588161328 -> 140571588975088
	140571588975088 [label=AccumulateGrad]
	140571588974176 -> 140571588973792
	140571586317920 [label="stage4.0.branches.1.0.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586317920 -> 140571588974176
	140571588974176 [label=AccumulateGrad]
	140571588972640 -> 140571588974080
	140571586318000 [label="stage4.0.branches.1.0.bn1.weight
 (36)" fillcolor=lightblue]
	140571586318000 -> 140571588972640
	140571588972640 [label=AccumulateGrad]
	140571588977584 -> 140571588974080
	140571586318080 [label="stage4.0.branches.1.0.bn1.bias
 (36)" fillcolor=lightblue]
	140571586318080 -> 140571588977584
	140571588977584 [label=AccumulateGrad]
	140571588978352 -> 140571588973360
	140571586318560 [label="stage4.0.branches.1.0.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586318560 -> 140571588978352
	140571588978352 [label=AccumulateGrad]
	140571588973648 -> 140571588973408
	140571586318640 [label="stage4.0.branches.1.0.bn2.weight
 (36)" fillcolor=lightblue]
	140571586318640 -> 140571588973648
	140571588973648 [label=AccumulateGrad]
	140571588973120 -> 140571588973408
	140571586318720 [label="stage4.0.branches.1.0.bn2.bias
 (36)" fillcolor=lightblue]
	140571586318720 -> 140571588973120
	140571588973120 [label=AccumulateGrad]
	140571588974320 -> 140571588974800
	140571588974656 -> 140571588976096
	140571586319200 [label="stage4.0.branches.1.1.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586319200 -> 140571588974656
	140571588974656 [label=AccumulateGrad]
	140571588976960 -> 140571592572784
	140571586319280 [label="stage4.0.branches.1.1.bn1.weight
 (36)" fillcolor=lightblue]
	140571586319280 -> 140571588976960
	140571588976960 [label=AccumulateGrad]
	140571588972736 -> 140571592572784
	140571586319360 [label="stage4.0.branches.1.1.bn1.bias
 (36)" fillcolor=lightblue]
	140571586319360 -> 140571588972736
	140571588972736 [label=AccumulateGrad]
	140571586003200 -> 140571586003920
	140571586319840 [label="stage4.0.branches.1.1.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586319840 -> 140571586003200
	140571586003200 [label=AccumulateGrad]
	140571586004256 -> 140571586004928
	140571586319920 [label="stage4.0.branches.1.1.bn2.weight
 (36)" fillcolor=lightblue]
	140571586319920 -> 140571586004256
	140571586004256 [label=AccumulateGrad]
	140571586004544 -> 140571586004928
	140571586320000 [label="stage4.0.branches.1.1.bn2.bias
 (36)" fillcolor=lightblue]
	140571586320000 -> 140571586004544
	140571586004544 [label=AccumulateGrad]
	140571586005264 -> 140571586005456
	140571586006272 -> 140571586007904
	140571586320480 [label="stage4.0.branches.1.2.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586320480 -> 140571586006272
	140571586006272 [label=AccumulateGrad]
	140571586007472 -> 140571586012176
	140571586320560 [label="stage4.0.branches.1.2.bn1.weight
 (36)" fillcolor=lightblue]
	140571586320560 -> 140571586007472
	140571586007472 [label=AccumulateGrad]
	140571586010592 -> 140571586012176
	140571586320640 [label="stage4.0.branches.1.2.bn1.bias
 (36)" fillcolor=lightblue]
	140571586320640 -> 140571586010592
	140571586010592 [label=AccumulateGrad]
	140571586011936 -> 140571600878848
	140571586321040 [label="stage4.0.branches.1.2.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586321040 -> 140571586011936
	140571586011936 [label=AccumulateGrad]
	140571586138784 -> 140571584820128
	140571586321120 [label="stage4.0.branches.1.2.bn2.weight
 (36)" fillcolor=lightblue]
	140571586321120 -> 140571586138784
	140571586138784 [label=AccumulateGrad]
	140571592821344 -> 140571584820128
	140571586321200 [label="stage4.0.branches.1.2.bn2.bias
 (36)" fillcolor=lightblue]
	140571586321200 -> 140571592821344
	140571592821344 [label=AccumulateGrad]
	140571584814320 -> 140571584820032
	140571584823056 -> 140571584970864
	140571586321680 [label="stage4.0.branches.1.3.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586321680 -> 140571584823056
	140571584823056 [label=AccumulateGrad]
	140571584971248 -> 140571584973024
	140571586321760 [label="stage4.0.branches.1.3.bn1.weight
 (36)" fillcolor=lightblue]
	140571586321760 -> 140571584971248
	140571584971248 [label=AccumulateGrad]
	140571584971728 -> 140571584973024
	140571586321840 [label="stage4.0.branches.1.3.bn1.bias
 (36)" fillcolor=lightblue]
	140571586321840 -> 140571584971728
	140571584971728 [label=AccumulateGrad]
	140571584973936 -> 140571584975088
	140571586322320 [label="stage4.0.branches.1.3.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586322320 -> 140571584973936
	140571584973936 [label=AccumulateGrad]
	140571584974944 -> 140571584975280
	140571586322400 [label="stage4.0.branches.1.3.bn2.weight
 (36)" fillcolor=lightblue]
	140571586322400 -> 140571584974944
	140571584974944 [label=AccumulateGrad]
	140571584974080 -> 140571584975280
	140571586322480 [label="stage4.0.branches.1.3.bn2.bias
 (36)" fillcolor=lightblue]
	140571586322480 -> 140571584974080
	140571584974080 [label=AccumulateGrad]
	140571584976240 -> 140571584974896
	140571584976384 -> 140571584976960
	140571586513328 [label="stage4.0.fuse_layers.0.1.0.weight
 (18, 36, 1, 1)" fillcolor=lightblue]
	140571586513328 -> 140571584976384
	140571584976384 [label=AccumulateGrad]
	140571584977968 -> 140571584978448
	140571586513408 [label="stage4.0.fuse_layers.0.1.1.weight
 (18)" fillcolor=lightblue]
	140571586513408 -> 140571584977968
	140571584977968 [label=AccumulateGrad]
	140571584979312 -> 140571584978448
	140571586513488 [label="stage4.0.fuse_layers.0.1.1.bias
 (18)" fillcolor=lightblue]
	140571586513488 -> 140571584979312
	140571584979312 [label=AccumulateGrad]
	140571584979984 -> 140571584980272
	140571584979984 [label=UpsampleBilinear2DBackward0]
	140571584978304 -> 140571584979984
	140571584978304 [label=NativeBatchNormBackward0]
	140571584973552 -> 140571584978304
	140571584973552 [label=ConvolutionBackward0]
	140571584971584 -> 140571584973552
	140571584971584 [label=ReluBackward0]
	140571584972592 -> 140571584971584
	140571584972592 [label=AddBackward0]
	140571584822144 -> 140571584972592
	140571584822144 [label=NativeBatchNormBackward0]
	140571586015680 -> 140571584822144
	140571586015680 [label=ConvolutionBackward0]
	140571586006608 -> 140571586015680
	140571586006608 [label=ReluBackward0]
	140571586003104 -> 140571586006608
	140571586003104 [label=NativeBatchNormBackward0]
	140571588973312 -> 140571586003104
	140571588973312 [label=ConvolutionBackward0]
	140571584823200 -> 140571588973312
	140571584823200 [label=ReluBackward0]
	140571588973984 -> 140571584823200
	140571588973984 [label=AddBackward0]
	140571588973936 -> 140571588973984
	140571588973936 [label=NativeBatchNormBackward0]
	140571588975856 -> 140571588973936
	140571588975856 [label=ConvolutionBackward0]
	140571588976432 -> 140571588975856
	140571588976432 [label=ReluBackward0]
	140571588979600 -> 140571588976432
	140571588979600 [label=NativeBatchNormBackward0]
	140571588974224 -> 140571588979600
	140571588974224 [label=ConvolutionBackward0]
	140571588974848 -> 140571588974224
	140571588974848 [label=ReluBackward0]
	140571588978400 -> 140571588974848
	140571588978400 [label=AddBackward0]
	140571588976864 -> 140571588978400
	140571588976864 [label=NativeBatchNormBackward0]
	140571860081232 -> 140571588976864
	140571860081232 [label=ConvolutionBackward0]
	140571591997712 -> 140571860081232
	140571591997712 [label=ReluBackward0]
	140571618222688 -> 140571591997712
	140571618222688 [label=NativeBatchNormBackward0]
	140571585871296 -> 140571618222688
	140571585871296 [label=ConvolutionBackward0]
	140571860081280 -> 140571585871296
	140571860081280 [label=ReluBackward0]
	140571585870624 -> 140571860081280
	140571585870624 [label=AddBackward0]
	140571585870576 -> 140571585870624
	140571585870576 [label=NativeBatchNormBackward0]
	140571585870096 -> 140571585870576
	140571585870096 [label=ConvolutionBackward0]
	140571585869760 -> 140571585870096
	140571585869760 [label=ReluBackward0]
	140571585869280 -> 140571585869760
	140571585869280 [label=NativeBatchNormBackward0]
	140571585869232 -> 140571585869280
	140571585869232 [label=ConvolutionBackward0]
	140571585870720 -> 140571585869232
	140571585870720 [label=ReluBackward0]
	140571585868560 -> 140571585870720
	140571585868560 [label=AddBackward0]
	140571585868272 -> 140571585868560
	140571585868272 [label=AddBackward0]
	140571585867936 -> 140571585868272
	140571585867936 [label=NativeBatchNormBackward0]
	140571585867744 -> 140571585867936
	140571585867744 [label=ConvolutionBackward0]
	140571585867360 -> 140571585867744
	140571585867360 [label=ReluBackward0]
	140571585866928 -> 140571585867360
	140571585866928 [label=NativeBatchNormBackward0]
	140571585866592 -> 140571585866928
	140571585866592 [label=ConvolutionBackward0]
	140571584822048 -> 140571585866592
	140571585866256 -> 140571585866592
	140571588161808 [label="stage3.3.fuse_layers.2.0.0.0.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571588161808 -> 140571585866256
	140571585866256 [label=AccumulateGrad]
	140571585866736 -> 140571585866928
	140571588161888 [label="stage3.3.fuse_layers.2.0.0.1.weight
 (18)" fillcolor=lightblue]
	140571588161888 -> 140571585866736
	140571585866736 [label=AccumulateGrad]
	140571585867216 -> 140571585866928
	140571588161968 [label="stage3.3.fuse_layers.2.0.0.1.bias
 (18)" fillcolor=lightblue]
	140571588161968 -> 140571585867216
	140571585867216 [label=AccumulateGrad]
	140571585867264 -> 140571585867744
	140571588162368 [label="stage3.3.fuse_layers.2.0.1.0.weight
 (72, 18, 3, 3)" fillcolor=lightblue]
	140571588162368 -> 140571585867264
	140571585867264 [label=AccumulateGrad]
	140571585867888 -> 140571585867936
	140571588162448 [label="stage3.3.fuse_layers.2.0.1.1.weight
 (72)" fillcolor=lightblue]
	140571588162448 -> 140571585867888
	140571585867888 [label=AccumulateGrad]
	140571585868032 -> 140571585867936
	140571588162528 [label="stage3.3.fuse_layers.2.0.1.1.bias
 (72)" fillcolor=lightblue]
	140571588162528 -> 140571585868032
	140571585868032 [label=AccumulateGrad]
	140571585868080 -> 140571585868272
	140571585868080 [label=NativeBatchNormBackward0]
	140571585866544 -> 140571585868080
	140571585866544 [label=ConvolutionBackward0]
	140571584819984 -> 140571585866544
	140571585866400 -> 140571585866544
	140571588163008 [label="stage3.3.fuse_layers.2.1.0.0.weight
 (72, 36, 3, 3)" fillcolor=lightblue]
	140571588163008 -> 140571585866400
	140571585866400 [label=AccumulateGrad]
	140571585867072 -> 140571585868080
	140571588163088 [label="stage3.3.fuse_layers.2.1.0.1.weight
 (72)" fillcolor=lightblue]
	140571588163088 -> 140571585867072
	140571585867072 [label=AccumulateGrad]
	140571585867600 -> 140571585868080
	140571588163168 [label="stage3.3.fuse_layers.2.1.0.1.bias
 (72)" fillcolor=lightblue]
	140571588163168 -> 140571585867600
	140571585867600 [label=AccumulateGrad]
	140571584813408 -> 140571585868560
	140571585868608 -> 140571585869232
	140571586322960 [label="stage4.0.branches.2.0.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571586322960 -> 140571585868608
	140571585868608 [label=AccumulateGrad]
	140571585869376 -> 140571585869280
	140571586323040 [label="stage4.0.branches.2.0.bn1.weight
 (72)" fillcolor=lightblue]
	140571586323040 -> 140571585869376
	140571585869376 [label=AccumulateGrad]
	140571585869616 -> 140571585869280
	140571586323120 [label="stage4.0.branches.2.0.bn1.bias
 (72)" fillcolor=lightblue]
	140571586323120 -> 140571585869616
	140571585869616 [label=AccumulateGrad]
	140571585869904 -> 140571585870096
	140571586323520 [label="stage4.0.branches.2.0.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571586323520 -> 140571585869904
	140571585869904 [label=AccumulateGrad]
	140571585870288 -> 140571585870576
	140571586323600 [label="stage4.0.branches.2.0.bn2.weight
 (72)" fillcolor=lightblue]
	140571586323600 -> 140571585870288
	140571585870288 [label=AccumulateGrad]
	140571585870432 -> 140571585870576
	140571586323680 [label="stage4.0.branches.2.0.bn2.bias
 (72)" fillcolor=lightblue]
	140571586323680 -> 140571585870432
	140571585870432 [label=AccumulateGrad]
	140571585870720 -> 140571585870624
	140571585870960 -> 140571585871296
	140571586324160 [label="stage4.0.branches.2.1.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571586324160 -> 140571585870960
	140571585870960 [label=AccumulateGrad]
	140571585871440 -> 140571618222688
	140571586324240 [label="stage4.0.branches.2.1.bn1.weight
 (72)" fillcolor=lightblue]
	140571586324240 -> 140571585871440
	140571585871440 [label=AccumulateGrad]
	140571585871776 -> 140571618222688
	140571586324320 [label="stage4.0.branches.2.1.bn1.bias
 (72)" fillcolor=lightblue]
	140571586324320 -> 140571585871776
	140571585871776 [label=AccumulateGrad]
	140571860081136 -> 140571860081232
	140571586324800 [label="stage4.0.branches.2.1.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571586324800 -> 140571860081136
	140571860081136 [label=AccumulateGrad]
	140571860081184 -> 140571588976864
	140571586324880 [label="stage4.0.branches.2.1.bn2.weight
 (72)" fillcolor=lightblue]
	140571586324880 -> 140571860081184
	140571860081184 [label=AccumulateGrad]
	140571860081424 -> 140571588976864
	140571586324960 [label="stage4.0.branches.2.1.bn2.bias
 (72)" fillcolor=lightblue]
	140571586324960 -> 140571860081424
	140571860081424 [label=AccumulateGrad]
	140571860081280 -> 140571588978400
	140571588977200 -> 140571588974224
	140571586325360 [label="stage4.0.branches.2.2.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571586325360 -> 140571588977200
	140571588977200 [label=AccumulateGrad]
	140571588978736 -> 140571588979600
	140571586325440 [label="stage4.0.branches.2.2.bn1.weight
 (72)" fillcolor=lightblue]
	140571586325440 -> 140571588978736
	140571588978736 [label=AccumulateGrad]
	140571588978880 -> 140571588979600
	140571586325520 [label="stage4.0.branches.2.2.bn1.bias
 (72)" fillcolor=lightblue]
	140571586325520 -> 140571588978880
	140571588978880 [label=AccumulateGrad]
	140571588976144 -> 140571588975856
	140571586326000 [label="stage4.0.branches.2.2.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571586326000 -> 140571588976144
	140571588976144 [label=AccumulateGrad]
	140571588975664 -> 140571588973936
	140571586326080 [label="stage4.0.branches.2.2.bn2.weight
 (72)" fillcolor=lightblue]
	140571586326080 -> 140571588975664
	140571588975664 [label=AccumulateGrad]
	140571588973600 -> 140571588973936
	140571586326160 [label="stage4.0.branches.2.2.bn2.bias
 (72)" fillcolor=lightblue]
	140571586326160 -> 140571588973600
	140571588973600 [label=AccumulateGrad]
	140571588974848 -> 140571588973984
	140571588974416 -> 140571588973312
	140571586326640 [label="stage4.0.branches.2.3.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571586326640 -> 140571588974416
	140571588974416 [label=AccumulateGrad]
	140571588974464 -> 140571586003104
	140571586326720 [label="stage4.0.branches.2.3.bn1.weight
 (72)" fillcolor=lightblue]
	140571586326720 -> 140571588974464
	140571588974464 [label=AccumulateGrad]
	140571588978208 -> 140571586003104
	140571586326800 [label="stage4.0.branches.2.3.bn1.bias
 (72)" fillcolor=lightblue]
	140571586326800 -> 140571588978208
	140571588978208 [label=AccumulateGrad]
	140571586006560 -> 140571586015680
	140571586327280 [label="stage4.0.branches.2.3.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571586327280 -> 140571586006560
	140571586006560 [label=AccumulateGrad]
	140571586006464 -> 140571584822144
	140571586327360 [label="stage4.0.branches.2.3.bn2.weight
 (72)" fillcolor=lightblue]
	140571586327360 -> 140571586006464
	140571586006464 [label=AccumulateGrad]
	140571586010976 -> 140571584822144
	140571586327440 [label="stage4.0.branches.2.3.bn2.bias
 (72)" fillcolor=lightblue]
	140571586327440 -> 140571586010976
	140571586010976 [label=AccumulateGrad]
	140571584823200 -> 140571584972592
	140571584972880 -> 140571584973552
	140571586513968 [label="stage4.0.fuse_layers.0.2.0.weight
 (18, 72, 1, 1)" fillcolor=lightblue]
	140571586513968 -> 140571584972880
	140571584972880 [label=AccumulateGrad]
	140571584975616 -> 140571584978304
	140571586514048 [label="stage4.0.fuse_layers.0.2.1.weight
 (18)" fillcolor=lightblue]
	140571586514048 -> 140571584975616
	140571584975616 [label=AccumulateGrad]
	140571584979648 -> 140571584978304
	140571586514128 [label="stage4.0.fuse_layers.0.2.1.bias
 (18)" fillcolor=lightblue]
	140571586514128 -> 140571584979648
	140571584979648 [label=AccumulateGrad]
	140571584980416 -> 140571584980320
	140571584980416 [label=UpsampleBilinear2DBackward0]
	140571584977584 -> 140571584980416
	140571584977584 [label=NativeBatchNormBackward0]
	140571584822720 -> 140571584977584
	140571584822720 [label=ConvolutionBackward0]
	140571586004400 -> 140571584822720
	140571586004400 [label=ReluBackward0]
	140571588976336 -> 140571586004400
	140571588976336 [label=AddBackward0]
	140571588974128 -> 140571588976336
	140571588974128 [label=NativeBatchNormBackward0]
	140571588976000 -> 140571588974128
	140571588976000 [label=ConvolutionBackward0]
	140571588977488 -> 140571588976000
	140571588977488 [label=ReluBackward0]
	140571860082336 -> 140571588977488
	140571860082336 [label=NativeBatchNormBackward0]
	140571585869952 -> 140571860082336
	140571585869952 [label=ConvolutionBackward0]
	140571588971056 -> 140571585869952
	140571588971056 [label=ReluBackward0]
	140571585869088 -> 140571588971056
	140571585869088 [label=AddBackward0]
	140571585868752 -> 140571585869088
	140571585868752 [label=NativeBatchNormBackward0]
	140571585866688 -> 140571585868752
	140571585866688 [label=ConvolutionBackward0]
	140571585865872 -> 140571585866688
	140571585865872 [label=ReluBackward0]
	140571585865392 -> 140571585865872
	140571585865392 [label=NativeBatchNormBackward0]
	140571585865344 -> 140571585865392
	140571585865344 [label=ConvolutionBackward0]
	140571585868944 -> 140571585865344
	140571585868944 [label=ReluBackward0]
	140571585864672 -> 140571585868944
	140571585864672 [label=AddBackward0]
	140571585864384 -> 140571585864672
	140571585864384 [label=NativeBatchNormBackward0]
	140571585863904 -> 140571585864384
	140571585863904 [label=ConvolutionBackward0]
	140571585863568 -> 140571585863904
	140571585863568 [label=ReluBackward0]
	140571585863328 -> 140571585863568
	140571585863328 [label=NativeBatchNormBackward0]
	140571585863040 -> 140571585863328
	140571585863040 [label=ConvolutionBackward0]
	140571585864528 -> 140571585863040
	140571585864528 [label=ReluBackward0]
	140571585862368 -> 140571585864528
	140571585862368 [label=AddBackward0]
	140571585862032 -> 140571585862368
	140571585862032 [label=NativeBatchNormBackward0]
	140571585861840 -> 140571585862032
	140571585861840 [label=ConvolutionBackward0]
	140571585861216 -> 140571585861840
	140571585861216 [label=ReluBackward0]
	140571585861024 -> 140571585861216
	140571585861024 [label=NativeBatchNormBackward0]
	140571585860688 -> 140571585861024
	140571585860688 [label=ConvolutionBackward0]
	140571585862224 -> 140571585860688
	140571585862224 [label=ReluBackward0]
	140571585860016 -> 140571585862224
	140571585860016 [label=NativeBatchNormBackward0]
	140571585859968 -> 140571585860016
	140571585859968 [label=ConvolutionBackward0]
	140571585870720 -> 140571585859968
	140571585859344 -> 140571585859968
	140571588163488 [label="transition3.3.0.0.weight
 (144, 72, 3, 3)" fillcolor=lightblue]
	140571588163488 -> 140571585859344
	140571585859344 [label=AccumulateGrad]
	140571585859872 -> 140571585860016
	140571588163568 [label="transition3.3.0.1.weight
 (144)" fillcolor=lightblue]
	140571588163568 -> 140571585859872
	140571585859872 [label=AccumulateGrad]
	140571585860640 -> 140571585860016
	140571588163648 [label="transition3.3.0.1.bias
 (144)" fillcolor=lightblue]
	140571588163648 -> 140571585860640
	140571585860640 [label=AccumulateGrad]
	140571585860352 -> 140571585860688
	140571586327920 [label="stage4.0.branches.3.0.conv1.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571586327920 -> 140571585860352
	140571585860352 [label=AccumulateGrad]
	140571585860880 -> 140571585861024
	140571586328000 [label="stage4.0.branches.3.0.bn1.weight
 (144)" fillcolor=lightblue]
	140571586328000 -> 140571585860880
	140571585860880 [label=AccumulateGrad]
	140571585861312 -> 140571585861024
	140571586328080 [label="stage4.0.branches.3.0.bn1.bias
 (144)" fillcolor=lightblue]
	140571586328080 -> 140571585861312
	140571585861312 [label=AccumulateGrad]
	140571585861360 -> 140571585861840
	140571586328560 [label="stage4.0.branches.3.0.conv2.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571586328560 -> 140571585861360
	140571585861360 [label=AccumulateGrad]
	140571585861984 -> 140571585862032
	140571586328640 [label="stage4.0.branches.3.0.bn2.weight
 (144)" fillcolor=lightblue]
	140571586328640 -> 140571585861984
	140571585861984 [label=AccumulateGrad]
	140571585861888 -> 140571585862032
	140571586328720 [label="stage4.0.branches.3.0.bn2.bias
 (144)" fillcolor=lightblue]
	140571586328720 -> 140571585861888
	140571585861888 [label=AccumulateGrad]
	140571585862224 -> 140571585862368
	140571585862656 -> 140571585863040
	140571586329200 [label="stage4.0.branches.3.1.conv1.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571586329200 -> 140571585862656
	140571585862656 [label=AccumulateGrad]
	140571585863184 -> 140571585863328
	140571586329280 [label="stage4.0.branches.3.1.bn1.weight
 (144)" fillcolor=lightblue]
	140571586329280 -> 140571585863184
	140571585863184 [label=AccumulateGrad]
	140571585863376 -> 140571585863328
	140571586329360 [label="stage4.0.branches.3.1.bn1.bias
 (144)" fillcolor=lightblue]
	140571586329360 -> 140571585863376
	140571585863376 [label=AccumulateGrad]
	140571585863712 -> 140571585863904
	140571586329840 [label="stage4.0.branches.3.1.conv2.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571586329840 -> 140571585863712
	140571585863712 [label=AccumulateGrad]
	140571585864048 -> 140571585864384
	140571586329920 [label="stage4.0.branches.3.1.bn2.weight
 (144)" fillcolor=lightblue]
	140571586329920 -> 140571585864048
	140571585864048 [label=AccumulateGrad]
	140571585864240 -> 140571585864384
	140571586330000 [label="stage4.0.branches.3.1.bn2.bias
 (144)" fillcolor=lightblue]
	140571586330000 -> 140571585864240
	140571585864240 [label=AccumulateGrad]
	140571585864528 -> 140571585864672
	140571585864720 -> 140571585865344
	140571586330480 [label="stage4.0.branches.3.2.conv1.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571586330480 -> 140571585864720
	140571585864720 [label=AccumulateGrad]
	140571585865248 -> 140571585865392
	140571586330560 [label="stage4.0.branches.3.2.bn1.weight
 (144)" fillcolor=lightblue]
	140571586330560 -> 140571585865248
	140571585865248 [label=AccumulateGrad]
	140571585865728 -> 140571585865392
	140571586510928 [label="stage4.0.branches.3.2.bn1.bias
 (144)" fillcolor=lightblue]
	140571586510928 -> 140571585865728
	140571585865728 [label=AccumulateGrad]
	140571585866064 -> 140571585866688
	140571586511408 [label="stage4.0.branches.3.2.conv2.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571586511408 -> 140571585866064
	140571585866064 [label=AccumulateGrad]
	140571585867408 -> 140571585868752
	140571586511488 [label="stage4.0.branches.3.2.bn2.weight
 (144)" fillcolor=lightblue]
	140571586511488 -> 140571585867408
	140571585867408 [label=AccumulateGrad]
	140571585868416 -> 140571585868752
	140571586511568 [label="stage4.0.branches.3.2.bn2.bias
 (144)" fillcolor=lightblue]
	140571586511568 -> 140571585868416
	140571585868416 [label=AccumulateGrad]
	140571585868944 -> 140571585869088
	140571585868704 -> 140571585869952
	140571586512048 [label="stage4.0.branches.3.3.conv1.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571586512048 -> 140571585868704
	140571585868704 [label=AccumulateGrad]
	140571585871392 -> 140571860082336
	140571586512128 [label="stage4.0.branches.3.3.bn1.weight
 (144)" fillcolor=lightblue]
	140571586512128 -> 140571585871392
	140571585871392 [label=AccumulateGrad]
	140571585871632 -> 140571860082336
	140571586512208 [label="stage4.0.branches.3.3.bn1.bias
 (144)" fillcolor=lightblue]
	140571586512208 -> 140571585871632
	140571585871632 [label=AccumulateGrad]
	140571860081088 -> 140571588976000
	140571586512688 [label="stage4.0.branches.3.3.conv2.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571586512688 -> 140571860081088
	140571860081088 [label=AccumulateGrad]
	140571588976624 -> 140571588974128
	140571586512768 [label="stage4.0.branches.3.3.bn2.weight
 (144)" fillcolor=lightblue]
	140571586512768 -> 140571588976624
	140571588976624 [label=AccumulateGrad]
	140571588975520 -> 140571588974128
	140571586512848 [label="stage4.0.branches.3.3.bn2.bias
 (144)" fillcolor=lightblue]
	140571586512848 -> 140571588975520
	140571588975520 [label=AccumulateGrad]
	140571588971056 -> 140571588976336
	140571586017984 -> 140571584822720
	140571586514608 [label="stage4.0.fuse_layers.0.3.0.weight
 (18, 144, 1, 1)" fillcolor=lightblue]
	140571586514608 -> 140571586017984
	140571586017984 [label=AccumulateGrad]
	140571584977440 -> 140571584977584
	140571586514688 [label="stage4.0.fuse_layers.0.3.1.weight
 (18)" fillcolor=lightblue]
	140571586514688 -> 140571584977440
	140571584977440 [label=AccumulateGrad]
	140571584980128 -> 140571584977584
	140571586514768 [label="stage4.0.fuse_layers.0.3.1.bias
 (18)" fillcolor=lightblue]
	140571586514768 -> 140571584980128
	140571584980128 [label=AccumulateGrad]
	140571584980656 -> 140571584980992
	140571586523088 [label="stage4.1.branches.0.0.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586523088 -> 140571584980656
	140571584980656 [label=AccumulateGrad]
	140571584981136 -> 140571584981328
	140571586523168 [label="stage4.1.branches.0.0.bn1.weight
 (18)" fillcolor=lightblue]
	140571586523168 -> 140571584981136
	140571584981136 [label=AccumulateGrad]
	140571584981616 -> 140571584981328
	140571586523248 [label="stage4.1.branches.0.0.bn1.bias
 (18)" fillcolor=lightblue]
	140571586523248 -> 140571584981616
	140571584981616 [label=AccumulateGrad]
	140571584981664 -> 140571584982144
	140571586523728 [label="stage4.1.branches.0.0.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586523728 -> 140571584981664
	140571584981664 [label=AccumulateGrad]
	140571584982288 -> 140571584982336
	140571586523808 [label="stage4.1.branches.0.0.bn2.weight
 (18)" fillcolor=lightblue]
	140571586523808 -> 140571584982288
	140571584982288 [label=AccumulateGrad]
	140571584982432 -> 140571584982336
	140571586523888 [label="stage4.1.branches.0.0.bn2.bias
 (18)" fillcolor=lightblue]
	140571586523888 -> 140571584982432
	140571584982432 [label=AccumulateGrad]
	140571584982480 -> 140571584982672
	140571584982960 -> 140571584983344
	140571586524288 [label="stage4.1.branches.0.1.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586524288 -> 140571584982960
	140571584982960 [label=AccumulateGrad]
	140571584983488 -> 140571584983632
	140571586524368 [label="stage4.1.branches.0.1.bn1.weight
 (18)" fillcolor=lightblue]
	140571586524368 -> 140571584983488
	140571584983488 [label=AccumulateGrad]
	140571584983680 -> 140571584983632
	140571586524448 [label="stage4.1.branches.0.1.bn1.bias
 (18)" fillcolor=lightblue]
	140571586524448 -> 140571584983680
	140571584983680 [label=AccumulateGrad]
	140571584984016 -> 140571584984448
	140571586524848 [label="stage4.1.branches.0.1.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586524848 -> 140571584984016
	140571584984016 [label=AccumulateGrad]
	140571584984352 -> 140571584984688
	140571586524928 [label="stage4.1.branches.0.1.bn2.weight
 (18)" fillcolor=lightblue]
	140571586524928 -> 140571584984352
	140571584984352 [label=AccumulateGrad]
	140571584984496 -> 140571584984688
	140571586525008 [label="stage4.1.branches.0.1.bn2.bias
 (18)" fillcolor=lightblue]
	140571586525008 -> 140571584984496
	140571584984496 [label=AccumulateGrad]
	140571584984832 -> 140571584984976
	140571584985024 -> 140571584985648
	140571586525408 [label="stage4.1.branches.0.2.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586525408 -> 140571584985024
	140571584985024 [label=AccumulateGrad]
	140571584985792 -> 140571584985696
	140571586525488 [label="stage4.1.branches.0.2.bn1.weight
 (18)" fillcolor=lightblue]
	140571586525488 -> 140571584985792
	140571584985792 [label=AccumulateGrad]
	140571584986032 -> 140571584985696
	140571586525568 [label="stage4.1.branches.0.2.bn1.bias
 (18)" fillcolor=lightblue]
	140571586525568 -> 140571584986032
	140571584986032 [label=AccumulateGrad]
	140571584986320 -> 140571584986512
	140571586526048 [label="stage4.1.branches.0.2.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586526048 -> 140571584986320
	140571584986320 [label=AccumulateGrad]
	140571584986704 -> 140571585118272
	140571586526128 [label="stage4.1.branches.0.2.bn2.weight
 (18)" fillcolor=lightblue]
	140571586526128 -> 140571584986704
	140571584986704 [label=AccumulateGrad]
	140571584986848 -> 140571585118272
	140571586526208 [label="stage4.1.branches.0.2.bn2.bias
 (18)" fillcolor=lightblue]
	140571586526208 -> 140571584986848
	140571584986848 [label=AccumulateGrad]
	140571585118800 -> 140571585118320
	140571585118512 -> 140571585118848
	140571586526688 [label="stage4.1.branches.0.3.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586526688 -> 140571585118512
	140571585118512 [label=AccumulateGrad]
	140571585118992 -> 140571585119184
	140571586526768 [label="stage4.1.branches.0.3.bn1.weight
 (18)" fillcolor=lightblue]
	140571586526768 -> 140571585118992
	140571585118992 [label=AccumulateGrad]
	140571585119472 -> 140571585119184
	140571586526848 [label="stage4.1.branches.0.3.bn1.bias
 (18)" fillcolor=lightblue]
	140571586526848 -> 140571585119472
	140571585119472 [label=AccumulateGrad]
	140571585119520 -> 140571585120000
	140571586773152 [label="stage4.1.branches.0.3.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586773152 -> 140571585119520
	140571585119520 [label=AccumulateGrad]
	140571585120144 -> 140571585120192
	140571586773232 [label="stage4.1.branches.0.3.bn2.weight
 (18)" fillcolor=lightblue]
	140571586773232 -> 140571585120144
	140571585120144 [label=AccumulateGrad]
	140571585120288 -> 140571585120192
	140571586773312 [label="stage4.1.branches.0.3.bn2.bias
 (18)" fillcolor=lightblue]
	140571586773312 -> 140571585120288
	140571585120288 [label=AccumulateGrad]
	140571585120336 -> 140571585120528
	140571585120864 -> 140571585121200
	140571585120864 [label=UpsampleBilinear2DBackward0]
	140571585119856 -> 140571585120864
	140571585119856 [label=NativeBatchNormBackward0]
	140571585118656 -> 140571585119856
	140571585118656 [label=ConvolutionBackward0]
	140571585118368 -> 140571585118656
	140571585118368 [label=ReluBackward0]
	140571584985120 -> 140571585118368
	140571584985120 [label=AddBackward0]
	140571584985504 -> 140571584985120
	140571584985504 [label=NativeBatchNormBackward0]
	140571584985360 -> 140571584985504
	140571584985360 [label=ConvolutionBackward0]
	140571584983152 -> 140571584985360
	140571584983152 [label=ReluBackward0]
	140571584983008 -> 140571584983152
	140571584983008 [label=NativeBatchNormBackward0]
	140571584980464 -> 140571584983008
	140571584980464 [label=ConvolutionBackward0]
	140571584986464 -> 140571584980464
	140571584986464 [label=ReluBackward0]
	140571586007232 -> 140571584986464
	140571586007232 [label=AddBackward0]
	140571584981088 -> 140571586007232
	140571584981088 [label=NativeBatchNormBackward0]
	140571860081328 -> 140571584981088
	140571860081328 [label=ConvolutionBackward0]
	140571585866016 -> 140571860081328
	140571585866016 [label=ReluBackward0]
	140571585865584 -> 140571585866016
	140571585865584 [label=NativeBatchNormBackward0]
	140571585865920 -> 140571585865584
	140571585865920 [label=ConvolutionBackward0]
	140571588978976 -> 140571585865920
	140571588978976 [label=ReluBackward0]
	140571585862512 -> 140571588978976
	140571585862512 [label=AddBackward0]
	140571585862896 -> 140571585862512
	140571585862896 [label=NativeBatchNormBackward0]
	140571585862704 -> 140571585862896
	140571585862704 [label=ConvolutionBackward0]
	140571585860544 -> 140571585862704
	140571585860544 [label=ReluBackward0]
	140571585859824 -> 140571585860544
	140571585859824 [label=NativeBatchNormBackward0]
	140571585859536 -> 140571585859824
	140571585859536 [label=ConvolutionBackward0]
	140571585863856 -> 140571585859536
	140571585863856 [label=ReluBackward0]
	140571585858528 -> 140571585863856
	140571585858528 [label=AddBackward0]
	140571585858480 -> 140571585858528
	140571585858480 [label=NativeBatchNormBackward0]
	140571585858000 -> 140571585858480
	140571585858000 [label=ConvolutionBackward0]
	140571585857664 -> 140571585858000
	140571585857664 [label=ReluBackward0]
	140571585857232 -> 140571585857664
	140571585857232 [label=NativeBatchNormBackward0]
	140571585857136 -> 140571585857232
	140571585857136 [label=ConvolutionBackward0]
	140571585858624 -> 140571585857136
	140571585858624 [label=ReluBackward0]
	140571585856464 -> 140571585858624
	140571585856464 [label=AddBackward0]
	140571585856176 -> 140571585856464
	140571585856176 [label=AddBackward0]
	140571585855936 -> 140571585856176
	140571585855936 [label=AddBackward0]
	140571585855600 -> 140571585855936
	140571585855600 [label=NativeBatchNormBackward0]
	140571585855744 -> 140571585855600
	140571585855744 [label=ConvolutionBackward0]
	140571584979600 -> 140571585855744
	140571585856128 -> 140571585855744
	140571586515168 [label="stage4.0.fuse_layers.1.0.0.0.weight
 (36, 18, 3, 3)" fillcolor=lightblue]
	140571586515168 -> 140571585856128
	140571585856128 [label=AccumulateGrad]
	140571585855552 -> 140571585855600
	140571586515248 [label="stage4.0.fuse_layers.1.0.0.1.weight
 (36)" fillcolor=lightblue]
	140571586515248 -> 140571585855552
	140571585855552 [label=AccumulateGrad]
	140571585855792 -> 140571585855600
	140571586515328 [label="stage4.0.fuse_layers.1.0.0.1.bias
 (36)" fillcolor=lightblue]
	140571586515328 -> 140571585855792
	140571585855792 [label=AccumulateGrad]
	140571584977296 -> 140571585855936
	140571585855840 -> 140571585856176
	140571585855840 [label=UpsampleBilinear2DBackward0]
	140571585855888 -> 140571585855840
	140571585855888 [label=NativeBatchNormBackward0]
	140571585856032 -> 140571585855888
	140571585856032 [label=ConvolutionBackward0]
	140571584971584 -> 140571585856032
	140571585856368 -> 140571585856032
	140571586515728 [label="stage4.0.fuse_layers.1.2.0.weight
 (36, 72, 1, 1)" fillcolor=lightblue]
	140571586515728 -> 140571585856368
	140571585856368 [label=AccumulateGrad]
	140571585856272 -> 140571585855888
	140571586515808 [label="stage4.0.fuse_layers.1.2.1.weight
 (36)" fillcolor=lightblue]
	140571586515808 -> 140571585856272
	140571585856272 [label=AccumulateGrad]
	140571585855648 -> 140571585855888
	140571586515888 [label="stage4.0.fuse_layers.1.2.1.bias
 (36)" fillcolor=lightblue]
	140571586515888 -> 140571585855648
	140571585855648 [label=AccumulateGrad]
	140571585856320 -> 140571585856464
	140571585856320 [label=UpsampleBilinear2DBackward0]
	140571585856080 -> 140571585856320
	140571585856080 [label=NativeBatchNormBackward0]
	140571585856704 -> 140571585856080
	140571585856704 [label=ConvolutionBackward0]
	140571586004400 -> 140571585856704
	140571585856752 -> 140571585856704
	140571586516368 [label="stage4.0.fuse_layers.1.3.0.weight
 (36, 144, 1, 1)" fillcolor=lightblue]
	140571586516368 -> 140571585856752
	140571585856752 [label=AccumulateGrad]
	140571585856224 -> 140571585856080
	140571586516448 [label="stage4.0.fuse_layers.1.3.1.weight
 (36)" fillcolor=lightblue]
	140571586516448 -> 140571585856224
	140571585856224 [label=AccumulateGrad]
	140571585855984 -> 140571585856080
	140571586516528 [label="stage4.0.fuse_layers.1.3.1.bias
 (36)" fillcolor=lightblue]
	140571586516528 -> 140571585855984
	140571585855984 [label=AccumulateGrad]
	140571585856512 -> 140571585857136
	140571586773792 [label="stage4.1.branches.1.0.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586773792 -> 140571585856512
	140571585856512 [label=AccumulateGrad]
	140571585857280 -> 140571585857232
	140571586773872 [label="stage4.1.branches.1.0.bn1.weight
 (36)" fillcolor=lightblue]
	140571586773872 -> 140571585857280
	140571585857280 [label=AccumulateGrad]
	140571585857520 -> 140571585857232
	140571586773952 [label="stage4.1.branches.1.0.bn1.bias
 (36)" fillcolor=lightblue]
	140571586773952 -> 140571585857520
	140571585857520 [label=AccumulateGrad]
	140571585857808 -> 140571585858000
	140571586774432 [label="stage4.1.branches.1.0.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586774432 -> 140571585857808
	140571585857808 [label=AccumulateGrad]
	140571585858192 -> 140571585858480
	140571586774512 [label="stage4.1.branches.1.0.bn2.weight
 (36)" fillcolor=lightblue]
	140571586774512 -> 140571585858192
	140571585858192 [label=AccumulateGrad]
	140571585858336 -> 140571585858480
	140571586774592 [label="stage4.1.branches.1.0.bn2.bias
 (36)" fillcolor=lightblue]
	140571586774592 -> 140571585858336
	140571585858336 [label=AccumulateGrad]
	140571585858624 -> 140571585858528
	140571585858864 -> 140571585859536
	140571586775072 [label="stage4.1.branches.1.1.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586775072 -> 140571585858864
	140571585858864 [label=AccumulateGrad]
	140571585859296 -> 140571585859824
	140571586775152 [label="stage4.1.branches.1.1.bn1.weight
 (36)" fillcolor=lightblue]
	140571586775152 -> 140571585859296
	140571585859296 [label=AccumulateGrad]
	140571585859680 -> 140571585859824
	140571586775232 [label="stage4.1.branches.1.1.bn1.bias
 (36)" fillcolor=lightblue]
	140571586775232 -> 140571585859680
	140571585859680 [label=AccumulateGrad]
	140571585861552 -> 140571585862704
	140571586775712 [label="stage4.1.branches.1.1.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586775712 -> 140571585861552
	140571585861552 [label=AccumulateGrad]
	140571585862560 -> 140571585862896
	140571586775792 [label="stage4.1.branches.1.1.bn2.weight
 (36)" fillcolor=lightblue]
	140571586775792 -> 140571585862560
	140571585862560 [label=AccumulateGrad]
	140571585861696 -> 140571585862896
	140571586775872 [label="stage4.1.branches.1.1.bn2.bias
 (36)" fillcolor=lightblue]
	140571586775872 -> 140571585861696
	140571585861696 [label=AccumulateGrad]
	140571585863856 -> 140571585862512
	140571585865056 -> 140571585865920
	140571586776352 [label="stage4.1.branches.1.2.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586776352 -> 140571585865056
	140571585865056 [label=AccumulateGrad]
	140571585864624 -> 140571585865584
	140571586776272 [label="stage4.1.branches.1.2.bn1.weight
 (36)" fillcolor=lightblue]
	140571586776272 -> 140571585864624
	140571585864624 [label=AccumulateGrad]
	140571585869424 -> 140571585865584
	140571586776432 [label="stage4.1.branches.1.2.bn1.bias
 (36)" fillcolor=lightblue]
	140571586776432 -> 140571585869424
	140571585869424 [label=AccumulateGrad]
	140571585871104 -> 140571860081328
	140571586776912 [label="stage4.1.branches.1.2.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586776912 -> 140571585871104
	140571585871104 [label=AccumulateGrad]
	140571588969088 -> 140571584981088
	140571586776992 [label="stage4.1.branches.1.2.bn2.weight
 (36)" fillcolor=lightblue]
	140571586776992 -> 140571588969088
	140571588969088 [label=AccumulateGrad]
	140571588974512 -> 140571584981088
	140571586777072 [label="stage4.1.branches.1.2.bn2.bias
 (36)" fillcolor=lightblue]
	140571586777072 -> 140571588974512
	140571588974512 [label=AccumulateGrad]
	140571588978976 -> 140571586007232
	140571584980800 -> 140571584980464
	140571586777552 [label="stage4.1.branches.1.3.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586777552 -> 140571584980800
	140571584980800 [label=AccumulateGrad]
	140571584981472 -> 140571584983008
	140571586777632 [label="stage4.1.branches.1.3.bn1.weight
 (36)" fillcolor=lightblue]
	140571586777632 -> 140571584981472
	140571584981472 [label=AccumulateGrad]
	140571584982000 -> 140571584983008
	140571586777712 [label="stage4.1.branches.1.3.bn1.bias
 (36)" fillcolor=lightblue]
	140571586777712 -> 140571584982000
	140571584982000 [label=AccumulateGrad]
	140571584984160 -> 140571584985360
	140571586778192 [label="stage4.1.branches.1.3.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586778192 -> 140571584984160
	140571584984160 [label=AccumulateGrad]
	140571584985168 -> 140571584985504
	140571586778272 [label="stage4.1.branches.1.3.bn2.weight
 (36)" fillcolor=lightblue]
	140571586778272 -> 140571584985168
	140571584985168 [label=AccumulateGrad]
	140571584984304 -> 140571584985504
	140571586778352 [label="stage4.1.branches.1.3.bn2.bias
 (36)" fillcolor=lightblue]
	140571586778352 -> 140571584984304
	140571584984304 [label=AccumulateGrad]
	140571584986464 -> 140571584985120
	140571585118944 -> 140571585118656
	140571586788752 [label="stage4.1.fuse_layers.0.1.0.weight
 (18, 36, 1, 1)" fillcolor=lightblue]
	140571586788752 -> 140571585118944
	140571585118944 [label=AccumulateGrad]
	140571585119328 -> 140571585119856
	140571586788832 [label="stage4.1.fuse_layers.0.1.1.weight
 (18)" fillcolor=lightblue]
	140571586788832 -> 140571585119328
	140571585119328 [label=AccumulateGrad]
	140571585120672 -> 140571585119856
	140571586788912 [label="stage4.1.fuse_layers.0.1.1.bias
 (18)" fillcolor=lightblue]
	140571586788912 -> 140571585120672
	140571585120672 [label=AccumulateGrad]
	140571585121344 -> 140571585121632
	140571585121344 [label=UpsampleBilinear2DBackward0]
	140571585119664 -> 140571585121344
	140571585119664 [label=NativeBatchNormBackward0]
	140571585121008 -> 140571585119664
	140571585121008 [label=ConvolutionBackward0]
	140571584980944 -> 140571585121008
	140571584980944 [label=ReluBackward0]
	140571584979456 -> 140571584980944
	140571584979456 [label=AddBackward0]
	140571588974704 -> 140571584979456
	140571588974704 [label=NativeBatchNormBackward0]
	140571585870048 -> 140571588974704
	140571585870048 [label=ConvolutionBackward0]
	140571585864000 -> 140571585870048
	140571585864000 [label=ReluBackward0]
	140571585860208 -> 140571585864000
	140571585860208 [label=NativeBatchNormBackward0]
	140571585857856 -> 140571585860208
	140571585857856 [label=ConvolutionBackward0]
	140571584982816 -> 140571585857856
	140571584982816 [label=ReluBackward0]
	140571585856992 -> 140571584982816
	140571585856992 [label=AddBackward0]
	140571585856656 -> 140571585856992
	140571585856656 [label=NativeBatchNormBackward0]
	140571585856560 -> 140571585856656
	140571585856560 [label=ConvolutionBackward0]
	140571585857088 -> 140571585856560
	140571585857088 [label=ReluBackward0]
	140571585857376 -> 140571585857088
	140571585857376 [label=NativeBatchNormBackward0]
	140571585857424 -> 140571585857376
	140571585857424 [label=ConvolutionBackward0]
	140571585855696 -> 140571585857424
	140571585855696 [label=ReluBackward0]
	140571585858048 -> 140571585855696
	140571585858048 [label=AddBackward0]
	140571585858096 -> 140571585858048
	140571585858096 [label=NativeBatchNormBackward0]
	140571585858432 -> 140571585858096
	140571585858432 [label=ConvolutionBackward0]
	140571585858816 -> 140571585858432
	140571585858816 [label=ReluBackward0]
	140571585858912 -> 140571585858816
	140571585858912 [label=NativeBatchNormBackward0]
	140571585859056 -> 140571585858912
	140571585859056 [label=ConvolutionBackward0]
	140571585858144 -> 140571585859056
	140571585858144 [label=ReluBackward0]
	140571585859584 -> 140571585858144
	140571585859584 [label=AddBackward0]
	140571585859728 -> 140571585859584
	140571585859728 [label=NativeBatchNormBackward0]
	140571585860160 -> 140571585859728
	140571585860160 [label=ConvolutionBackward0]
	140571585860448 -> 140571585860160
	140571585860448 [label=ReluBackward0]
	140571585860736 -> 140571585860448
	140571585860736 [label=NativeBatchNormBackward0]
	140571585860784 -> 140571585860736
	140571585860784 [label=ConvolutionBackward0]
	140571585859776 -> 140571585860784
	140571585859776 [label=ReluBackward0]
	140571585861408 -> 140571585859776
	140571585861408 [label=AddBackward0]
	140571585861456 -> 140571585861408
	140571585861456 [label=AddBackward0]
	140571585861792 -> 140571585861456
	140571585861792 [label=AddBackward0]
	140571585861936 -> 140571585861792
	140571585861936 [label=NativeBatchNormBackward0]
	140571585862128 -> 140571585861936
	140571585862128 [label=ConvolutionBackward0]
	140571585862416 -> 140571585862128
	140571585862416 [label=ReluBackward0]
	140571585862848 -> 140571585862416
	140571585862848 [label=NativeBatchNormBackward0]
	140571585862992 -> 140571585862848
	140571585862992 [label=ConvolutionBackward0]
	140571584979600 -> 140571585862992
	140571585863280 -> 140571585862992
	140571586517008 [label="stage4.0.fuse_layers.2.0.0.0.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586517008 -> 140571585863280
	140571585863280 [label=AccumulateGrad]
	140571585862800 -> 140571585862848
	140571586517088 [label="stage4.0.fuse_layers.2.0.0.1.weight
 (18)" fillcolor=lightblue]
	140571586517088 -> 140571585862800
	140571585862800 [label=AccumulateGrad]
	140571585862608 -> 140571585862848
	140571586517168 [label="stage4.0.fuse_layers.2.0.0.1.bias
 (18)" fillcolor=lightblue]
	140571586517168 -> 140571585862608
	140571585862608 [label=AccumulateGrad]
	140571585862464 -> 140571585862128
	140571586517568 [label="stage4.0.fuse_layers.2.0.1.0.weight
 (72, 18, 3, 3)" fillcolor=lightblue]
	140571586517568 -> 140571585862464
	140571585862464 [label=AccumulateGrad]
	140571585862176 -> 140571585861936
	140571586517648 [label="stage4.0.fuse_layers.2.0.1.1.weight
 (72)" fillcolor=lightblue]
	140571586517648 -> 140571585862176
	140571585862176 [label=AccumulateGrad]
	140571585862080 -> 140571585861936
	140571586517728 [label="stage4.0.fuse_layers.2.0.1.1.bias
 (72)" fillcolor=lightblue]
	140571586517728 -> 140571585862080
	140571585862080 [label=AccumulateGrad]
	140571585861744 -> 140571585861792
	140571585861744 [label=NativeBatchNormBackward0]
	140571585863136 -> 140571585861744
	140571585863136 [label=ConvolutionBackward0]
	140571584977296 -> 140571585863136
	140571585863088 -> 140571585863136
	140571586518128 [label="stage4.0.fuse_layers.2.1.0.0.weight
 (72, 36, 3, 3)" fillcolor=lightblue]
	140571586518128 -> 140571585863088
	140571585863088 [label=AccumulateGrad]
	140571585862752 -> 140571585861744
	140571586518208 [label="stage4.0.fuse_layers.2.1.0.1.weight
 (72)" fillcolor=lightblue]
	140571586518208 -> 140571585862752
	140571585862752 [label=AccumulateGrad]
	140571585862320 -> 140571585861744
	140571586518288 [label="stage4.0.fuse_layers.2.1.0.1.bias
 (72)" fillcolor=lightblue]
	140571586518288 -> 140571585862320
	140571585862320 [label=AccumulateGrad]
	140571584971584 -> 140571585861456
	140571585861504 -> 140571585861408
	140571585861504 [label=UpsampleBilinear2DBackward0]
	140571585862272 -> 140571585861504
	140571585862272 [label=NativeBatchNormBackward0]
	140571585863472 -> 140571585862272
	140571585863472 [label=ConvolutionBackward0]
	140571586004400 -> 140571585863472
	140571585863616 -> 140571585863472
	140571586518768 [label="stage4.0.fuse_layers.2.3.0.weight
 (72, 144, 1, 1)" fillcolor=lightblue]
	140571586518768 -> 140571585863616
	140571585863616 [label=AccumulateGrad]
	140571585862944 -> 140571585862272
	140571586518848 [label="stage4.0.fuse_layers.2.3.1.weight
 (72)" fillcolor=lightblue]
	140571586518848 -> 140571585862944
	140571585862944 [label=AccumulateGrad]
	140571585861600 -> 140571585862272
	140571586518928 [label="stage4.0.fuse_layers.2.3.1.bias
 (72)" fillcolor=lightblue]
	140571586518928 -> 140571585861600
	140571585861600 [label=AccumulateGrad]
	140571585861072 -> 140571585860784
	140571586778832 [label="stage4.1.branches.2.0.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571586778832 -> 140571585861072
	140571585861072 [label=AccumulateGrad]
	140571585860832 -> 140571585860736
	140571586778912 [label="stage4.1.branches.2.0.bn1.weight
 (72)" fillcolor=lightblue]
	140571586778912 -> 140571585860832
	140571585860832 [label=AccumulateGrad]
	140571585860400 -> 140571585860736
	140571586778992 [label="stage4.1.branches.2.0.bn1.bias
 (72)" fillcolor=lightblue]
	140571586778992 -> 140571585860400
	140571585860400 [label=AccumulateGrad]
	140571585860256 -> 140571585860160
	140571586779472 [label="stage4.1.branches.2.0.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571586779472 -> 140571585860256
	140571585860256 [label=AccumulateGrad]
	140571585860064 -> 140571585859728
	140571586779552 [label="stage4.1.branches.2.0.bn2.weight
 (72)" fillcolor=lightblue]
	140571586779552 -> 140571585860064
	140571585860064 [label=AccumulateGrad]
	140571585859920 -> 140571585859728
	140571586779632 [label="stage4.1.branches.2.0.bn2.bias
 (72)" fillcolor=lightblue]
	140571586779632 -> 140571585859920
	140571585859920 [label=AccumulateGrad]
	140571585859776 -> 140571585859584
	140571585859440 -> 140571585859056
	140571586780032 [label="stage4.1.branches.2.1.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571586780032 -> 140571585859440
	140571585859440 [label=AccumulateGrad]
	140571585859104 -> 140571585858912
	140571586780112 [label="stage4.1.branches.2.1.bn1.weight
 (72)" fillcolor=lightblue]
	140571586780112 -> 140571585859104
	140571585859104 [label=AccumulateGrad]
	140571585858768 -> 140571585858912
	140571586780192 [label="stage4.1.branches.2.1.bn1.bias
 (72)" fillcolor=lightblue]
	140571586780192 -> 140571585858768
	140571585858768 [label=AccumulateGrad]
	140571585858720 -> 140571585858432
	140571586780672 [label="stage4.1.branches.2.1.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571586780672 -> 140571585858720
	140571585858720 [label=AccumulateGrad]
	140571585858240 -> 140571585858096
	140571586780752 [label="stage4.1.branches.2.1.bn2.weight
 (72)" fillcolor=lightblue]
	140571586780752 -> 140571585858240
	140571585858240 [label=AccumulateGrad]
	140571585858288 -> 140571585858096
	140571586780832 [label="stage4.1.branches.2.1.bn2.bias
 (72)" fillcolor=lightblue]
	140571586780832 -> 140571585858288
	140571585858288 [label=AccumulateGrad]
	140571585858144 -> 140571585858048
	140571585857712 -> 140571585857424
	140571586781312 [label="stage4.1.branches.2.2.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571586781312 -> 140571585857712
	140571585857712 [label=AccumulateGrad]
	140571585857472 -> 140571585857376
	140571586781392 [label="stage4.1.branches.2.2.bn1.weight
 (72)" fillcolor=lightblue]
	140571586781392 -> 140571585857472
	140571585857472 [label=AccumulateGrad]
	140571585857040 -> 140571585857376
	140571586781472 [label="stage4.1.branches.2.2.bn1.bias
 (72)" fillcolor=lightblue]
	140571586781472 -> 140571585857040
	140571585857040 [label=AccumulateGrad]
	140571585856944 -> 140571585856560
	140571586781952 [label="stage4.1.branches.2.2.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571586781952 -> 140571585856944
	140571585856944 [label=AccumulateGrad]
	140571585856416 -> 140571585856656
	140571586782032 [label="stage4.1.branches.2.2.bn2.weight
 (72)" fillcolor=lightblue]
	140571586782032 -> 140571585856416
	140571585856416 [label=AccumulateGrad]
	140571585856848 -> 140571585856656
	140571586782112 [label="stage4.1.branches.2.2.bn2.bias
 (72)" fillcolor=lightblue]
	140571586782112 -> 140571585856848
	140571585856848 [label=AccumulateGrad]
	140571585855696 -> 140571585856992
	140571585856608 -> 140571585857856
	140571586782592 [label="stage4.1.branches.2.3.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571586782592 -> 140571585856608
	140571585856608 [label=AccumulateGrad]
	140571585859200 -> 140571585860208
	140571586782672 [label="stage4.1.branches.2.3.bn1.weight
 (72)" fillcolor=lightblue]
	140571586782672 -> 140571585859200
	140571585859200 [label=AccumulateGrad]
	140571585860496 -> 140571585860208
	140571586782752 [label="stage4.1.branches.2.3.bn1.bias
 (72)" fillcolor=lightblue]
	140571586782752 -> 140571585860496
	140571585860496 [label=AccumulateGrad]
	140571585864912 -> 140571585870048
	140571586783232 [label="stage4.1.branches.2.3.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571586783232 -> 140571585864912
	140571585864912 [label=AccumulateGrad]
	140571585863232 -> 140571588974704
	140571586783312 [label="stage4.1.branches.2.3.bn2.weight
 (72)" fillcolor=lightblue]
	140571586783312 -> 140571585863232
	140571585863232 [label=AccumulateGrad]
	140571585871248 -> 140571588974704
	140571586783392 [label="stage4.1.branches.2.3.bn2.bias
 (72)" fillcolor=lightblue]
	140571586783392 -> 140571585871248
	140571585871248 [label=AccumulateGrad]
	140571584982816 -> 140571584979456
	140571584983104 -> 140571585121008
	140571586986064 [label="stage4.1.fuse_layers.0.2.0.weight
 (18, 72, 1, 1)" fillcolor=lightblue]
	140571586986064 -> 140571584983104
	140571584983104 [label=AccumulateGrad]
	140571584983776 -> 140571585119664
	140571586986144 [label="stage4.1.fuse_layers.0.2.1.weight
 (18)" fillcolor=lightblue]
	140571586986144 -> 140571584983776
	140571584983776 [label=AccumulateGrad]
	140571584985840 -> 140571585119664
	140571586986224 [label="stage4.1.fuse_layers.0.2.1.bias
 (18)" fillcolor=lightblue]
	140571586986224 -> 140571584985840
	140571584985840 [label=AccumulateGrad]
	140571585121536 -> 140571585121680
	140571585121536 [label=UpsampleBilinear2DBackward0]
	140571585120816 -> 140571585121536
	140571585120816 [label=NativeBatchNormBackward0]
	140571584981808 -> 140571585120816
	140571584981808 [label=ConvolutionBackward0]
	140571585857952 -> 140571584981808
	140571585857952 [label=ReluBackward0]
	140571585856896 -> 140571585857952
	140571585856896 [label=AddBackward0]
	140571585859152 -> 140571585856896
	140571585859152 [label=NativeBatchNormBackward0]
	140571585856800 -> 140571585859152
	140571585856800 [label=ConvolutionBackward0]
	140571585857568 -> 140571585856800
	140571585857568 [label=ReluBackward0]
	140571585858576 -> 140571585857568
	140571585858576 [label=NativeBatchNormBackward0]
	140571585860112 -> 140571585858576
	140571585860112 [label=ConvolutionBackward0]
	140571585857328 -> 140571585860112
	140571585857328 [label=ReluBackward0]
	140571585860976 -> 140571585857328
	140571585860976 [label=AddBackward0]
	140571585861120 -> 140571585860976
	140571585861120 [label=NativeBatchNormBackward0]
	140571585863424 -> 140571585861120
	140571585863424 [label=ConvolutionBackward0]
	140571585863952 -> 140571585863424
	140571585863952 [label=ReluBackward0]
	140571585864144 -> 140571585863952
	140571585864144 [label=NativeBatchNormBackward0]
	140571585864288 -> 140571585864144
	140571585864288 [label=ConvolutionBackward0]
	140571585861648 -> 140571585864288
	140571585861648 [label=ReluBackward0]
	140571585864816 -> 140571585861648
	140571585864816 [label=AddBackward0]
	140571585864960 -> 140571585864816
	140571585864960 [label=NativeBatchNormBackward0]
	140571585865296 -> 140571585864960
	140571585865296 [label=ConvolutionBackward0]
	140571896003264 -> 140571585865296
	140571896003264 [label=ReluBackward0]
	140571601175744 -> 140571896003264
	140571601175744 [label=NativeBatchNormBackward0]
	140571592706032 -> 140571601175744
	140571592706032 [label=ConvolutionBackward0]
	140571585865008 -> 140571592706032
	140571585865008 [label=ReluBackward0]
	140571587198304 -> 140571585865008
	140571587198304 [label=AddBackward0]
	140571587198256 -> 140571587198304
	140571587198256 [label=NativeBatchNormBackward0]
	140571587197776 -> 140571587198256
	140571587197776 [label=ConvolutionBackward0]
	140571587197440 -> 140571587197776
	140571587197440 [label=ReluBackward0]
	140571587196960 -> 140571587197440
	140571587196960 [label=NativeBatchNormBackward0]
	140571587196912 -> 140571587196960
	140571587196912 [label=ConvolutionBackward0]
	140571587198400 -> 140571587196912
	140571587198400 [label=ReluBackward0]
	140571587196240 -> 140571587198400
	140571587196240 [label=AddBackward0]
	140571587195952 -> 140571587196240
	140571587195952 [label=AddBackward0]
	140571587195616 -> 140571587195952
	140571587195616 [label=AddBackward0]
	140571587195424 -> 140571587195616
	140571587195424 [label=NativeBatchNormBackward0]
	140571587195088 -> 140571587195424
	140571587195088 [label=ConvolutionBackward0]
	140571587194608 -> 140571587195088
	140571587194608 [label=ReluBackward0]
	140571587194560 -> 140571587194608
	140571587194560 [label=NativeBatchNormBackward0]
	140571587194704 -> 140571587194560
	140571587194704 [label=ConvolutionBackward0]
	140571587195136 -> 140571587194704
	140571587195136 [label=ReluBackward0]
	140571587195376 -> 140571587195136
	140571587195376 [label=NativeBatchNormBackward0]
	140571587195520 -> 140571587195376
	140571587195520 [label=ConvolutionBackward0]
	140571584979600 -> 140571587195520
	140571587195904 -> 140571587195520
	140571586519408 [label="stage4.0.fuse_layers.3.0.0.0.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586519408 -> 140571587195904
	140571587195904 [label=AccumulateGrad]
	140571587195328 -> 140571587195376
	140571586519488 [label="stage4.0.fuse_layers.3.0.0.1.weight
 (18)" fillcolor=lightblue]
	140571586519488 -> 140571587195328
	140571587195328 [label=AccumulateGrad]
	140571587195232 -> 140571587195376
	140571586519568 [label="stage4.0.fuse_layers.3.0.0.1.bias
 (18)" fillcolor=lightblue]
	140571586519568 -> 140571587195232
	140571587195232 [label=AccumulateGrad]
	140571587194992 -> 140571587194704
	140571586520048 [label="stage4.0.fuse_layers.3.0.1.0.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586520048 -> 140571587194992
	140571587194992 [label=AccumulateGrad]
	140571587194656 -> 140571587194560
	140571586520128 [label="stage4.0.fuse_layers.3.0.1.1.weight
 (18)" fillcolor=lightblue]
	140571586520128 -> 140571587194656
	140571587194656 [label=AccumulateGrad]
	140571587194320 -> 140571587194560
	140571586520208 [label="stage4.0.fuse_layers.3.0.1.1.bias
 (18)" fillcolor=lightblue]
	140571586520208 -> 140571587194320
	140571587194320 [label=AccumulateGrad]
	140571587194752 -> 140571587195088
	140571586520688 [label="stage4.0.fuse_layers.3.0.2.0.weight
 (144, 18, 3, 3)" fillcolor=lightblue]
	140571586520688 -> 140571587194752
	140571587194752 [label=AccumulateGrad]
	140571587194944 -> 140571587195424
	140571586520768 [label="stage4.0.fuse_layers.3.0.2.1.weight
 (144)" fillcolor=lightblue]
	140571586520768 -> 140571587194944
	140571587194944 [label=AccumulateGrad]
	140571587195280 -> 140571587195424
	140571586520848 [label="stage4.0.fuse_layers.3.0.2.1.bias
 (144)" fillcolor=lightblue]
	140571586520848 -> 140571587195280
	140571587195280 [label=AccumulateGrad]
	140571587195568 -> 140571587195616
	140571587195568 [label=NativeBatchNormBackward0]
	140571587195184 -> 140571587195568
	140571587195184 [label=ConvolutionBackward0]
	140571587194848 -> 140571587195184
	140571587194848 [label=ReluBackward0]
	140571587195808 -> 140571587194848
	140571587195808 [label=NativeBatchNormBackward0]
	140571587196000 -> 140571587195808
	140571587196000 [label=ConvolutionBackward0]
	140571584977296 -> 140571587196000
	140571587196480 -> 140571587196000
	140571586521248 [label="stage4.0.fuse_layers.3.1.0.0.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586521248 -> 140571587196480
	140571587196480 [label=AccumulateGrad]
	140571587195856 -> 140571587195808
	140571586521328 [label="stage4.0.fuse_layers.3.1.0.1.weight
 (36)" fillcolor=lightblue]
	140571586521328 -> 140571587195856
	140571587195856 [label=AccumulateGrad]
	140571587195472 -> 140571587195808
	140571586521408 [label="stage4.0.fuse_layers.3.1.0.1.bias
 (36)" fillcolor=lightblue]
	140571586521408 -> 140571587195472
	140571587195472 [label=AccumulateGrad]
	140571587195664 -> 140571587195184
	140571586521888 [label="stage4.0.fuse_layers.3.1.1.0.weight
 (144, 36, 3, 3)" fillcolor=lightblue]
	140571586521888 -> 140571587195664
	140571587195664 [label=AccumulateGrad]
	140571587194416 -> 140571587195568
	140571586521968 [label="stage4.0.fuse_layers.3.1.1.1.weight
 (144)" fillcolor=lightblue]
	140571586521968 -> 140571587194416
	140571587194416 [label=AccumulateGrad]
	140571587195040 -> 140571587195568
	140571586522048 [label="stage4.0.fuse_layers.3.1.1.1.bias
 (144)" fillcolor=lightblue]
	140571586522048 -> 140571587195040
	140571587195040 [label=AccumulateGrad]
	140571587195760 -> 140571587195952
	140571587195760 [label=NativeBatchNormBackward0]
	140571587196048 -> 140571587195760
	140571587196048 [label=ConvolutionBackward0]
	140571584971584 -> 140571587196048
	140571587196528 -> 140571587196048
	140571586522528 [label="stage4.0.fuse_layers.3.2.0.0.weight
 (144, 72, 3, 3)" fillcolor=lightblue]
	140571586522528 -> 140571587196528
	140571587196528 [label=AccumulateGrad]
	140571587194896 -> 140571587195760
	140571586522608 [label="stage4.0.fuse_layers.3.2.0.1.weight
 (144)" fillcolor=lightblue]
	140571586522608 -> 140571587194896
	140571587194896 [label=AccumulateGrad]
	140571587195712 -> 140571587195760
	140571586522688 [label="stage4.0.fuse_layers.3.2.0.1.bias
 (144)" fillcolor=lightblue]
	140571586522688 -> 140571587195712
	140571587195712 [label=AccumulateGrad]
	140571586004400 -> 140571587196240
	140571587196288 -> 140571587196912
	140571586783872 [label="stage4.1.branches.3.0.conv1.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571586783872 -> 140571587196288
	140571587196288 [label=AccumulateGrad]
	140571587197056 -> 140571587196960
	140571586783952 [label="stage4.1.branches.3.0.bn1.weight
 (144)" fillcolor=lightblue]
	140571586783952 -> 140571587197056
	140571587197056 [label=AccumulateGrad]
	140571587197296 -> 140571587196960
	140571586784032 [label="stage4.1.branches.3.0.bn1.bias
 (144)" fillcolor=lightblue]
	140571586784032 -> 140571587197296
	140571587197296 [label=AccumulateGrad]
	140571587197584 -> 140571587197776
	140571586784432 [label="stage4.1.branches.3.0.conv2.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571586784432 -> 140571587197584
	140571587197584 [label=AccumulateGrad]
	140571587197968 -> 140571587198256
	140571586784512 [label="stage4.1.branches.3.0.bn2.weight
 (144)" fillcolor=lightblue]
	140571586784512 -> 140571587197968
	140571587197968 [label=AccumulateGrad]
	140571587198112 -> 140571587198256
	140571586784592 [label="stage4.1.branches.3.0.bn2.bias
 (144)" fillcolor=lightblue]
	140571586784592 -> 140571587198112
	140571587198112 [label=AccumulateGrad]
	140571587198400 -> 140571587198304
	140571587198640 -> 140571592706032
	140571586784992 [label="stage4.1.branches.3.1.conv1.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571586784992 -> 140571587198640
	140571587198640 [label=AccumulateGrad]
	140571592390976 -> 140571601175744
	140571586785072 [label="stage4.1.branches.3.1.bn1.weight
 (144)" fillcolor=lightblue]
	140571586785072 -> 140571592390976
	140571592390976 [label=AccumulateGrad]
	140571587194224 -> 140571601175744
	140571586785152 [label="stage4.1.branches.3.1.bn1.bias
 (144)" fillcolor=lightblue]
	140571586785152 -> 140571587194224
	140571587194224 [label=AccumulateGrad]
	140571585865536 -> 140571585865296
	140571586785632 [label="stage4.1.branches.3.1.conv2.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571586785632 -> 140571585865536
	140571585865536 [label=AccumulateGrad]
	140571585865104 -> 140571585864960
	140571586785712 [label="stage4.1.branches.3.1.bn2.weight
 (144)" fillcolor=lightblue]
	140571586785712 -> 140571585865104
	140571585865104 [label=AccumulateGrad]
	140571585865152 -> 140571585864960
	140571586785792 [label="stage4.1.branches.3.1.bn2.bias
 (144)" fillcolor=lightblue]
	140571586785792 -> 140571585865152
	140571585865152 [label=AccumulateGrad]
	140571585865008 -> 140571585864816
	140571585864768 -> 140571585864288
	140571586786272 [label="stage4.1.branches.3.2.conv1.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571586786272 -> 140571585864768
	140571585864768 [label=AccumulateGrad]
	140571585864336 -> 140571585864144
	140571586786352 [label="stage4.1.branches.3.2.bn1.weight
 (144)" fillcolor=lightblue]
	140571586786352 -> 140571585864336
	140571585864336 [label=AccumulateGrad]
	140571585864096 -> 140571585864144
	140571586786432 [label="stage4.1.branches.3.2.bn1.bias
 (144)" fillcolor=lightblue]
	140571586786432 -> 140571585864096
	140571585864096 [label=AccumulateGrad]
	140571585863808 -> 140571585863424
	140571586786912 [label="stage4.1.branches.3.2.conv2.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571586786912 -> 140571585863808
	140571585863808 [label=AccumulateGrad]
	140571585863520 -> 140571585861120
	140571586786992 [label="stage4.1.branches.3.2.bn2.weight
 (144)" fillcolor=lightblue]
	140571586786992 -> 140571585863520
	140571585863520 [label=AccumulateGrad]
	140571585860928 -> 140571585861120
	140571586787072 [label="stage4.1.branches.3.2.bn2.bias
 (144)" fillcolor=lightblue]
	140571586787072 -> 140571585860928
	140571585860928 [label=AccumulateGrad]
	140571585861648 -> 140571585860976
	140571585861264 -> 140571585860112
	140571586787552 [label="stage4.1.branches.3.3.conv1.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571586787552 -> 140571585861264
	140571585861264 [label=AccumulateGrad]
	140571585859248 -> 140571585858576
	140571586787632 [label="stage4.1.branches.3.3.bn1.weight
 (144)" fillcolor=lightblue]
	140571586787632 -> 140571585859248
	140571585859248 [label=AccumulateGrad]
	140571585858960 -> 140571585858576
	140571586787712 [label="stage4.1.branches.3.3.bn1.bias
 (144)" fillcolor=lightblue]
	140571586787712 -> 140571585858960
	140571585858960 [label=AccumulateGrad]
	140571585857760 -> 140571585856800
	140571586788112 [label="stage4.1.branches.3.3.conv2.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571586788112 -> 140571585857760
	140571585857760 [label=AccumulateGrad]
	140571585857904 -> 140571585859152
	140571586788192 [label="stage4.1.branches.3.3.bn2.weight
 (144)" fillcolor=lightblue]
	140571586788192 -> 140571585857904
	140571585857904 [label=AccumulateGrad]
	140571585857184 -> 140571585859152
	140571586788272 [label="stage4.1.branches.3.3.bn2.bias
 (144)" fillcolor=lightblue]
	140571586788272 -> 140571585857184
	140571585857184 [label=AccumulateGrad]
	140571585857328 -> 140571585856896
	140571585858672 -> 140571584981808
	140571586986704 [label="stage4.1.fuse_layers.0.3.0.weight
 (18, 144, 1, 1)" fillcolor=lightblue]
	140571586986704 -> 140571585858672
	140571585858672 [label=AccumulateGrad]
	140571584986992 -> 140571585120816
	140571586986784 [label="stage4.1.fuse_layers.0.3.1.weight
 (18)" fillcolor=lightblue]
	140571586986784 -> 140571584986992
	140571584986992 [label=AccumulateGrad]
	140571584986368 -> 140571585120816
	140571586986864 [label="stage4.1.fuse_layers.0.3.1.bias
 (18)" fillcolor=lightblue]
	140571586986864 -> 140571584986368
	140571584986368 [label=AccumulateGrad]
	140571585122016 -> 140571585122400
	140571586994944 [label="stage4.2.branches.0.0.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586994944 -> 140571585122016
	140571585122016 [label=AccumulateGrad]
	140571585122544 -> 140571585122688
	140571586995024 [label="stage4.2.branches.0.0.bn1.weight
 (18)" fillcolor=lightblue]
	140571586995024 -> 140571585122544
	140571585122544 [label=AccumulateGrad]
	140571585122976 -> 140571585122688
	140571586995104 [label="stage4.2.branches.0.0.bn1.bias
 (18)" fillcolor=lightblue]
	140571586995104 -> 140571585122976
	140571585122976 [label=AccumulateGrad]
	140571585123024 -> 140571585123504
	140571586995504 [label="stage4.2.branches.0.0.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586995504 -> 140571585123024
	140571585123024 [label=AccumulateGrad]
	140571585123648 -> 140571585123696
	140571586995584 [label="stage4.2.branches.0.0.bn2.weight
 (18)" fillcolor=lightblue]
	140571586995584 -> 140571585123648
	140571585123648 [label=AccumulateGrad]
	140571585123552 -> 140571585123696
	140571586995664 [label="stage4.2.branches.0.0.bn2.bias
 (18)" fillcolor=lightblue]
	140571586995664 -> 140571585123552
	140571585123552 [label=AccumulateGrad]
	140571585123888 -> 140571585124032
	140571585124320 -> 140571585124704
	140571586996064 [label="stage4.2.branches.0.1.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586996064 -> 140571585124320
	140571585124320 [label=AccumulateGrad]
	140571585124848 -> 140571585124992
	140571586996144 [label="stage4.2.branches.0.1.bn1.weight
 (18)" fillcolor=lightblue]
	140571586996144 -> 140571585124848
	140571585124848 [label=AccumulateGrad]
	140571585125040 -> 140571585124992
	140571586996224 [label="stage4.2.branches.0.1.bn1.bias
 (18)" fillcolor=lightblue]
	140571586996224 -> 140571585125040
	140571585125040 [label=AccumulateGrad]
	140571585125376 -> 140571585125568
	140571586996704 [label="stage4.2.branches.0.1.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586996704 -> 140571585125376
	140571585125376 [label=AccumulateGrad]
	140571585125712 -> 140571585126048
	140571586996784 [label="stage4.2.branches.0.1.bn2.weight
 (18)" fillcolor=lightblue]
	140571586996784 -> 140571585125712
	140571585125712 [label=AccumulateGrad]
	140571585125904 -> 140571585126048
	140571586996864 [label="stage4.2.branches.0.1.bn2.bias
 (18)" fillcolor=lightblue]
	140571586996864 -> 140571585125904
	140571585125904 [label=AccumulateGrad]
	140571585126192 -> 140571585126336
	140571585126384 -> 140571585127008
	140571586997344 [label="stage4.2.branches.0.2.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586997344 -> 140571585126384
	140571585126384 [label=AccumulateGrad]
	140571585126912 -> 140571585127056
	140571586997424 [label="stage4.2.branches.0.2.bn1.weight
 (18)" fillcolor=lightblue]
	140571586997424 -> 140571585126912
	140571585126912 [label=AccumulateGrad]
	140571585127392 -> 140571585127056
	140571586997504 [label="stage4.2.branches.0.2.bn1.bias
 (18)" fillcolor=lightblue]
	140571586997504 -> 140571585127392
	140571585127392 [label=AccumulateGrad]
	140571585127680 -> 140571585127920
	140571586997984 [label="stage4.2.branches.0.2.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586997984 -> 140571585127680
	140571585127680 [label=AccumulateGrad]
	140571585128064 -> 140571585128352
	140571586998064 [label="stage4.2.branches.0.2.bn2.weight
 (18)" fillcolor=lightblue]
	140571586998064 -> 140571585128064
	140571585128064 [label=AccumulateGrad]
	140571585128208 -> 140571585128352
	140571586998144 [label="stage4.2.branches.0.2.bn2.bias
 (18)" fillcolor=lightblue]
	140571586998144 -> 140571585128208
	140571585128208 [label=AccumulateGrad]
	140571585128256 -> 140571585128400
	140571585128736 -> 140571585129072
	140571586998624 [label="stage4.2.branches.0.3.conv1.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586998624 -> 140571585128736
	140571585128736 [label=AccumulateGrad]
	140571585129264 -> 140571585130752
	140571586998704 [label="stage4.2.branches.0.3.bn1.weight
 (18)" fillcolor=lightblue]
	140571586998704 -> 140571585129264
	140571585129264 [label=AccumulateGrad]
	140571585134448 -> 140571585130752
	140571586998784 [label="stage4.2.branches.0.3.bn1.bias
 (18)" fillcolor=lightblue]
	140571586998784 -> 140571585134448
	140571585134448 [label=AccumulateGrad]
	140571585131424 -> 140571585130416
	140571586999264 [label="stage4.2.branches.0.3.conv2.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586999264 -> 140571585131424
	140571585131424 [label=AccumulateGrad]
	140571585129600 -> 140571585133968
	140571586999344 [label="stage4.2.branches.0.3.bn2.weight
 (18)" fillcolor=lightblue]
	140571586999344 -> 140571585129600
	140571585129600 [label=AccumulateGrad]
	140571585129696 -> 140571585133968
	140571586999424 [label="stage4.2.branches.0.3.bn2.bias
 (18)" fillcolor=lightblue]
	140571586999424 -> 140571585129696
	140571585129696 [label=AccumulateGrad]
	140571585131952 -> 140571585131712
	140571585132960 -> 140571585132432
	140571585132960 [label=UpsampleBilinear2DBackward0]
	140571585131280 -> 140571585132960
	140571585131280 [label=NativeBatchNormBackward0]
	140571585128592 -> 140571585131280
	140571585128592 [label=ConvolutionBackward0]
	140571585128880 -> 140571585128592
	140571585128880 [label=ReluBackward0]
	140571585126240 -> 140571585128880
	140571585126240 [label=AddBackward0]
	140571585126864 -> 140571585126240
	140571585126864 [label=NativeBatchNormBackward0]
	140571585126720 -> 140571585126864
	140571585126720 [label=ConvolutionBackward0]
	140571585124560 -> 140571585126720
	140571585124560 [label=ReluBackward0]
	140571585124368 -> 140571585124560
	140571585124368 [label=NativeBatchNormBackward0]
	140571585121872 -> 140571585124368
	140571585121872 [label=ConvolutionBackward0]
	140571585127584 -> 140571585121872
	140571585127584 [label=ReluBackward0]
	140571585122304 -> 140571585127584
	140571585122304 [label=AddBackward0]
	140571585857616 -> 140571585122304
	140571585857616 [label=NativeBatchNormBackward0]
	140571585859632 -> 140571585857616
	140571585859632 [label=ConvolutionBackward0]
	140571585863760 -> 140571585859632
	140571585863760 [label=ReluBackward0]
	140571585864192 -> 140571585863760
	140571585864192 [label=NativeBatchNormBackward0]
	140571585863664 -> 140571585864192
	140571585863664 [label=ConvolutionBackward0]
	140571585859008 -> 140571585863664
	140571585859008 [label=ReluBackward0]
	140571592380512 -> 140571585859008
	140571592380512 [label=AddBackward0]
	140571585865440 -> 140571592380512
	140571585865440 [label=NativeBatchNormBackward0]
	140571587198928 -> 140571585865440
	140571587198928 [label=ConvolutionBackward0]
	140571587196768 -> 140571587198928
	140571587196768 [label=ReluBackward0]
	140571587196096 -> 140571587196768
	140571587196096 [label=NativeBatchNormBackward0]
	140571587194800 -> 140571587196096
	140571587194800 [label=ConvolutionBackward0]
	140571587194272 -> 140571587194800
	140571587194272 [label=ReluBackward0]
	140571587196864 -> 140571587194272
	140571587196864 [label=AddBackward0]
	140571587197008 -> 140571587196864
	140571587197008 [label=NativeBatchNormBackward0]
	140571587197200 -> 140571587197008
	140571587197200 [label=ConvolutionBackward0]
	140571587197488 -> 140571587197200
	140571587197488 [label=ReluBackward0]
	140571587197920 -> 140571587197488
	140571587197920 [label=NativeBatchNormBackward0]
	140571587198064 -> 140571587197920
	140571587198064 [label=ConvolutionBackward0]
	140571587196816 -> 140571587198064
	140571587196816 [label=ReluBackward0]
	140571587198592 -> 140571587196816
	140571587198592 [label=AddBackward0]
	140571587198736 -> 140571587198592
	140571587198736 [label=AddBackward0]
	140571587198832 -> 140571587198736
	140571587198832 [label=AddBackward0]
	140571587193984 -> 140571587198832
	140571587193984 [label=NativeBatchNormBackward0]
	140571583183648 -> 140571587193984
	140571583183648 [label=ConvolutionBackward0]
	140571585120960 -> 140571583183648
	140571583183456 -> 140571583183648
	140571586987264 [label="stage4.1.fuse_layers.1.0.0.0.weight
 (36, 18, 3, 3)" fillcolor=lightblue]
	140571586987264 -> 140571583183456
	140571583183456 [label=AccumulateGrad]
	140571583183216 -> 140571587193984
	140571586987344 [label="stage4.1.fuse_layers.1.0.0.1.weight
 (36)" fillcolor=lightblue]
	140571586987344 -> 140571583183216
	140571583183216 [label=AccumulateGrad]
	140571583184800 -> 140571587193984
	140571586987424 [label="stage4.1.fuse_layers.1.0.0.1.bias
 (36)" fillcolor=lightblue]
	140571586987424 -> 140571583184800
	140571583184800 [label=AccumulateGrad]
	140571585118368 -> 140571587198832
	140571587198880 -> 140571587198736
	140571587198880 [label=UpsampleBilinear2DBackward0]
	140571587194368 -> 140571587198880
	140571587194368 [label=NativeBatchNormBackward0]
	140571583183504 -> 140571587194368
	140571583183504 [label=ConvolutionBackward0]
	140571584980944 -> 140571583183504
	140571583184704 -> 140571583183504
	140571586987824 [label="stage4.1.fuse_layers.1.2.0.weight
 (36, 72, 1, 1)" fillcolor=lightblue]
	140571586987824 -> 140571583184704
	140571583184704 [label=AccumulateGrad]
	140571583183360 -> 140571587194368
	140571586987904 [label="stage4.1.fuse_layers.1.2.1.weight
 (36)" fillcolor=lightblue]
	140571586987904 -> 140571583183360
	140571583183360 [label=AccumulateGrad]
	140571583183600 -> 140571587194368
	140571586987984 [label="stage4.1.fuse_layers.1.2.1.bias
 (36)" fillcolor=lightblue]
	140571586987984 -> 140571583183600
	140571583183600 [label=AccumulateGrad]
	140571587198544 -> 140571587198592
	140571587198544 [label=UpsampleBilinear2DBackward0]
	140571587198688 -> 140571587198544
	140571587198688 [label=NativeBatchNormBackward0]
	140571583184128 -> 140571587198688
	140571583184128 [label=ConvolutionBackward0]
	140571585857952 -> 140571583184128
	140571583183120 -> 140571583184128
	140571586988464 [label="stage4.1.fuse_layers.1.3.0.weight
 (36, 144, 1, 1)" fillcolor=lightblue]
	140571586988464 -> 140571583183120
	140571583183120 [label=AccumulateGrad]
	140571583183312 -> 140571587198688
	140571586988544 [label="stage4.1.fuse_layers.1.3.1.weight
 (36)" fillcolor=lightblue]
	140571586988544 -> 140571583183312
	140571583183312 [label=AccumulateGrad]
	140571583183552 -> 140571587198688
	140571586988624 [label="stage4.1.fuse_layers.1.3.1.bias
 (36)" fillcolor=lightblue]
	140571586988624 -> 140571583183552
	140571583183552 [label=AccumulateGrad]
	140571587198352 -> 140571587198064
	140571586999904 [label="stage4.2.branches.1.0.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586999904 -> 140571587198352
	140571587198352 [label=AccumulateGrad]
	140571587197872 -> 140571587197920
	140571586999984 [label="stage4.2.branches.1.0.bn1.weight
 (36)" fillcolor=lightblue]
	140571586999984 -> 140571587197872
	140571587197872 [label=AccumulateGrad]
	140571587197680 -> 140571587197920
	140571587000064 [label="stage4.2.branches.1.0.bn1.bias
 (36)" fillcolor=lightblue]
	140571587000064 -> 140571587197680
	140571587197680 [label=AccumulateGrad]
	140571587197536 -> 140571587197200
	140571587000544 [label="stage4.2.branches.1.0.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571587000544 -> 140571587197536
	140571587197536 [label=AccumulateGrad]
	140571587197248 -> 140571587197008
	140571587000624 [label="stage4.2.branches.1.0.bn2.weight
 (36)" fillcolor=lightblue]
	140571587000624 -> 140571587197248
	140571587197248 [label=AccumulateGrad]
	140571587197152 -> 140571587197008
	140571587000704 [label="stage4.2.branches.1.0.bn2.bias
 (36)" fillcolor=lightblue]
	140571587000704 -> 140571587197152
	140571587197152 [label=AccumulateGrad]
	140571587196816 -> 140571587196864
	140571587196720 -> 140571587194800
	140571587001104 [label="stage4.2.branches.1.1.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571587001104 -> 140571587196720
	140571587196720 [label=AccumulateGrad]
	140571587196144 -> 140571587196096
	140571587001184 [label="stage4.2.branches.1.1.bn1.weight
 (36)" fillcolor=lightblue]
	140571587001184 -> 140571587196144
	140571587196144 [label=AccumulateGrad]
	140571587196624 -> 140571587196096
	140571587001264 [label="stage4.2.branches.1.1.bn1.bias
 (36)" fillcolor=lightblue]
	140571587001264 -> 140571587196624
	140571587196624 [label=AccumulateGrad]
	140571587197728 -> 140571587198928
	140571587001664 [label="stage4.2.branches.1.1.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571587001664 -> 140571587197728
	140571587197728 [label=AccumulateGrad]
	140571587198784 -> 140571585865440
	140571587001744 [label="stage4.2.branches.1.1.bn2.weight
 (36)" fillcolor=lightblue]
	140571587001744 -> 140571587198784
	140571587198784 [label=AccumulateGrad]
	140571587197632 -> 140571585865440
	140571587001824 [label="stage4.2.branches.1.1.bn2.bias
 (36)" fillcolor=lightblue]
	140571587001824 -> 140571587197632
	140571587197632 [label=AccumulateGrad]
	140571587194272 -> 140571592380512
	140571585864432 -> 140571585863664
	140571587002224 [label="stage4.2.branches.1.2.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571587002224 -> 140571585864432
	140571585864432 [label=AccumulateGrad]
	140571585864864 -> 140571585864192
	140571587002304 [label="stage4.2.branches.1.2.bn1.weight
 (36)" fillcolor=lightblue]
	140571587002304 -> 140571585864864
	140571585864864 [label=AccumulateGrad]
	140571585860592 -> 140571585864192
	140571587264592 [label="stage4.2.branches.1.2.bn1.bias
 (36)" fillcolor=lightblue]
	140571587264592 -> 140571585860592
	140571585860592 [label=AccumulateGrad]
	140571585859488 -> 140571585859632
	140571587264992 [label="stage4.2.branches.1.2.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571587264992 -> 140571585859488
	140571585859488 [label=AccumulateGrad]
	140571585861168 -> 140571585857616
	140571587265072 [label="stage4.2.branches.1.2.bn2.weight
 (36)" fillcolor=lightblue]
	140571587265072 -> 140571585861168
	140571585861168 [label=AccumulateGrad]
	140571585870768 -> 140571585857616
	140571587265152 [label="stage4.2.branches.1.2.bn2.bias
 (36)" fillcolor=lightblue]
	140571587265152 -> 140571585870768
	140571585870768 [label=AccumulateGrad]
	140571585859008 -> 140571585122304
	140571585122160 -> 140571585121872
	140571587265552 [label="stage4.2.branches.1.3.conv1.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571587265552 -> 140571585122160
	140571585122160 [label=AccumulateGrad]
	140571585122832 -> 140571585124368
	140571587265632 [label="stage4.2.branches.1.3.bn1.weight
 (36)" fillcolor=lightblue]
	140571587265632 -> 140571585122832
	140571585122832 [label=AccumulateGrad]
	140571585123360 -> 140571585124368
	140571587265712 [label="stage4.2.branches.1.3.bn1.bias
 (36)" fillcolor=lightblue]
	140571587265712 -> 140571585123360
	140571585123360 [label=AccumulateGrad]
	140571585125520 -> 140571585126720
	140571587266192 [label="stage4.2.branches.1.3.conv2.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571587266192 -> 140571585125520
	140571585125520 [label=AccumulateGrad]
	140571585126576 -> 140571585126864
	140571587266272 [label="stage4.2.branches.1.3.bn2.weight
 (36)" fillcolor=lightblue]
	140571587266272 -> 140571585126576
	140571585126576 [label=AccumulateGrad]
	140571585125664 -> 140571585126864
	140571587266352 [label="stage4.2.branches.1.3.bn2.bias
 (36)" fillcolor=lightblue]
	140571587266352 -> 140571585125664
	140571585125664 [label=AccumulateGrad]
	140571585127584 -> 140571585126240
	140571585127728 -> 140571585128592
	140571587276512 [label="stage4.2.fuse_layers.0.1.0.weight
 (18, 36, 1, 1)" fillcolor=lightblue]
	140571587276512 -> 140571585127728
	140571585127728 [label=AccumulateGrad]
	140571585130272 -> 140571585131280
	140571587276592 [label="stage4.2.fuse_layers.0.1.1.weight
 (18)" fillcolor=lightblue]
	140571587276592 -> 140571585130272
	140571585130272 [label=AccumulateGrad]
	140571585131088 -> 140571585131280
	140571587276672 [label="stage4.2.fuse_layers.0.1.1.bias
 (18)" fillcolor=lightblue]
	140571587276672 -> 140571585131088
	140571585131088 [label=AccumulateGrad]
	140571585134256 -> 140571585132096
	140571585134256 [label=UpsampleBilinear2DBackward0]
	140571585132912 -> 140571585134256
	140571585132912 [label=NativeBatchNormBackward0]
	140571585124896 -> 140571585132912
	140571585124896 [label=ConvolutionBackward0]
	140571585121488 -> 140571585124896
	140571585121488 [label=ReluBackward0]
	140571585122208 -> 140571585121488
	140571585122208 [label=AddBackward0]
	140571585124176 -> 140571585122208
	140571585124176 [label=NativeBatchNormBackward0]
	140571585858384 -> 140571585124176
	140571585858384 [label=ConvolutionBackward0]
	140571592380560 -> 140571585858384
	140571592380560 [label=ReluBackward0]
	140571587196384 -> 140571592380560
	140571587196384 [label=NativeBatchNormBackward0]
	140571587197392 -> 140571587196384
	140571587197392 [label=ConvolutionBackward0]
	140571585865200 -> 140571587197392
	140571585865200 [label=ReluBackward0]
	140571587198016 -> 140571585865200
	140571587198016 [label=AddBackward0]
	140571587198160 -> 140571587198016
	140571587198160 [label=NativeBatchNormBackward0]
	140571583184752 -> 140571587198160
	140571583184752 [label=ConvolutionBackward0]
	140571583182928 -> 140571583184752
	140571583182928 [label=ReluBackward0]
	140571583182496 -> 140571583182928
	140571583182496 [label=NativeBatchNormBackward0]
	140571583182160 -> 140571583182496
	140571583182160 [label=ConvolutionBackward0]
	140571587198208 -> 140571583182160
	140571587198208 [label=ReluBackward0]
	140571583181488 -> 140571587198208
	140571583181488 [label=AddBackward0]
	140571583181440 -> 140571583181488
	140571583181440 [label=NativeBatchNormBackward0]
	140571583181008 -> 140571583181440
	140571583181008 [label=ConvolutionBackward0]
	140571583180624 -> 140571583181008
	140571583180624 [label=ReluBackward0]
	140571583180192 -> 140571583180624
	140571583180192 [label=NativeBatchNormBackward0]
	140571583179856 -> 140571583180192
	140571583179856 [label=ConvolutionBackward0]
	140571583181344 -> 140571583179856
	140571583181344 [label=ReluBackward0]
	140571583179184 -> 140571583181344
	140571583179184 [label=AddBackward0]
	140571583179136 -> 140571583179184
	140571583179136 [label=NativeBatchNormBackward0]
	140571583178704 -> 140571583179136
	140571583178704 [label=ConvolutionBackward0]
	140571583178320 -> 140571583178704
	140571583178320 [label=ReluBackward0]
	140571583177840 -> 140571583178320
	140571583177840 [label=NativeBatchNormBackward0]
	140571583177792 -> 140571583177840
	140571583177792 [label=ConvolutionBackward0]
	140571583179040 -> 140571583177792
	140571583179040 [label=ReluBackward0]
	140571583177120 -> 140571583179040
	140571583177120 [label=AddBackward0]
	140571583176832 -> 140571583177120
	140571583176832 [label=AddBackward0]
	140571583176352 -> 140571583176832
	140571583176352 [label=AddBackward0]
	140571583176304 -> 140571583176352
	140571583176304 [label=NativeBatchNormBackward0]
	140571583175824 -> 140571583176304
	140571583175824 [label=ConvolutionBackward0]
	140571583175488 -> 140571583175824
	140571583175488 [label=ReluBackward0]
	140571583175152 -> 140571583175488
	140571583175152 [label=NativeBatchNormBackward0]
	140571583174960 -> 140571583175152
	140571583174960 [label=ConvolutionBackward0]
	140571585120960 -> 140571583174960
	140571583174336 -> 140571583174960
	140571586989104 [label="stage4.1.fuse_layers.2.0.0.0.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586989104 -> 140571583174336
	140571583174336 [label=AccumulateGrad]
	140571583175104 -> 140571583175152
	140571586989184 [label="stage4.1.fuse_layers.2.0.0.1.weight
 (18)" fillcolor=lightblue]
	140571586989184 -> 140571583175104
	140571583175104 [label=AccumulateGrad]
	140571583175344 -> 140571583175152
	140571586989264 [label="stage4.1.fuse_layers.2.0.0.1.bias
 (18)" fillcolor=lightblue]
	140571586989264 -> 140571583175344
	140571583175344 [label=AccumulateGrad]
	140571583175632 -> 140571583175824
	140571586989584 [label="stage4.1.fuse_layers.2.0.1.0.weight
 (72, 18, 3, 3)" fillcolor=lightblue]
	140571586989584 -> 140571583175632
	140571583175632 [label=AccumulateGrad]
	140571583176016 -> 140571583176304
	140571586989664 [label="stage4.1.fuse_layers.2.0.1.1.weight
 (72)" fillcolor=lightblue]
	140571586989664 -> 140571583176016
	140571583176016 [label=AccumulateGrad]
	140571583176160 -> 140571583176304
	140571586989744 [label="stage4.1.fuse_layers.2.0.1.1.bias
 (72)" fillcolor=lightblue]
	140571586989744 -> 140571583176160
	140571583176160 [label=AccumulateGrad]
	140571583176448 -> 140571583176352
	140571583176448 [label=NativeBatchNormBackward0]
	140571583174672 -> 140571583176448
	140571583174672 [label=ConvolutionBackward0]
	140571585118368 -> 140571583174672
	140571583174480 -> 140571583174672
	140571586990144 [label="stage4.1.fuse_layers.2.1.0.0.weight
 (72, 36, 3, 3)" fillcolor=lightblue]
	140571586990144 -> 140571583174480
	140571583174480 [label=AccumulateGrad]
	140571583175008 -> 140571583176448
	140571586990224 [label="stage4.1.fuse_layers.2.1.0.1.weight
 (72)" fillcolor=lightblue]
	140571586990224 -> 140571583175008
	140571583175008 [label=AccumulateGrad]
	140571583175680 -> 140571583176448
	140571586990304 [label="stage4.1.fuse_layers.2.1.0.1.bias
 (72)" fillcolor=lightblue]
	140571586990304 -> 140571583175680
	140571583175680 [label=AccumulateGrad]
	140571584980944 -> 140571583176832
	140571583176976 -> 140571583177120
	140571583176976 [label=UpsampleBilinear2DBackward0]
	140571583175776 -> 140571583176976
	140571583175776 [label=NativeBatchNormBackward0]
	140571583174144 -> 140571583175776
	140571583174144 [label=ConvolutionBackward0]
	140571585857952 -> 140571583174144
	140571583173808 -> 140571583174144
	140571586990784 [label="stage4.1.fuse_layers.2.3.0.weight
 (72, 144, 1, 1)" fillcolor=lightblue]
	140571586990784 -> 140571583173808
	140571583173808 [label=AccumulateGrad]
	140571583174816 -> 140571583175776
	140571586990864 [label="stage4.1.fuse_layers.2.3.1.weight
 (72)" fillcolor=lightblue]
	140571586990864 -> 140571583174816
	140571583174816 [label=AccumulateGrad]
	140571583176496 -> 140571583175776
	140571586990944 [label="stage4.1.fuse_layers.2.3.1.bias
 (72)" fillcolor=lightblue]
	140571586990944 -> 140571583176496
	140571583176496 [label=AccumulateGrad]
	140571583177168 -> 140571583177792
	140571587266832 [label="stage4.2.branches.2.0.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587266832 -> 140571583177168
	140571583177168 [label=AccumulateGrad]
	140571583177696 -> 140571583177840
	140571587266912 [label="stage4.2.branches.2.0.bn1.weight
 (72)" fillcolor=lightblue]
	140571587266912 -> 140571583177696
	140571583177696 [label=AccumulateGrad]
	140571583178176 -> 140571583177840
	140571587266992 [label="stage4.2.branches.2.0.bn1.bias
 (72)" fillcolor=lightblue]
	140571587266992 -> 140571583178176
	140571583178176 [label=AccumulateGrad]
	140571583178464 -> 140571583178704
	140571587267472 [label="stage4.2.branches.2.0.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587267472 -> 140571583178464
	140571583178464 [label=AccumulateGrad]
	140571583178848 -> 140571583179136
	140571587267552 [label="stage4.2.branches.2.0.bn2.weight
 (72)" fillcolor=lightblue]
	140571587267552 -> 140571583178848
	140571583178848 [label=AccumulateGrad]
	140571583178992 -> 140571583179136
	140571587267632 [label="stage4.2.branches.2.0.bn2.bias
 (72)" fillcolor=lightblue]
	140571587267632 -> 140571583178992
	140571583178992 [label=AccumulateGrad]
	140571583179040 -> 140571583179184
	140571583179520 -> 140571583179856
	140571587268112 [label="stage4.2.branches.2.1.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587268112 -> 140571583179520
	140571583179520 [label=AccumulateGrad]
	140571583180048 -> 140571583180192
	140571587268192 [label="stage4.2.branches.2.1.bn1.weight
 (72)" fillcolor=lightblue]
	140571587268192 -> 140571583180048
	140571583180048 [label=AccumulateGrad]
	140571583180480 -> 140571583180192
	140571587268272 [label="stage4.2.branches.2.1.bn1.bias
 (72)" fillcolor=lightblue]
	140571587268272 -> 140571583180480
	140571583180480 [label=AccumulateGrad]
	140571583180528 -> 140571583181008
	140571587268752 [label="stage4.2.branches.2.1.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587268752 -> 140571583180528
	140571583180528 [label=AccumulateGrad]
	140571583181152 -> 140571583181440
	140571587268832 [label="stage4.2.branches.2.1.bn2.weight
 (72)" fillcolor=lightblue]
	140571587268832 -> 140571583181152
	140571583181152 [label=AccumulateGrad]
	140571583181296 -> 140571583181440
	140571587268912 [label="stage4.2.branches.2.1.bn2.bias
 (72)" fillcolor=lightblue]
	140571587268912 -> 140571583181296
	140571583181296 [label=AccumulateGrad]
	140571583181344 -> 140571583181488
	140571583181824 -> 140571583182160
	140571587269392 [label="stage4.2.branches.2.2.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587269392 -> 140571583181824
	140571583181824 [label=AccumulateGrad]
	140571583182304 -> 140571583182496
	140571587269472 [label="stage4.2.branches.2.2.bn1.weight
 (72)" fillcolor=lightblue]
	140571587269472 -> 140571583182304
	140571583182304 [label=AccumulateGrad]
	140571583182784 -> 140571583182496
	140571587269552 [label="stage4.2.branches.2.2.bn1.bias
 (72)" fillcolor=lightblue]
	140571587269552 -> 140571583182784
	140571583182784 [label=AccumulateGrad]
	140571583182976 -> 140571583184752
	140571587269952 [label="stage4.2.branches.2.2.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587269952 -> 140571583182976
	140571583182976 [label=AccumulateGrad]
	140571583183264 -> 140571587198160
	140571587270032 [label="stage4.2.branches.2.2.bn2.weight
 (72)" fillcolor=lightblue]
	140571587270032 -> 140571583183264
	140571583183264 [label=AccumulateGrad]
	140571583183408 -> 140571587198160
	140571587270112 [label="stage4.2.branches.2.2.bn2.bias
 (72)" fillcolor=lightblue]
	140571587270112 -> 140571583183408
	140571583183408 [label=AccumulateGrad]
	140571587198208 -> 140571587198016
	140571587198496 -> 140571587197392
	140571587270512 [label="stage4.2.branches.2.3.conv1.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587270512 -> 140571587198496
	140571587198496 [label=AccumulateGrad]
	140571587196576 -> 140571587196384
	140571587270592 [label="stage4.2.branches.2.3.bn1.weight
 (72)" fillcolor=lightblue]
	140571587270592 -> 140571587196576
	140571587196576 [label=AccumulateGrad]
	140571587196432 -> 140571587196384
	140571587270672 [label="stage4.2.branches.2.3.bn1.bias
 (72)" fillcolor=lightblue]
	140571587270672 -> 140571587196432
	140571587196432 [label=AccumulateGrad]
	140571587198448 -> 140571585858384
	140571587271152 [label="stage4.2.branches.2.3.conv2.weight
 (72, 72, 3, 3)" fillcolor=lightblue]
	140571587271152 -> 140571587198448
	140571587198448 [label=AccumulateGrad]
	140571585864576 -> 140571585124176
	140571587271232 [label="stage4.2.branches.2.3.bn2.weight
 (72)" fillcolor=lightblue]
	140571587271232 -> 140571585864576
	140571585864576 [label=AccumulateGrad]
	140571585859392 -> 140571585124176
	140571587271312 [label="stage4.2.branches.2.3.bn2.bias
 (72)" fillcolor=lightblue]
	140571587271312 -> 140571585859392
	140571585859392 [label=AccumulateGrad]
	140571585865200 -> 140571585122208
	140571585124224 -> 140571585124896
	140571587276992 [label="stage4.2.fuse_layers.0.2.0.weight
 (18, 72, 1, 1)" fillcolor=lightblue]
	140571587276992 -> 140571585124224
	140571585124224 [label=AccumulateGrad]
	140571585127248 -> 140571585132912
	140571587277072 [label="stage4.2.fuse_layers.0.2.1.weight
 (18)" fillcolor=lightblue]
	140571587277072 -> 140571585127248
	140571585127248 [label=AccumulateGrad]
	140571585132624 -> 140571585132912
	140571587277152 [label="stage4.2.fuse_layers.0.2.1.bias
 (18)" fillcolor=lightblue]
	140571587277152 -> 140571585132624
	140571585132624 [label=AccumulateGrad]
	140571585131616 -> 140571585131760
	140571585131616 [label=UpsampleBilinear2DBackward0]
	140571585128928 -> 140571585131616
	140571585128928 [label=NativeBatchNormBackward0]
	140571585123216 -> 140571585128928
	140571585123216 [label=ConvolutionBackward0]
	140571585860304 -> 140571585123216
	140571585860304 [label=ReluBackward0]
	140571587196192 -> 140571585860304
	140571587196192 [label=AddBackward0]
	140571587197824 -> 140571587196192
	140571587197824 [label=NativeBatchNormBackward0]
	140571583184320 -> 140571587197824
	140571583184320 [label=ConvolutionBackward0]
	140571583182112 -> 140571583184320
	140571583182112 [label=ReluBackward0]
	140571583180672 -> 140571583182112
	140571583180672 [label=NativeBatchNormBackward0]
	140571583178512 -> 140571583180672
	140571583178512 [label=ConvolutionBackward0]
	140571587197104 -> 140571583178512
	140571587197104 [label=ReluBackward0]
	140571583177648 -> 140571587197104
	140571583177648 [label=AddBackward0]
	140571583177360 -> 140571583177648
	140571583177360 [label=NativeBatchNormBackward0]
	140571583174432 -> 140571583177360
	140571583174432 [label=ConvolutionBackward0]
	140571583173616 -> 140571583174432
	140571583173616 [label=ReluBackward0]
	140571583173136 -> 140571583173616
	140571583173136 [label=NativeBatchNormBackward0]
	140571583173088 -> 140571583173136
	140571583173088 [label=ConvolutionBackward0]
	140571583176688 -> 140571583173088
	140571583176688 [label=ReluBackward0]
	140571583172416 -> 140571583176688
	140571583172416 [label=AddBackward0]
	140571583172128 -> 140571583172416
	140571583172128 [label=NativeBatchNormBackward0]
	140571583171648 -> 140571583172128
	140571583171648 [label=ConvolutionBackward0]
	140571583171312 -> 140571583171648
	140571583171312 [label=ReluBackward0]
	140571583171072 -> 140571583171312
	140571583171072 [label=NativeBatchNormBackward0]
	140571583170784 -> 140571583171072
	140571583170784 [label=ConvolutionBackward0]
	140571583172272 -> 140571583170784
	140571583172272 [label=ReluBackward0]
	140571583170112 -> 140571583172272
	140571583170112 [label=AddBackward0]
	140571583169776 -> 140571583170112
	140571583169776 [label=NativeBatchNormBackward0]
	140571583169584 -> 140571583169776
	140571583169584 [label=ConvolutionBackward0]
	140571583168960 -> 140571583169584
	140571583168960 [label=ReluBackward0]
	140571583168768 -> 140571583168960
	140571583168768 [label=NativeBatchNormBackward0]
	140571583183744 -> 140571583168768
	140571583183744 [label=ConvolutionBackward0]
	140571583169968 -> 140571583183744
	140571583169968 [label=ReluBackward0]
	140571609236096 -> 140571583169968
	140571609236096 [label=AddBackward0]
	140571583807696 -> 140571609236096
	140571583807696 [label=AddBackward0]
	140571583809232 -> 140571583807696
	140571583809232 [label=AddBackward0]
	140571583809376 -> 140571583809232
	140571583809376 [label=NativeBatchNormBackward0]
	140571583809520 -> 140571583809376
	140571583809520 [label=ConvolutionBackward0]
	140571583809712 -> 140571583809520
	140571583809712 [label=ReluBackward0]
	140571583809856 -> 140571583809712
	140571583809856 [label=NativeBatchNormBackward0]
	140571583809952 -> 140571583809856
	140571583809952 [label=ConvolutionBackward0]
	140571583810144 -> 140571583809952
	140571583810144 [label=ReluBackward0]
	140571583810288 -> 140571583810144
	140571583810288 [label=NativeBatchNormBackward0]
	140571583810384 -> 140571583810288
	140571583810384 [label=ConvolutionBackward0]
	140571585120960 -> 140571583810384
	140571583810576 -> 140571583810384
	140571586991344 [label="stage4.1.fuse_layers.3.0.0.0.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586991344 -> 140571583810576
	140571583810576 [label=AccumulateGrad]
	140571583810336 -> 140571583810288
	140571586991424 [label="stage4.1.fuse_layers.3.0.0.1.weight
 (18)" fillcolor=lightblue]
	140571586991424 -> 140571583810336
	140571583810336 [label=AccumulateGrad]
	140571583810192 -> 140571583810288
	140571586991504 [label="stage4.1.fuse_layers.3.0.0.1.bias
 (18)" fillcolor=lightblue]
	140571586991504 -> 140571583810192
	140571583810192 [label=AccumulateGrad]
	140571583810096 -> 140571583809952
	140571586991984 [label="stage4.1.fuse_layers.3.0.1.0.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571586991984 -> 140571583810096
	140571583810096 [label=AccumulateGrad]
	140571583809904 -> 140571583809856
	140571586992064 [label="stage4.1.fuse_layers.3.0.1.1.weight
 (18)" fillcolor=lightblue]
	140571586992064 -> 140571583809904
	140571583809904 [label=AccumulateGrad]
	140571583809760 -> 140571583809856
	140571586992144 [label="stage4.1.fuse_layers.3.0.1.1.bias
 (18)" fillcolor=lightblue]
	140571586992144 -> 140571583809760
	140571583809760 [label=AccumulateGrad]
	140571583809664 -> 140571583809520
	140571586992544 [label="stage4.1.fuse_layers.3.0.2.0.weight
 (144, 18, 3, 3)" fillcolor=lightblue]
	140571586992544 -> 140571583809664
	140571583809664 [label=AccumulateGrad]
	140571583809472 -> 140571583809376
	140571586992624 [label="stage4.1.fuse_layers.3.0.2.1.weight
 (144)" fillcolor=lightblue]
	140571586992624 -> 140571583809472
	140571583809472 [label=AccumulateGrad]
	140571583809424 -> 140571583809376
	140571586992704 [label="stage4.1.fuse_layers.3.0.2.1.bias
 (144)" fillcolor=lightblue]
	140571586992704 -> 140571583809424
	140571583809424 [label=AccumulateGrad]
	140571583809328 -> 140571583809232
	140571583809328 [label=NativeBatchNormBackward0]
	140571583810240 -> 140571583809328
	140571583810240 [label=ConvolutionBackward0]
	140571583810048 -> 140571583810240
	140571583810048 [label=ReluBackward0]
	140571583810528 -> 140571583810048
	140571583810528 [label=NativeBatchNormBackward0]
	140571583810720 -> 140571583810528
	140571583810720 [label=ConvolutionBackward0]
	140571585118368 -> 140571583810720
	140571583810912 -> 140571583810720
	140571586993184 [label="stage4.1.fuse_layers.3.1.0.0.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571586993184 -> 140571583810912
	140571583810912 [label=AccumulateGrad]
	140571583810624 -> 140571583810528
	140571586993264 [label="stage4.1.fuse_layers.3.1.0.1.weight
 (36)" fillcolor=lightblue]
	140571586993264 -> 140571583810624
	140571583810624 [label=AccumulateGrad]
	140571583810432 -> 140571583810528
	140571586993344 [label="stage4.1.fuse_layers.3.1.0.1.bias
 (36)" fillcolor=lightblue]
	140571586993344 -> 140571583810432
	140571583810432 [label=AccumulateGrad]
	140571583810480 -> 140571583810240
	140571586993824 [label="stage4.1.fuse_layers.3.1.1.0.weight
 (144, 36, 3, 3)" fillcolor=lightblue]
	140571586993824 -> 140571583810480
	140571583810480 [label=AccumulateGrad]
	140571583809808 -> 140571583809328
	140571586993904 [label="stage4.1.fuse_layers.3.1.1.1.weight
 (144)" fillcolor=lightblue]
	140571586993904 -> 140571583809808
	140571583809808 [label=AccumulateGrad]
	140571583809568 -> 140571583809328
	140571586993984 [label="stage4.1.fuse_layers.3.1.1.1.bias
 (144)" fillcolor=lightblue]
	140571586993984 -> 140571583809568
	140571583809568 [label=AccumulateGrad]
	140571583807552 -> 140571583807696
	140571583807552 [label=NativeBatchNormBackward0]
	140571583810672 -> 140571583807552
	140571583810672 [label=ConvolutionBackward0]
	140571584980944 -> 140571583810672
	140571583811008 -> 140571583810672
	140571586994384 [label="stage4.1.fuse_layers.3.2.0.0.weight
 (144, 72, 3, 3)" fillcolor=lightblue]
	140571586994384 -> 140571583811008
	140571583811008 [label=AccumulateGrad]
	140571583809616 -> 140571583807552
	140571586994464 [label="stage4.1.fuse_layers.3.2.0.1.weight
 (144)" fillcolor=lightblue]
	140571586994464 -> 140571583809616
	140571583809616 [label=AccumulateGrad]
	140571583809280 -> 140571583807552
	140571586994544 [label="stage4.1.fuse_layers.3.2.0.1.bias
 (144)" fillcolor=lightblue]
	140571586994544 -> 140571583809280
	140571583809280 [label=AccumulateGrad]
	140571585857952 -> 140571609236096
	140571609235616 -> 140571583183744
	140571587271712 [label="stage4.2.branches.3.0.conv1.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571587271712 -> 140571609235616
	140571609235616 [label=AccumulateGrad]
	140571583168624 -> 140571583168768
	140571587271792 [label="stage4.2.branches.3.0.bn1.weight
 (144)" fillcolor=lightblue]
	140571587271792 -> 140571583168624
	140571583168624 [label=AccumulateGrad]
	140571583169056 -> 140571583168768
	140571587271872 [label="stage4.2.branches.3.0.bn1.bias
 (144)" fillcolor=lightblue]
	140571587271872 -> 140571583169056
	140571583169056 [label=AccumulateGrad]
	140571583169104 -> 140571583169584
	140571587272352 [label="stage4.2.branches.3.0.conv2.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571587272352 -> 140571583169104
	140571583169104 [label=AccumulateGrad]
	140571583169728 -> 140571583169776
	140571587272432 [label="stage4.2.branches.3.0.bn2.weight
 (144)" fillcolor=lightblue]
	140571587272432 -> 140571583169728
	140571583169728 [label=AccumulateGrad]
	140571583169632 -> 140571583169776
	140571587272512 [label="stage4.2.branches.3.0.bn2.bias
 (144)" fillcolor=lightblue]
	140571587272512 -> 140571583169632
	140571583169632 [label=AccumulateGrad]
	140571583169968 -> 140571583170112
	140571583170400 -> 140571583170784
	140571587272912 [label="stage4.2.branches.3.1.conv1.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571587272912 -> 140571583170400
	140571583170400 [label=AccumulateGrad]
	140571583170928 -> 140571583171072
	140571587272992 [label="stage4.2.branches.3.1.bn1.weight
 (144)" fillcolor=lightblue]
	140571587272992 -> 140571583170928
	140571583170928 [label=AccumulateGrad]
	140571583171120 -> 140571583171072
	140571587273072 [label="stage4.2.branches.3.1.bn1.bias
 (144)" fillcolor=lightblue]
	140571587273072 -> 140571583171120
	140571583171120 [label=AccumulateGrad]
	140571583171456 -> 140571583171648
	140571587273472 [label="stage4.2.branches.3.1.conv2.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571587273472 -> 140571583171456
	140571583171456 [label=AccumulateGrad]
	140571583171792 -> 140571583172128
	140571587273552 [label="stage4.2.branches.3.1.bn2.weight
 (144)" fillcolor=lightblue]
	140571587273552 -> 140571583171792
	140571583171792 [label=AccumulateGrad]
	140571583171984 -> 140571583172128
	140571587273632 [label="stage4.2.branches.3.1.bn2.bias
 (144)" fillcolor=lightblue]
	140571587273632 -> 140571583171984
	140571583171984 [label=AccumulateGrad]
	140571583172272 -> 140571583172416
	140571583172464 -> 140571583173088
	140571587274032 [label="stage4.2.branches.3.2.conv1.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571587274032 -> 140571583172464
	140571583172464 [label=AccumulateGrad]
	140571583172992 -> 140571583173136
	140571587274112 [label="stage4.2.branches.3.2.bn1.weight
 (144)" fillcolor=lightblue]
	140571587274112 -> 140571583172992
	140571583172992 [label=AccumulateGrad]
	140571583173472 -> 140571583173136
	140571587274192 [label="stage4.2.branches.3.2.bn1.bias
 (144)" fillcolor=lightblue]
	140571587274192 -> 140571583173472
	140571583173472 [label=AccumulateGrad]
	140571583173664 -> 140571583174432
	140571587274672 [label="stage4.2.branches.3.2.conv2.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571587274672 -> 140571583173664
	140571583173664 [label=AccumulateGrad]
	140571583174288 -> 140571583177360
	140571587274752 [label="stage4.2.branches.3.2.bn2.weight
 (144)" fillcolor=lightblue]
	140571587274752 -> 140571583174288
	140571583174288 [label=AccumulateGrad]
	140571583177504 -> 140571583177360
	140571587274832 [label="stage4.2.branches.3.2.bn2.bias
 (144)" fillcolor=lightblue]
	140571587274832 -> 140571583177504
	140571583177504 [label=AccumulateGrad]
	140571583176688 -> 140571583177648
	140571583177024 -> 140571583178512
	140571587275312 [label="stage4.2.branches.3.3.conv1.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571587275312 -> 140571583177024
	140571583177024 [label=AccumulateGrad]
	140571583179712 -> 140571583180672
	140571587275392 [label="stage4.2.branches.3.3.bn1.weight
 (144)" fillcolor=lightblue]
	140571587275392 -> 140571583179712
	140571583179712 [label=AccumulateGrad]
	140571583180336 -> 140571583180672
	140571587275472 [label="stage4.2.branches.3.3.bn1.bias
 (144)" fillcolor=lightblue]
	140571587275472 -> 140571583180336
	140571583180336 [label=AccumulateGrad]
	140571583181968 -> 140571583184320
	140571587275952 [label="stage4.2.branches.3.3.conv2.weight
 (144, 144, 3, 3)" fillcolor=lightblue]
	140571587275952 -> 140571583181968
	140571583181968 [label=AccumulateGrad]
	140571583181680 -> 140571587197824
	140571587276032 [label="stage4.2.branches.3.3.bn2.weight
 (144)" fillcolor=lightblue]
	140571587276032 -> 140571583181680
	140571583181680 [label=AccumulateGrad]
	140571583182640 -> 140571587197824
	140571587276112 [label="stage4.2.branches.3.3.bn2.bias
 (144)" fillcolor=lightblue]
	140571587276112 -> 140571583182640
	140571583182640 [label=AccumulateGrad]
	140571587197104 -> 140571587196192
	140571587197344 -> 140571585123216
	140571587277632 [label="stage4.2.fuse_layers.0.3.0.weight
 (18, 144, 1, 1)" fillcolor=lightblue]
	140571587277632 -> 140571587197344
	140571587197344 [label=AccumulateGrad]
	140571585129024 -> 140571585128928
	140571587277712 [label="stage4.2.fuse_layers.0.3.1.weight
 (18)" fillcolor=lightblue]
	140571587277712 -> 140571585129024
	140571585129024 [label=AccumulateGrad]
	140571585129936 -> 140571585128928
	140571587277792 [label="stage4.2.fuse_layers.0.3.1.bias
 (18)" fillcolor=lightblue]
	140571587277792 -> 140571585129936
	140571585129936 [label=AccumulateGrad]
	140571585132384 -> 140571585133728
	140571585132384 [label=UpsampleBilinear2DBackward0]
	140571585130608 -> 140571585132384
	140571585130608 [label=ReluBackward0]
	140571585864480 -> 140571585130608
	140571585864480 [label=AddBackward0]
	140571587196336 -> 140571585864480
	140571587196336 [label=AddBackward0]
	140571583178368 -> 140571587196336
	140571583178368 [label=AddBackward0]
	140571583173760 -> 140571583178368
	140571583173760 [label=NativeBatchNormBackward0]
	140571583179808 -> 140571583173760
	140571583179808 [label=ConvolutionBackward0]
	140571585129744 -> 140571583179808
	140571583172944 -> 140571583179808
	140571587278192 [label="stage4.2.fuse_layers.1.0.0.0.weight
 (36, 18, 3, 3)" fillcolor=lightblue]
	140571587278192 -> 140571583172944
	140571583172944 [label=AccumulateGrad]
	140571583178032 -> 140571583173760
	140571587278272 [label="stage4.2.fuse_layers.1.0.0.1.weight
 (36)" fillcolor=lightblue]
	140571587278272 -> 140571583178032
	140571583178032 [label=AccumulateGrad]
	140571583180864 -> 140571583173760
	140571587278352 [label="stage4.2.fuse_layers.1.0.0.1.bias
 (36)" fillcolor=lightblue]
	140571587278352 -> 140571583180864
	140571583180864 [label=AccumulateGrad]
	140571585128880 -> 140571583178368
	140571583179376 -> 140571587196336
	140571583179376 [label=UpsampleBilinear2DBackward0]
	140571583172320 -> 140571583179376
	140571583172320 [label=NativeBatchNormBackward0]
	140571583174000 -> 140571583172320
	140571583174000 [label=ConvolutionBackward0]
	140571585121488 -> 140571583174000
	140571583170256 -> 140571583174000
	140571587278752 [label="stage4.2.fuse_layers.1.2.0.weight
 (36, 72, 1, 1)" fillcolor=lightblue]
	140571587278752 -> 140571583170256
	140571583170256 [label=AccumulateGrad]
	140571583172656 -> 140571583172320
	140571587278832 [label="stage4.2.fuse_layers.1.2.1.weight
 (36)" fillcolor=lightblue]
	140571587278832 -> 140571583172656
	140571583172656 [label=AccumulateGrad]
	140571583179664 -> 140571583172320
	140571587278912 [label="stage4.2.fuse_layers.1.2.1.bias
 (36)" fillcolor=lightblue]
	140571587278912 -> 140571583179664
	140571583179664 [label=AccumulateGrad]
	140571587196672 -> 140571585864480
	140571587196672 [label=UpsampleBilinear2DBackward0]
	140571583171744 -> 140571587196672
	140571583171744 [label=NativeBatchNormBackward0]
	140571583170640 -> 140571583171744
	140571583170640 [label=ConvolutionBackward0]
	140571585860304 -> 140571583170640
	140571583170304 -> 140571583170640
	140571587279232 [label="stage4.2.fuse_layers.1.3.0.weight
 (36, 144, 1, 1)" fillcolor=lightblue]
	140571587279232 -> 140571583170304
	140571583170304 [label=AccumulateGrad]
	140571583172800 -> 140571583171744
	140571587279312 [label="stage4.2.fuse_layers.1.3.1.weight
 (36)" fillcolor=lightblue]
	140571587279312 -> 140571583172800
	140571583172800 [label=AccumulateGrad]
	140571583183072 -> 140571583171744
	140571587279392 [label="stage4.2.fuse_layers.1.3.1.bias
 (36)" fillcolor=lightblue]
	140571587279392 -> 140571583183072
	140571583183072 [label=AccumulateGrad]
	140571585133296 -> 140571585133728
	140571585133296 [label=UpsampleBilinear2DBackward0]
	140571585134304 -> 140571585133296
	140571585134304 [label=ReluBackward0]
	140571583170976 -> 140571585134304
	140571583170976 [label=AddBackward0]
	140571583169296 -> 140571583170976
	140571583169296 [label=AddBackward0]
	140571583168912 -> 140571583169296
	140571583168912 [label=AddBackward0]
	140571609244400 -> 140571583168912
	140571609244400 [label=NativeBatchNormBackward0]
	140571583808560 -> 140571609244400
	140571583808560 [label=ConvolutionBackward0]
	140571583810768 -> 140571583808560
	140571583810768 [label=ReluBackward0]
	140571583811104 -> 140571583810768
	140571583811104 [label=NativeBatchNormBackward0]
	140571583811200 -> 140571583811104
	140571583811200 [label=ConvolutionBackward0]
	140571585129744 -> 140571583811200
	140571583811392 -> 140571583811200
	140571587279792 [label="stage4.2.fuse_layers.2.0.0.0.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571587279792 -> 140571583811392
	140571583811392 [label=AccumulateGrad]
	140571583811152 -> 140571583811104
	140571587279872 [label="stage4.2.fuse_layers.2.0.0.1.weight
 (18)" fillcolor=lightblue]
	140571587279872 -> 140571583811152
	140571583811152 [label=AccumulateGrad]
	140571583810864 -> 140571583811104
	140571587279952 [label="stage4.2.fuse_layers.2.0.0.1.bias
 (18)" fillcolor=lightblue]
	140571587279952 -> 140571583810864
	140571583810864 [label=AccumulateGrad]
	140571583810960 -> 140571583808560
	140571587280352 [label="stage4.2.fuse_layers.2.0.1.0.weight
 (72, 18, 3, 3)" fillcolor=lightblue]
	140571587280352 -> 140571583810960
	140571583810960 [label=AccumulateGrad]
	140571583808320 -> 140571609244400
	140571587280432 [label="stage4.2.fuse_layers.2.0.1.1.weight
 (72)" fillcolor=lightblue]
	140571587280432 -> 140571583808320
	140571583808320 [label=AccumulateGrad]
	140571583808896 -> 140571609244400
	140571587280512 [label="stage4.2.fuse_layers.2.0.1.1.bias
 (72)" fillcolor=lightblue]
	140571587280512 -> 140571583808896
	140571583808896 [label=AccumulateGrad]
	140571609235568 -> 140571583168912
	140571609235568 [label=NativeBatchNormBackward0]
	140571583811296 -> 140571609235568
	140571583811296 [label=ConvolutionBackward0]
	140571585128880 -> 140571583811296
	140571583811344 -> 140571583811296
	140571587280832 [label="stage4.2.fuse_layers.2.1.0.0.weight
 (72, 36, 3, 3)" fillcolor=lightblue]
	140571587280832 -> 140571583811344
	140571583811344 [label=AccumulateGrad]
	140571583811056 -> 140571609235568
	140571585396816 [label="stage4.2.fuse_layers.2.1.0.1.weight
 (72)" fillcolor=lightblue]
	140571585396816 -> 140571583811056
	140571583811056 [label=AccumulateGrad]
	140571583810816 -> 140571609235568
	140571585396896 [label="stage4.2.fuse_layers.2.1.0.1.bias
 (72)" fillcolor=lightblue]
	140571585396896 -> 140571583810816
	140571583810816 [label=AccumulateGrad]
	140571585121488 -> 140571583169296
	140571583171600 -> 140571583170976
	140571583171600 [label=UpsampleBilinear2DBackward0]
	140571583169440 -> 140571583171600
	140571583169440 [label=NativeBatchNormBackward0]
	140571583811536 -> 140571583169440
	140571583811536 [label=ConvolutionBackward0]
	140571585860304 -> 140571583811536
	140571583811632 -> 140571583811536
	140571585397376 [label="stage4.2.fuse_layers.2.3.0.weight
 (72, 144, 1, 1)" fillcolor=lightblue]
	140571585397376 -> 140571583811632
	140571583811632 [label=AccumulateGrad]
	140571583811248 -> 140571583169440
	140571585397456 [label="stage4.2.fuse_layers.2.3.1.weight
 (72)" fillcolor=lightblue]
	140571585397456 -> 140571583811248
	140571583811248 [label=AccumulateGrad]
	140571583810000 -> 140571583169440
	140571585397536 [label="stage4.2.fuse_layers.2.3.1.bias
 (72)" fillcolor=lightblue]
	140571585397536 -> 140571583810000
	140571583810000 [label=AccumulateGrad]
	140571585133584 -> 140571585133728
	140571585133584 [label=UpsampleBilinear2DBackward0]
	140571585130944 -> 140571585133584
	140571585130944 [label=ReluBackward0]
	140571583170448 -> 140571585130944
	140571583170448 [label=AddBackward0]
	140571583811728 -> 140571583170448
	140571583811728 [label=AddBackward0]
	140571583811680 -> 140571583811728
	140571583811680 [label=AddBackward0]
	140571583811872 -> 140571583811680
	140571583811872 [label=NativeBatchNormBackward0]
	140571583812016 -> 140571583811872
	140571583812016 [label=ConvolutionBackward0]
	140571583812208 -> 140571583812016
	140571583812208 [label=ReluBackward0]
	140571583812352 -> 140571583812208
	140571583812352 [label=NativeBatchNormBackward0]
	140571583812448 -> 140571583812352
	140571583812448 [label=ConvolutionBackward0]
	140571583812640 -> 140571583812448
	140571583812640 [label=ReluBackward0]
	140571583812784 -> 140571583812640
	140571583812784 [label=NativeBatchNormBackward0]
	140571583812880 -> 140571583812784
	140571583812880 [label=ConvolutionBackward0]
	140571585129744 -> 140571583812880
	140571583813072 -> 140571583812880
	140571585398016 [label="stage4.2.fuse_layers.3.0.0.0.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571585398016 -> 140571583813072
	140571583813072 [label=AccumulateGrad]
	140571583812832 -> 140571583812784
	140571585398096 [label="stage4.2.fuse_layers.3.0.0.1.weight
 (18)" fillcolor=lightblue]
	140571585398096 -> 140571583812832
	140571583812832 [label=AccumulateGrad]
	140571583812688 -> 140571583812784
	140571585398176 [label="stage4.2.fuse_layers.3.0.0.1.bias
 (18)" fillcolor=lightblue]
	140571585398176 -> 140571583812688
	140571583812688 [label=AccumulateGrad]
	140571583812592 -> 140571583812448
	140571585398656 [label="stage4.2.fuse_layers.3.0.1.0.weight
 (18, 18, 3, 3)" fillcolor=lightblue]
	140571585398656 -> 140571583812592
	140571583812592 [label=AccumulateGrad]
	140571583812400 -> 140571583812352
	140571585398736 [label="stage4.2.fuse_layers.3.0.1.1.weight
 (18)" fillcolor=lightblue]
	140571585398736 -> 140571583812400
	140571583812400 [label=AccumulateGrad]
	140571583812256 -> 140571583812352
	140571585398816 [label="stage4.2.fuse_layers.3.0.1.1.bias
 (18)" fillcolor=lightblue]
	140571585398816 -> 140571583812256
	140571583812256 [label=AccumulateGrad]
	140571583812160 -> 140571583812016
	140571585399296 [label="stage4.2.fuse_layers.3.0.2.0.weight
 (144, 18, 3, 3)" fillcolor=lightblue]
	140571585399296 -> 140571583812160
	140571583812160 [label=AccumulateGrad]
	140571583811968 -> 140571583811872
	140571585399376 [label="stage4.2.fuse_layers.3.0.2.1.weight
 (144)" fillcolor=lightblue]
	140571585399376 -> 140571583811968
	140571583811968 [label=AccumulateGrad]
	140571583811920 -> 140571583811872
	140571585399456 [label="stage4.2.fuse_layers.3.0.2.1.bias
 (144)" fillcolor=lightblue]
	140571585399456 -> 140571583811920
	140571583811920 [label=AccumulateGrad]
	140571583811824 -> 140571583811680
	140571583811824 [label=NativeBatchNormBackward0]
	140571583812736 -> 140571583811824
	140571583812736 [label=ConvolutionBackward0]
	140571583812544 -> 140571583812736
	140571583812544 [label=ReluBackward0]
	140571583813024 -> 140571583812544
	140571583813024 [label=NativeBatchNormBackward0]
	140571583813216 -> 140571583813024
	140571583813216 [label=ConvolutionBackward0]
	140571585128880 -> 140571583813216
	140571583813408 -> 140571583813216
	140571585399856 [label="stage4.2.fuse_layers.3.1.0.0.weight
 (36, 36, 3, 3)" fillcolor=lightblue]
	140571585399856 -> 140571583813408
	140571583813408 [label=AccumulateGrad]
	140571583813120 -> 140571583813024
	140571585399936 [label="stage4.2.fuse_layers.3.1.0.1.weight
 (36)" fillcolor=lightblue]
	140571585399936 -> 140571583813120
	140571583813120 [label=AccumulateGrad]
	140571583812928 -> 140571583813024
	140571585400016 [label="stage4.2.fuse_layers.3.1.0.1.bias
 (36)" fillcolor=lightblue]
	140571585400016 -> 140571583812928
	140571583812928 [label=AccumulateGrad]
	140571583812976 -> 140571583812736
	140571585400416 [label="stage4.2.fuse_layers.3.1.1.0.weight
 (144, 36, 3, 3)" fillcolor=lightblue]
	140571585400416 -> 140571583812976
	140571583812976 [label=AccumulateGrad]
	140571583812304 -> 140571583811824
	140571585400496 [label="stage4.2.fuse_layers.3.1.1.1.weight
 (144)" fillcolor=lightblue]
	140571585400496 -> 140571583812304
	140571583812304 [label=AccumulateGrad]
	140571583812064 -> 140571583811824
	140571585400576 [label="stage4.2.fuse_layers.3.1.1.1.bias
 (144)" fillcolor=lightblue]
	140571585400576 -> 140571583812064
	140571583812064 [label=AccumulateGrad]
	140571583811584 -> 140571583811728
	140571583811584 [label=NativeBatchNormBackward0]
	140571583813168 -> 140571583811584
	140571583813168 [label=ConvolutionBackward0]
	140571585121488 -> 140571583813168
	140571583813504 -> 140571583813168
	140571585400976 [label="stage4.2.fuse_layers.3.2.0.0.weight
 (144, 72, 3, 3)" fillcolor=lightblue]
	140571585400976 -> 140571583813504
	140571583813504 [label=AccumulateGrad]
	140571583812112 -> 140571583811584
	140571585401056 [label="stage4.2.fuse_layers.3.2.0.1.weight
 (144)" fillcolor=lightblue]
	140571585401056 -> 140571583812112
	140571583812112 [label=AccumulateGrad]
	140571583811776 -> 140571583811584
	140571585401136 [label="stage4.2.fuse_layers.3.2.0.1.bias
 (144)" fillcolor=lightblue]
	140571585401136 -> 140571583811776
	140571583811776 [label=AccumulateGrad]
	140571585860304 -> 140571583170448
	140571585133632 -> 140571585132288
	140571585401536 [label="head.0.weight
 (270, 270, 1, 1)" fillcolor=lightblue]
	140571585401536 -> 140571585133632
	140571585133632 [label=AccumulateGrad]
	140571585133056 -> 140571585132288
	140571585401616 [label="head.0.bias
 (270)" fillcolor=lightblue]
	140571585401616 -> 140571585133056
	140571585133056 [label=AccumulateGrad]
	140571585131040 -> 140571585130896
	140571585401696 [label="head.1.weight
 (270)" fillcolor=lightblue]
	140571585401696 -> 140571585131040
	140571585131040 [label=AccumulateGrad]
	140571585134400 -> 140571585130896
	140571585401776 [label="head.1.bias
 (270)" fillcolor=lightblue]
	140571585401776 -> 140571585134400
	140571585134400 [label=AccumulateGrad]
	140571585133776 -> 140571601152096
	140571585402176 [label="head.3.weight
 (9, 270, 1, 1)" fillcolor=lightblue]
	140571585402176 -> 140571585133776
	140571585133776 [label=AccumulateGrad]
	140571585131568 -> 140571601152096
	140571585402256 [label="head.3.bias
 (9)" fillcolor=lightblue]
	140571585402256 -> 140571585131568
	140571585131568 [label=AccumulateGrad]
	140571601152096 -> 140571584970208
}
